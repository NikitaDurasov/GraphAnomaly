{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "from shutil import copyfile\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import OneClassSVM\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, accuracy_score\n",
    "\n",
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def projection_on_set(vector, set_of_vectors):\n",
    "    projection_vector = np.zeros(*vector.shape) \n",
    "    for ort_vector in set_of_vectors:\n",
    "        projection_vector += np.dot(vector, ort_vector) / np.linalg.norm(ort_vector) * ort_vector\n",
    "    return projection_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import AnonymousWalkEmbeddings, AnonymousWalkKernel, func_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'AnonymousWalkEmbeddings' from 'AnonymousWalkEmbeddings.pyc'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(AnonymousWalkEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'AnonymousWalkKernel' from 'AnonymousWalkKernel.pyc'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(AnonymousWalkKernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Детектирование аномалий в потоках графов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Описание \n",
    "\n",
    "В этом блоке будем сравнивать алгоритмы, которые могут выделять аномальные элементы в потоках графов.\n",
    "Для сравнения будем использовать алгоритмы [ParCube](#parcube), [DeltaCon](#deltacon), [Concept Drift and Anomaly Detection in Graph Streams](#cdadgs) и [наш](#ref) алгоритм\n",
    "\n",
    "Будем проводить эксперименты на датасете [TwitterSecurity2014](http://odds.cs.stonybrook.edu/twittersecurity-dataset/)\n",
    "* количество нод ~ 130 к\n",
    "* количество таймстемпов ~ 120 \n",
    "* 20 аномальных дней \n",
    "\n",
    "Сам датасет состоит из строк вида: user_1 user_2 number_of_co-mentions\n",
    "\n",
    "В качестве метрики для сравнения будем считать precision/recall \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='parcube'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ParCube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='deltacon'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DeltaCon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='cdadgs'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Concept Drift and Anomaly Detection in Graph Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='ref'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Сначала пробмуем поучить модель с такими же параметрами, как на MUTAG из туториала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 600\n",
      "Number of words: 203\n",
      "Initialized\n",
      "Epoch: 0\n",
      "Graph 0: 37 nodes\n",
      "Average loss at step 100: 6583.571959\n",
      "Average loss at step 200: 1092.649179\n",
      "Average loss at step 300: 608.776665\n",
      "Average loss at step 400: 512.629523\n",
      "Average loss at step 500: 475.712387\n",
      "Average loss at step 600: 465.809455\n",
      "Average loss at step 700: 452.367608\n",
      "Average loss at step 800: 449.253075\n",
      "Average loss at step 900: 440.461396\n",
      "Average loss at step 1000: 436.225709\n",
      "Time: 57.7240409851\n",
      "Graph 1: 23 nodes\n",
      "Average loss at step 1100: 775.175831\n",
      "Average loss at step 1200: 469.608985\n",
      "Average loss at step 1300: 461.144233\n",
      "Average loss at step 1400: 459.399336\n",
      "Average loss at step 1500: 454.750434\n",
      "Average loss at step 1600: 445.991252\n",
      "Average loss at step 1700: 449.588799\n",
      "Average loss at step 1800: 449.613976\n",
      "Average loss at step 1900: 448.775502\n",
      "Average loss at step 2000: 442.475475\n",
      "Graph 2: 25 nodes\n",
      "Average loss at step 2100: 706.207592\n",
      "Average loss at step 2200: 477.423511\n",
      "Average loss at step 2300: 470.975546\n",
      "Average loss at step 2400: 469.875986\n",
      "Average loss at step 2500: 467.977812\n",
      "Average loss at step 2600: 465.170220\n",
      "Average loss at step 2700: 466.187656\n",
      "Average loss at step 2800: 462.785411\n",
      "Average loss at step 2900: 461.770910\n",
      "Average loss at step 3000: 462.617732\n",
      "Graph 3: 24 nodes\n",
      "Average loss at step 3100: 675.064457\n",
      "Average loss at step 3200: 480.636468\n",
      "Average loss at step 3300: 472.855062\n",
      "Average loss at step 3400: 469.591521\n",
      "Average loss at step 3500: 469.221331\n",
      "Average loss at step 3600: 469.630137\n",
      "Average loss at step 3700: 470.517323\n",
      "Average loss at step 3800: 470.315766\n",
      "Average loss at step 3900: 466.498334\n",
      "Average loss at step 4000: 469.414114\n",
      "Graph 4: 23 nodes\n",
      "Average loss at step 4100: 610.423584\n",
      "Average loss at step 4200: 469.691662\n",
      "Average loss at step 4300: 464.565078\n",
      "Average loss at step 4400: 463.398726\n",
      "Average loss at step 4500: 461.723047\n",
      "Average loss at step 4600: 461.243201\n",
      "Average loss at step 4700: 460.324206\n",
      "Average loss at step 4800: 460.662096\n",
      "Average loss at step 4900: 460.219085\n",
      "Average loss at step 5000: 461.811900\n",
      "Graph 5: 24 nodes\n",
      "Average loss at step 5100: 627.225907\n",
      "Average loss at step 5200: 470.688210\n",
      "Average loss at step 5300: 464.561460\n",
      "Average loss at step 5400: 465.959247\n",
      "Average loss at step 5500: 464.845497\n",
      "Average loss at step 5600: 462.820334\n",
      "Average loss at step 5700: 464.567181\n",
      "Average loss at step 5800: 463.589744\n",
      "Average loss at step 5900: 462.379558\n",
      "Average loss at step 6000: 462.297693\n",
      "Graph 6: 26 nodes\n",
      "Average loss at step 6100: 587.369563\n",
      "Average loss at step 6200: 431.915097\n",
      "Average loss at step 6300: 428.807439\n",
      "Average loss at step 6400: 426.898381\n",
      "Average loss at step 6500: 423.934343\n",
      "Average loss at step 6600: 424.539476\n",
      "Average loss at step 6700: 427.161054\n",
      "Average loss at step 6800: 424.026461\n",
      "Average loss at step 6900: 425.744380\n",
      "Average loss at step 7000: 426.226838\n",
      "Graph 7: 88 nodes\n",
      "Average loss at step 7100: 763.283030\n",
      "Average loss at step 7200: 362.268754\n",
      "Average loss at step 7300: 355.181056\n",
      "Average loss at step 7400: 347.070744\n",
      "Average loss at step 7500: 345.362079\n",
      "Average loss at step 7600: 343.968734\n",
      "Average loss at step 7700: 340.574883\n",
      "Average loss at step 7800: 344.369420\n",
      "Average loss at step 7900: 338.965894\n",
      "Average loss at step 8000: 340.846733\n",
      "Graph 8: 23 nodes\n",
      "Average loss at step 8100: 685.149745\n",
      "Average loss at step 8200: 475.598992\n",
      "Average loss at step 8300: 471.543175\n",
      "Average loss at step 8400: 471.202831\n",
      "Average loss at step 8500: 466.795914\n",
      "Average loss at step 8600: 466.728018\n",
      "Average loss at step 8700: 467.079708\n",
      "Average loss at step 8800: 468.691707\n",
      "Average loss at step 8900: 466.611945\n",
      "Average loss at step 9000: 465.048524\n",
      "Graph 9: 32 nodes\n",
      "Average loss at step 9100: 634.474651\n",
      "Average loss at step 9200: 467.719365\n",
      "Average loss at step 9300: 463.110146\n",
      "Average loss at step 9400: 460.009725\n",
      "Average loss at step 9500: 460.773346\n",
      "Average loss at step 9600: 460.112475\n",
      "Average loss at step 9700: 460.148153\n",
      "Average loss at step 9800: 459.179058\n",
      "Average loss at step 9900: 458.010691\n",
      "Average loss at step 10000: 458.153907\n",
      "Graph 10: 4 nodes\n",
      "Average loss at step 10100: 817.246451\n",
      "Average loss at step 10200: 482.295617\n",
      "Average loss at step 10300: 464.499669\n",
      "Average loss at step 10400: 457.698856\n",
      "Average loss at step 10500: 454.864383\n",
      "Average loss at step 10600: 451.895114\n",
      "Average loss at step 10700: 450.446633\n",
      "Average loss at step 10800: 449.352571\n",
      "Average loss at step 10900: 448.246386\n",
      "Average loss at step 11000: 448.434769\n",
      "Time: 45.7181830406\n",
      "Graph 11: 14 nodes\n",
      "Average loss at step 11100: 661.716112\n",
      "Average loss at step 11200: 470.815570\n",
      "Average loss at step 11300: 463.745600\n",
      "Average loss at step 11400: 465.610996\n",
      "Average loss at step 11500: 463.609203\n",
      "Average loss at step 11600: 464.247956\n",
      "Average loss at step 11700: 465.999322\n",
      "Average loss at step 11800: 463.941087\n",
      "Average loss at step 11900: 463.795097\n",
      "Average loss at step 12000: 463.983841\n",
      "Graph 12: 42 nodes\n",
      "Average loss at step 12100: 589.579488\n",
      "Average loss at step 12200: 459.172241\n",
      "Average loss at step 12300: 455.819428\n",
      "Average loss at step 12400: 454.529944\n",
      "Average loss at step 12500: 452.097026\n",
      "Average loss at step 12600: 452.456197\n",
      "Average loss at step 12700: 452.328355\n",
      "Average loss at step 12800: 450.349226\n",
      "Average loss at step 12900: 450.854981\n",
      "Average loss at step 13000: 451.599523\n",
      "Graph 13: 41 nodes\n",
      "Average loss at step 13100: 564.228882\n",
      "Average loss at step 13200: 458.048275\n",
      "Average loss at step 13300: 454.937272\n",
      "Average loss at step 13400: 453.353789\n",
      "Average loss at step 13500: 453.423565\n",
      "Average loss at step 13600: 450.317322\n",
      "Average loss at step 13700: 451.734803\n",
      "Average loss at step 13800: 452.623446\n",
      "Average loss at step 13900: 450.600704\n",
      "Average loss at step 14000: 450.863828\n",
      "Graph 14: 36 nodes\n",
      "Average loss at step 14100: 596.376993\n",
      "Average loss at step 14200: 466.004201\n",
      "Average loss at step 14300: 459.091247\n",
      "Average loss at step 14400: 457.890794\n",
      "Average loss at step 14500: 457.916479\n",
      "Average loss at step 14600: 454.730118\n",
      "Average loss at step 14700: 455.769725\n",
      "Average loss at step 14800: 456.507643\n",
      "Average loss at step 14900: 455.720488\n",
      "Average loss at step 15000: 454.641573\n",
      "Graph 15: 55 nodes\n",
      "Average loss at step 15100: 491.444622\n",
      "Average loss at step 15200: 390.928686\n",
      "Average loss at step 15300: 387.636728\n",
      "Average loss at step 15400: 387.825262\n",
      "Average loss at step 15500: 384.799245\n",
      "Average loss at step 15600: 383.627301\n",
      "Average loss at step 15700: 384.522998\n",
      "Average loss at step 15800: 381.897077\n",
      "Average loss at step 15900: 382.235003\n",
      "Average loss at step 16000: 382.286905\n",
      "Graph 16: 40 nodes\n",
      "Average loss at step 16100: 556.179269\n",
      "Average loss at step 16200: 400.467387\n",
      "Average loss at step 16300: 402.347709\n",
      "Average loss at step 16400: 402.076616\n",
      "Average loss at step 16500: 401.492192\n",
      "Average loss at step 16600: 396.148461\n",
      "Average loss at step 16700: 400.748767\n",
      "Average loss at step 16800: 396.889817\n",
      "Average loss at step 16900: 401.512361\n",
      "Average loss at step 17000: 400.500774\n",
      "Graph 17: 38 nodes\n",
      "Average loss at step 17100: 517.919657\n",
      "Average loss at step 17200: 406.443017\n",
      "Average loss at step 17300: 405.553352\n",
      "Average loss at step 17400: 400.764984\n",
      "Average loss at step 17500: 401.124457\n",
      "Average loss at step 17600: 398.429709\n",
      "Average loss at step 17700: 400.700853\n",
      "Average loss at step 17800: 404.954962\n",
      "Average loss at step 17900: 399.502867\n",
      "Average loss at step 18000: 402.601190\n",
      "Graph 18: 2 nodes\n",
      "Average loss at step 18100: 4.451840\n",
      "Average loss at step 18200: 0.000000\n",
      "Average loss at step 18300: 0.000000\n",
      "Average loss at step 18400: 0.000000\n",
      "Average loss at step 18500: 0.000000\n",
      "Average loss at step 18600: 0.000000\n",
      "Average loss at step 18700: 0.000000\n",
      "Average loss at step 18800: 0.000000\n",
      "Average loss at step 18900: 0.000000\n",
      "Average loss at step 19000: 0.000000\n",
      "Graph 19: 35 nodes\n",
      "Average loss at step 19100: 604.786213\n",
      "Average loss at step 19200: 447.213068\n",
      "Average loss at step 19300: 442.486677\n",
      "Average loss at step 19400: 444.728138\n",
      "Average loss at step 19500: 442.586278\n",
      "Average loss at step 19600: 443.323137\n",
      "Average loss at step 19700: 443.023435\n",
      "Average loss at step 19800: 442.533850\n",
      "Average loss at step 19900: 440.484862\n",
      "Average loss at step 20000: 444.256211\n",
      "Graph 20: 42 nodes\n",
      "Average loss at step 20100: 550.068772\n",
      "Average loss at step 20200: 442.443131\n",
      "Average loss at step 20300: 438.210442\n",
      "Average loss at step 20400: 436.163640\n",
      "Average loss at step 20500: 435.954906\n",
      "Average loss at step 20600: 434.563309\n",
      "Average loss at step 20700: 434.841744\n",
      "Average loss at step 20800: 435.205173\n",
      "Average loss at step 20900: 432.276522\n",
      "Average loss at step 21000: 432.350700\n",
      "Time: 54.787017107\n",
      "Graph 21: 41 nodes\n",
      "Average loss at step 21100: 553.534026\n",
      "Average loss at step 21200: 428.336238\n",
      "Average loss at step 21300: 432.999392\n",
      "Average loss at step 21400: 427.023417\n",
      "Average loss at step 21500: 428.629481\n",
      "Average loss at step 21600: 426.384999\n",
      "Average loss at step 21700: 426.289871\n",
      "Average loss at step 21800: 422.541787\n",
      "Average loss at step 21900: 427.174054\n",
      "Average loss at step 22000: 426.435695\n",
      "Graph 22: 39 nodes\n",
      "Average loss at step 22100: 546.396700\n",
      "Average loss at step 22200: 436.679061\n",
      "Average loss at step 22300: 439.751906\n",
      "Average loss at step 22400: 436.710417\n",
      "Average loss at step 22500: 436.953556\n",
      "Average loss at step 22600: 435.142686\n",
      "Average loss at step 22700: 434.273205\n",
      "Average loss at step 22800: 435.098484\n",
      "Average loss at step 22900: 434.700156\n",
      "Average loss at step 23000: 436.727196\n",
      "Graph 23: 42 nodes\n",
      "Average loss at step 23100: 506.864949\n",
      "Average loss at step 23200: 434.232399\n",
      "Average loss at step 23300: 426.572629\n",
      "Average loss at step 23400: 430.763351\n",
      "Average loss at step 23500: 426.585070\n",
      "Average loss at step 23600: 425.495304\n",
      "Average loss at step 23700: 426.308673\n",
      "Average loss at step 23800: 427.257565\n",
      "Average loss at step 23900: 427.022088\n",
      "Average loss at step 24000: 428.835927\n",
      "Graph 24: 41 nodes\n",
      "Average loss at step 24100: 553.682045\n",
      "Average loss at step 24200: 435.666658\n",
      "Average loss at step 24300: 432.696034\n",
      "Average loss at step 24400: 430.160039\n",
      "Average loss at step 24500: 430.182229\n",
      "Average loss at step 24600: 428.507335\n",
      "Average loss at step 24700: 430.040619\n",
      "Average loss at step 24800: 429.289262\n",
      "Average loss at step 24900: 429.419867\n",
      "Average loss at step 25000: 429.001261\n",
      "Graph 25: 40 nodes\n",
      "Average loss at step 25100: 540.156615\n",
      "Average loss at step 25200: 443.995385\n",
      "Average loss at step 25300: 441.748806\n",
      "Average loss at step 25400: 439.597815\n",
      "Average loss at step 25500: 440.146111\n",
      "Average loss at step 25600: 439.552176\n",
      "Average loss at step 25700: 438.284888\n",
      "Average loss at step 25800: 437.906872\n",
      "Average loss at step 25900: 437.478517\n",
      "Average loss at step 26000: 436.293645\n",
      "Graph 26: 37 nodes\n",
      "Average loss at step 26100: 502.811739\n",
      "Average loss at step 26200: 428.836646\n",
      "Average loss at step 26300: 424.042218\n",
      "Average loss at step 26400: 424.257354\n",
      "Average loss at step 26500: 423.175584\n",
      "Average loss at step 26600: 425.235616\n",
      "Average loss at step 26700: 420.687711\n",
      "Average loss at step 26800: 421.316325\n",
      "Average loss at step 26900: 423.083203\n",
      "Average loss at step 27000: 423.917182\n",
      "Graph 27: 23 nodes\n",
      "Average loss at step 27100: 532.669751\n",
      "Average loss at step 27200: 435.011062\n",
      "Average loss at step 27300: 438.875225\n",
      "Average loss at step 27400: 437.840100\n",
      "Average loss at step 27500: 435.824744\n",
      "Average loss at step 27600: 439.275342\n",
      "Average loss at step 27700: 432.134816\n",
      "Average loss at step 27800: 436.351661\n",
      "Average loss at step 27900: 435.909251\n",
      "Average loss at step 28000: 434.026331\n",
      "Graph 28: 22 nodes\n",
      "Average loss at step 28100: 534.051192\n",
      "Average loss at step 28200: 447.737712\n",
      "Average loss at step 28300: 446.495425\n",
      "Average loss at step 28400: 448.357706\n",
      "Average loss at step 28500: 443.664950\n",
      "Average loss at step 28600: 444.158525\n",
      "Average loss at step 28700: 441.069862\n",
      "Average loss at step 28800: 444.948771\n",
      "Average loss at step 28900: 442.629596\n",
      "Average loss at step 29000: 443.813374\n",
      "Graph 29: 34 nodes\n",
      "Average loss at step 29100: 600.802641\n",
      "Average loss at step 29200: 454.996375\n",
      "Average loss at step 29300: 452.617532\n",
      "Average loss at step 29400: 451.817097\n",
      "Average loss at step 29500: 450.414933\n",
      "Average loss at step 29600: 450.358977\n",
      "Average loss at step 29700: 450.521961\n",
      "Average loss at step 29800: 450.776763\n",
      "Average loss at step 29900: 451.633739\n",
      "Average loss at step 30000: 450.704251\n",
      "Graph 30: 38 nodes\n",
      "Average loss at step 30100: 549.104259\n",
      "Average loss at step 30200: 430.195959\n",
      "Average loss at step 30300: 425.421356\n",
      "Average loss at step 30400: 427.372844\n",
      "Average loss at step 30500: 426.254104\n",
      "Average loss at step 30600: 425.549074\n",
      "Average loss at step 30700: 424.140233\n",
      "Average loss at step 30800: 423.975889\n",
      "Average loss at step 30900: 422.723653\n",
      "Average loss at step 31000: 424.463918\n",
      "Time: 52.4287741184\n",
      "Graph 31: 38 nodes\n",
      "Average loss at step 31100: 527.099603\n",
      "Average loss at step 31200: 434.138284\n",
      "Average loss at step 31300: 433.723146\n",
      "Average loss at step 31400: 432.483002\n",
      "Average loss at step 31500: 434.277788\n",
      "Average loss at step 31600: 431.636619\n",
      "Average loss at step 31700: 433.845118\n",
      "Average loss at step 31800: 433.220593\n",
      "Average loss at step 31900: 431.293725\n",
      "Average loss at step 32000: 430.622779\n",
      "Graph 32: 39 nodes\n",
      "Average loss at step 32100: 539.704169\n",
      "Average loss at step 32200: 428.726862\n",
      "Average loss at step 32300: 429.509329\n",
      "Average loss at step 32400: 427.770725\n",
      "Average loss at step 32500: 426.800545\n",
      "Average loss at step 32600: 426.135090\n",
      "Average loss at step 32700: 427.946990\n",
      "Average loss at step 32800: 424.832569\n",
      "Average loss at step 32900: 430.041739\n",
      "Average loss at step 33000: 424.795434\n",
      "Graph 33: 8 nodes\n",
      "Average loss at step 33100: 577.843021\n",
      "Average loss at step 33200: 483.195929\n",
      "Average loss at step 33300: 480.792956\n",
      "Average loss at step 33400: 477.367228\n",
      "Average loss at step 33500: 475.907684\n",
      "Average loss at step 33600: 476.012418\n",
      "Average loss at step 33700: 476.460571\n",
      "Average loss at step 33800: 474.717938\n",
      "Average loss at step 33900: 475.223676\n",
      "Average loss at step 34000: 473.650349\n",
      "Graph 34: 23 nodes\n",
      "Average loss at step 34100: 527.884429\n",
      "Average loss at step 34200: 455.641280\n",
      "Average loss at step 34300: 454.049479\n",
      "Average loss at step 34400: 456.452695\n",
      "Average loss at step 34500: 452.486044\n",
      "Average loss at step 34600: 449.509367\n",
      "Average loss at step 34700: 451.140359\n",
      "Average loss at step 34800: 448.608818\n",
      "Average loss at step 34900: 449.710557\n",
      "Average loss at step 35000: 451.680523\n",
      "Graph 35: 42 nodes\n",
      "Average loss at step 35100: 476.907375\n",
      "Average loss at step 35200: 410.343489\n",
      "Average loss at step 35300: 409.076684\n",
      "Average loss at step 35400: 407.864521\n",
      "Average loss at step 35500: 407.668140\n",
      "Average loss at step 35600: 407.299885\n",
      "Average loss at step 35700: 407.193360\n",
      "Average loss at step 35800: 406.179258\n",
      "Average loss at step 35900: 405.358491\n",
      "Average loss at step 36000: 405.584982\n",
      "Graph 36: 42 nodes\n",
      "Average loss at step 36100: 475.695114\n",
      "Average loss at step 36200: 408.636394\n",
      "Average loss at step 36300: 408.607758\n",
      "Average loss at step 36400: 408.194006\n",
      "Average loss at step 36500: 406.086070\n",
      "Average loss at step 36600: 407.506856\n",
      "Average loss at step 36700: 405.371579\n",
      "Average loss at step 36800: 408.435139\n",
      "Average loss at step 36900: 404.174065\n",
      "Average loss at step 37000: 403.166556\n",
      "Graph 37: 28 nodes\n",
      "Average loss at step 37100: 1207.383787\n",
      "Average loss at step 37200: 341.151315\n",
      "Average loss at step 37300: 127.023919\n",
      "Average loss at step 37400: 93.391658\n",
      "Average loss at step 37500: 77.270864\n",
      "Average loss at step 37600: 72.329217\n",
      "Average loss at step 37700: 71.086450\n",
      "Average loss at step 37800: 71.978736\n",
      "Average loss at step 37900: 73.400626\n",
      "Average loss at step 38000: 67.876268\n",
      "Graph 38: 24 nodes\n",
      "Average loss at step 38100: 544.203653\n",
      "Average loss at step 38200: 468.647004\n",
      "Average loss at step 38300: 466.339871\n",
      "Average loss at step 38400: 466.346780\n",
      "Average loss at step 38500: 464.580186\n",
      "Average loss at step 38600: 463.895973\n",
      "Average loss at step 38700: 463.688659\n",
      "Average loss at step 38800: 463.351595\n",
      "Average loss at step 38900: 464.296394\n",
      "Average loss at step 39000: 463.423292\n",
      "Graph 39: 24 nodes\n",
      "Average loss at step 39100: 497.864446\n",
      "Average loss at step 39200: 439.120376\n",
      "Average loss at step 39300: 436.960824\n",
      "Average loss at step 39400: 438.731946\n",
      "Average loss at step 39500: 436.693380\n",
      "Average loss at step 39600: 438.129519\n",
      "Average loss at step 39700: 436.939983\n",
      "Average loss at step 39800: 435.194298\n",
      "Average loss at step 39900: 437.254073\n",
      "Average loss at step 40000: 437.081383\n",
      "Graph 40: 47 nodes\n",
      "Average loss at step 40100: 487.803798\n",
      "Average loss at step 40200: 414.433434\n",
      "Average loss at step 40300: 414.897193\n",
      "Average loss at step 40400: 413.489495\n",
      "Average loss at step 40500: 413.207134\n",
      "Average loss at step 40600: 410.951370\n",
      "Average loss at step 40700: 409.774983\n",
      "Average loss at step 40800: 412.813818\n",
      "Average loss at step 40900: 410.719180\n",
      "Average loss at step 41000: 412.115821\n",
      "Time: 52.2515580654\n",
      "Graph 41: 45 nodes\n",
      "Average loss at step 41100: 455.016365\n",
      "Average loss at step 41200: 403.291378\n",
      "Average loss at step 41300: 404.857127\n",
      "Average loss at step 41400: 404.255590\n",
      "Average loss at step 41500: 398.365375\n",
      "Average loss at step 41600: 401.099358\n",
      "Average loss at step 41700: 398.133652\n",
      "Average loss at step 41800: 402.612243\n",
      "Average loss at step 41900: 396.509721\n",
      "Average loss at step 42000: 402.463303\n",
      "Graph 42: 45 nodes\n",
      "Average loss at step 42100: 457.657639\n",
      "Average loss at step 42200: 407.233112\n",
      "Average loss at step 42300: 402.925487\n",
      "Average loss at step 42400: 400.853515\n",
      "Average loss at step 42500: 402.803446\n",
      "Average loss at step 42600: 401.131027\n",
      "Average loss at step 42700: 401.096005\n",
      "Average loss at step 42800: 402.589544\n",
      "Average loss at step 42900: 403.602258\n",
      "Average loss at step 43000: 401.740808\n",
      "Graph 43: 45 nodes\n",
      "Average loss at step 43100: 459.789728\n",
      "Average loss at step 43200: 405.193130\n",
      "Average loss at step 43300: 403.729689\n",
      "Average loss at step 43400: 406.144224\n",
      "Average loss at step 43500: 401.005741\n",
      "Average loss at step 43600: 400.205443\n",
      "Average loss at step 43700: 401.605394\n",
      "Average loss at step 43800: 402.255716\n",
      "Average loss at step 43900: 398.582340\n",
      "Average loss at step 44000: 400.474283\n",
      "Graph 44: 46 nodes\n",
      "Average loss at step 44100: 442.524736\n",
      "Average loss at step 44200: 401.946158\n",
      "Average loss at step 44300: 397.514856\n",
      "Average loss at step 44400: 394.657215\n",
      "Average loss at step 44500: 397.298590\n",
      "Average loss at step 44600: 398.235473\n",
      "Average loss at step 44700: 396.475794\n",
      "Average loss at step 44800: 397.303725\n",
      "Average loss at step 44900: 397.409142\n",
      "Average loss at step 45000: 398.300650\n",
      "Graph 45: 44 nodes\n",
      "Average loss at step 45100: 470.386966\n",
      "Average loss at step 45200: 397.477827\n",
      "Average loss at step 45300: 400.242712\n",
      "Average loss at step 45400: 398.554180\n",
      "Average loss at step 45500: 398.172758\n",
      "Average loss at step 45600: 399.996293\n",
      "Average loss at step 45700: 393.538845\n",
      "Average loss at step 45800: 399.440487\n",
      "Average loss at step 45900: 399.697499\n",
      "Average loss at step 46000: 398.513815\n",
      "Graph 46: 30 nodes\n",
      "Average loss at step 46100: 552.644165\n",
      "Average loss at step 46200: 462.599544\n",
      "Average loss at step 46300: 459.467785\n",
      "Average loss at step 46400: 459.809934\n",
      "Average loss at step 46500: 459.575441\n",
      "Average loss at step 46600: 457.570159\n",
      "Average loss at step 46700: 459.358739\n",
      "Average loss at step 46800: 457.981822\n",
      "Average loss at step 46900: 457.356923\n",
      "Average loss at step 47000: 458.162415\n",
      "Graph 47: 32 nodes\n",
      "Average loss at step 47100: 516.876933\n",
      "Average loss at step 47200: 462.445223\n",
      "Average loss at step 47300: 458.992859\n",
      "Average loss at step 47400: 456.893219\n",
      "Average loss at step 47500: 458.254311\n",
      "Average loss at step 47600: 457.995432\n",
      "Average loss at step 47700: 456.766726\n",
      "Average loss at step 47800: 457.511781\n",
      "Average loss at step 47900: 457.461720\n",
      "Average loss at step 48000: 458.361371\n",
      "Graph 48: 33 nodes\n",
      "Average loss at step 48100: 515.465119\n",
      "Average loss at step 48200: 458.757913\n",
      "Average loss at step 48300: 460.282359\n",
      "Average loss at step 48400: 457.712666\n",
      "Average loss at step 48500: 457.891266\n",
      "Average loss at step 48600: 457.713499\n",
      "Average loss at step 48700: 453.791438\n",
      "Average loss at step 48800: 456.143174\n",
      "Average loss at step 48900: 457.104205\n",
      "Average loss at step 49000: 456.912866\n",
      "Graph 49: 9 nodes\n",
      "Average loss at step 49100: 578.607129\n",
      "Average loss at step 49200: 479.556522\n",
      "Average loss at step 49300: 477.887199\n",
      "Average loss at step 49400: 477.102045\n",
      "Average loss at step 49500: 475.197578\n",
      "Average loss at step 49600: 474.308597\n",
      "Average loss at step 49700: 474.634828\n",
      "Average loss at step 49800: 473.964590\n",
      "Average loss at step 49900: 474.477962\n",
      "Average loss at step 50000: 473.337127\n",
      "Graph 50: 27 nodes\n",
      "Average loss at step 50100: 479.409371\n",
      "Average loss at step 50200: 426.673534\n",
      "Average loss at step 50300: 420.879966\n",
      "Average loss at step 50400: 419.723852\n",
      "Average loss at step 50500: 421.245586\n",
      "Average loss at step 50600: 418.576400\n",
      "Average loss at step 50700: 420.070855\n",
      "Average loss at step 50800: 418.884614\n",
      "Average loss at step 50900: 419.416243\n",
      "Average loss at step 51000: 416.627476\n",
      "Time: 49.0778679848\n",
      "Graph 51: 39 nodes\n",
      "Average loss at step 51100: 488.091184\n",
      "Average loss at step 51200: 435.220340\n",
      "Average loss at step 51300: 434.775291\n",
      "Average loss at step 51400: 433.657625\n",
      "Average loss at step 51500: 434.500612\n",
      "Average loss at step 51600: 433.511574\n",
      "Average loss at step 51700: 430.955475\n",
      "Average loss at step 51800: 429.628447\n",
      "Average loss at step 51900: 432.025791\n",
      "Average loss at step 52000: 431.849272\n",
      "Graph 52: 16 nodes\n",
      "Average loss at step 52100: 458.595140\n",
      "Average loss at step 52200: 430.205215\n",
      "Average loss at step 52300: 426.910880\n",
      "Average loss at step 52400: 425.629469\n",
      "Average loss at step 52500: 425.754861\n",
      "Average loss at step 52600: 424.300719\n",
      "Average loss at step 52700: 424.353653\n",
      "Average loss at step 52800: 425.087840\n",
      "Average loss at step 52900: 424.257763\n",
      "Average loss at step 53000: 422.053053\n",
      "Graph 53: 18 nodes\n",
      "Average loss at step 53100: 459.251845\n",
      "Average loss at step 53200: 420.037633\n",
      "Average loss at step 53300: 417.917230\n",
      "Average loss at step 53400: 414.551575\n",
      "Average loss at step 53500: 414.125596\n",
      "Average loss at step 53600: 414.607876\n",
      "Average loss at step 53700: 412.540971\n",
      "Average loss at step 53800: 412.401735\n",
      "Average loss at step 53900: 415.987806\n",
      "Average loss at step 54000: 413.539932\n",
      "Graph 54: 7 nodes\n",
      "Average loss at step 54100: 570.664758\n",
      "Average loss at step 54200: 475.491306\n",
      "Average loss at step 54300: 474.258080\n",
      "Average loss at step 54400: 471.911980\n",
      "Average loss at step 54500: 470.253139\n",
      "Average loss at step 54600: 469.386967\n",
      "Average loss at step 54700: 471.146589\n",
      "Average loss at step 54800: 469.029103\n",
      "Average loss at step 54900: 470.092853\n",
      "Average loss at step 55000: 468.309269\n",
      "Graph 55: 18 nodes\n",
      "Average loss at step 55100: 496.339794\n",
      "Average loss at step 55200: 447.776505\n",
      "Average loss at step 55300: 445.218390\n",
      "Average loss at step 55400: 445.014447\n",
      "Average loss at step 55500: 446.338169\n",
      "Average loss at step 55600: 443.164709\n",
      "Average loss at step 55700: 440.385915\n",
      "Average loss at step 55800: 439.359020\n",
      "Average loss at step 55900: 442.358492\n",
      "Average loss at step 56000: 439.326476\n",
      "Graph 56: 10 nodes\n",
      "Average loss at step 56100: 528.870092\n",
      "Average loss at step 56200: 470.590650\n",
      "Average loss at step 56300: 468.383826\n",
      "Average loss at step 56400: 467.799677\n",
      "Average loss at step 56500: 468.772510\n",
      "Average loss at step 56600: 467.640042\n",
      "Average loss at step 56700: 468.328328\n",
      "Average loss at step 56800: 467.771222\n",
      "Average loss at step 56900: 466.594019\n",
      "Average loss at step 57000: 466.025749\n",
      "Graph 57: 21 nodes\n",
      "Average loss at step 57100: 518.939352\n",
      "Average loss at step 57200: 436.320914\n",
      "Average loss at step 57300: 433.916599\n",
      "Average loss at step 57400: 432.848168\n",
      "Average loss at step 57500: 435.569574\n",
      "Average loss at step 57600: 434.981290\n",
      "Average loss at step 57700: 436.241858\n",
      "Average loss at step 57800: 436.178151\n",
      "Average loss at step 57900: 435.855964\n",
      "Average loss at step 58000: 435.887681\n",
      "Graph 58: 18 nodes\n",
      "Average loss at step 58100: 466.953461\n",
      "Average loss at step 58200: 417.397877\n",
      "Average loss at step 58300: 418.052978\n",
      "Average loss at step 58400: 415.353491\n",
      "Average loss at step 58500: 416.356279\n",
      "Average loss at step 58600: 413.784968\n",
      "Average loss at step 58700: 415.123206\n",
      "Average loss at step 58800: 412.811403\n",
      "Average loss at step 58900: 412.470613\n",
      "Average loss at step 59000: 413.023780\n",
      "Graph 59: 10 nodes\n",
      "Average loss at step 59100: 529.204411\n",
      "Average loss at step 59200: 470.276236\n",
      "Average loss at step 59300: 467.623604\n",
      "Average loss at step 59400: 467.713477\n",
      "Average loss at step 59500: 467.879546\n",
      "Average loss at step 59600: 468.421884\n",
      "Average loss at step 59700: 466.643934\n",
      "Average loss at step 59800: 466.687317\n",
      "Average loss at step 59900: 467.075558\n",
      "Average loss at step 60000: 467.377216\n",
      "Graph 60: 9 nodes\n",
      "Average loss at step 60100: 579.622633\n",
      "Average loss at step 60200: 482.462453\n",
      "Average loss at step 60300: 481.747323\n",
      "Average loss at step 60400: 479.548237\n",
      "Average loss at step 60500: 481.111358\n",
      "Average loss at step 60600: 479.018994\n",
      "Average loss at step 60700: 477.572690\n",
      "Average loss at step 60800: 477.483516\n",
      "Average loss at step 60900: 479.375565\n",
      "Average loss at step 61000: 479.295667\n",
      "Time: 48.6167891026\n",
      "Graph 61: 39 nodes\n",
      "Average loss at step 61100: 535.105232\n",
      "Average loss at step 61200: 458.230289\n",
      "Average loss at step 61300: 459.704150\n",
      "Average loss at step 61400: 458.169820\n",
      "Average loss at step 61500: 458.102440\n",
      "Average loss at step 61600: 457.096699\n",
      "Average loss at step 61700: 456.962940\n",
      "Average loss at step 61800: 460.145826\n",
      "Average loss at step 61900: 458.971434\n",
      "Average loss at step 62000: 454.624742\n",
      "Graph 62: 33 nodes\n",
      "Average loss at step 62100: 489.046704\n",
      "Average loss at step 62200: 457.595415\n",
      "Average loss at step 62300: 456.579242\n",
      "Average loss at step 62400: 454.276019\n",
      "Average loss at step 62500: 453.795147\n",
      "Average loss at step 62600: 454.720089\n",
      "Average loss at step 62700: 453.559159\n",
      "Average loss at step 62800: 452.827491\n",
      "Average loss at step 62900: 453.129561\n",
      "Average loss at step 63000: 454.041010\n",
      "Graph 63: 29 nodes\n",
      "Average loss at step 63100: 514.943737\n",
      "Average loss at step 63200: 455.240911\n",
      "Average loss at step 63300: 455.840405\n",
      "Average loss at step 63400: 452.814723\n",
      "Average loss at step 63500: 454.550285\n",
      "Average loss at step 63600: 452.920558\n",
      "Average loss at step 63700: 451.957526\n",
      "Average loss at step 63800: 452.405533\n",
      "Average loss at step 63900: 453.646552\n",
      "Average loss at step 64000: 451.237885\n",
      "Graph 64: 24 nodes\n",
      "Average loss at step 64100: 504.563513\n",
      "Average loss at step 64200: 459.837131\n",
      "Average loss at step 64300: 460.335369\n",
      "Average loss at step 64400: 459.841780\n",
      "Average loss at step 64500: 457.635628\n",
      "Average loss at step 64600: 459.560720\n",
      "Average loss at step 64700: 456.680767\n",
      "Average loss at step 64800: 458.805818\n",
      "Average loss at step 64900: 458.108560\n",
      "Average loss at step 65000: 456.772081\n",
      "Graph 65: 25 nodes\n",
      "Average loss at step 65100: 484.819853\n",
      "Average loss at step 65200: 458.501909\n",
      "Average loss at step 65300: 456.550015\n",
      "Average loss at step 65400: 456.611960\n",
      "Average loss at step 65500: 455.492564\n",
      "Average loss at step 65600: 453.994740\n",
      "Average loss at step 65700: 455.581896\n",
      "Average loss at step 65800: 453.744774\n",
      "Average loss at step 65900: 454.024556\n",
      "Average loss at step 66000: 454.034157\n",
      "Graph 66: 30 nodes\n",
      "Average loss at step 66100: 545.851208\n",
      "Average loss at step 66200: 459.478698\n",
      "Average loss at step 66300: 457.263399\n",
      "Average loss at step 66400: 457.113348\n",
      "Average loss at step 66500: 456.570389\n",
      "Average loss at step 66600: 458.363536\n",
      "Average loss at step 66700: 455.964283\n",
      "Average loss at step 66800: 454.860809\n",
      "Average loss at step 66900: 456.790538\n",
      "Average loss at step 67000: 457.666562\n",
      "Graph 67: 38 nodes\n",
      "Average loss at step 67100: 439.415948\n",
      "Average loss at step 67200: 409.102016\n",
      "Average loss at step 67300: 408.940300\n",
      "Average loss at step 67400: 405.613233\n",
      "Average loss at step 67500: 404.673779\n",
      "Average loss at step 67600: 407.366885\n",
      "Average loss at step 67700: 403.701390\n",
      "Average loss at step 67800: 403.820767\n",
      "Average loss at step 67900: 401.635488\n",
      "Average loss at step 68000: 404.473242\n",
      "Graph 68: 28 nodes\n",
      "Average loss at step 68100: 485.066980\n",
      "Average loss at step 68200: 449.688137\n",
      "Average loss at step 68300: 453.204823\n",
      "Average loss at step 68400: 449.629064\n",
      "Average loss at step 68500: 446.806848\n",
      "Average loss at step 68600: 448.873822\n",
      "Average loss at step 68700: 448.521105\n",
      "Average loss at step 68800: 448.453292\n",
      "Average loss at step 68900: 449.068861\n",
      "Average loss at step 69000: 448.598888\n",
      "Graph 69: 28 nodes\n",
      "Average loss at step 69100: 484.507764\n",
      "Average loss at step 69200: 458.207783\n",
      "Average loss at step 69300: 457.144634\n",
      "Average loss at step 69400: 455.830294\n",
      "Average loss at step 69500: 455.055964\n",
      "Average loss at step 69600: 454.276854\n",
      "Average loss at step 69700: 454.262921\n",
      "Average loss at step 69800: 455.565432\n",
      "Average loss at step 69900: 455.915702\n",
      "Average loss at step 70000: 453.958751\n",
      "Graph 70: 38 nodes\n",
      "Average loss at step 70100: 450.044747\n",
      "Average loss at step 70200: 406.842351\n",
      "Average loss at step 70300: 405.181197\n",
      "Average loss at step 70400: 407.076277\n",
      "Average loss at step 70500: 406.119708\n",
      "Average loss at step 70600: 406.020252\n",
      "Average loss at step 70700: 405.388968\n",
      "Average loss at step 70800: 405.706316\n",
      "Average loss at step 70900: 405.175606\n",
      "Average loss at step 71000: 404.615802\n",
      "Time: 54.0792078972\n",
      "Graph 71: 40 nodes\n",
      "Average loss at step 71100: 486.761131\n",
      "Average loss at step 71200: 452.033062\n",
      "Average loss at step 71300: 452.074708\n",
      "Average loss at step 71400: 450.216958\n",
      "Average loss at step 71500: 451.822741\n",
      "Average loss at step 71600: 451.861441\n",
      "Average loss at step 71700: 450.523197\n",
      "Average loss at step 71800: 451.374434\n",
      "Average loss at step 71900: 449.230927\n",
      "Average loss at step 72000: 449.527798\n",
      "Graph 72: 40 nodes\n",
      "Average loss at step 72100: 451.279045\n",
      "Average loss at step 72200: 420.121562\n",
      "Average loss at step 72300: 419.837959\n",
      "Average loss at step 72400: 418.494128\n",
      "Average loss at step 72500: 419.462858\n",
      "Average loss at step 72600: 422.829000\n",
      "Average loss at step 72700: 421.622530\n",
      "Average loss at step 72800: 417.315988\n",
      "Average loss at step 72900: 418.157924\n",
      "Average loss at step 73000: 419.235841\n",
      "Graph 73: 42 nodes\n",
      "Average loss at step 73100: 448.891784\n",
      "Average loss at step 73200: 420.280267\n",
      "Average loss at step 73300: 423.473973\n",
      "Average loss at step 73400: 418.450205\n",
      "Average loss at step 73500: 419.834303\n",
      "Average loss at step 73600: 418.215442\n",
      "Average loss at step 73700: 417.567279\n",
      "Average loss at step 73800: 421.301429\n",
      "Average loss at step 73900: 416.545076\n",
      "Average loss at step 74000: 418.842834\n",
      "Graph 74: 20 nodes\n",
      "Average loss at step 74100: 488.599178\n",
      "Average loss at step 74200: 461.914317\n",
      "Average loss at step 74300: 459.665222\n",
      "Average loss at step 74400: 457.630913\n",
      "Average loss at step 74500: 458.591393\n",
      "Average loss at step 74600: 457.709018\n",
      "Average loss at step 74700: 456.692265\n",
      "Average loss at step 74800: 455.584438\n",
      "Average loss at step 74900: 457.997154\n",
      "Average loss at step 75000: 458.239065\n",
      "Graph 75: 19 nodes\n",
      "Average loss at step 75100: 491.551579\n",
      "Average loss at step 75200: 457.694980\n",
      "Average loss at step 75300: 458.247025\n",
      "Average loss at step 75400: 454.016066\n",
      "Average loss at step 75500: 455.880328\n",
      "Average loss at step 75600: 455.752729\n",
      "Average loss at step 75700: 456.405746\n",
      "Average loss at step 75800: 457.396040\n",
      "Average loss at step 75900: 455.439054\n",
      "Average loss at step 76000: 456.265653\n",
      "Graph 76: 16 nodes\n",
      "Average loss at step 76100: 487.917524\n",
      "Average loss at step 76200: 465.312962\n",
      "Average loss at step 76300: 463.482727\n",
      "Average loss at step 76400: 463.525868\n",
      "Average loss at step 76500: 461.457326\n",
      "Average loss at step 76600: 463.060534\n",
      "Average loss at step 76700: 462.979608\n",
      "Average loss at step 76800: 462.239204\n",
      "Average loss at step 76900: 462.090193\n",
      "Average loss at step 77000: 461.089257\n",
      "Graph 77: 17 nodes\n",
      "Average loss at step 77100: 483.343067\n",
      "Average loss at step 77200: 462.040845\n",
      "Average loss at step 77300: 462.398650\n",
      "Average loss at step 77400: 460.929742\n",
      "Average loss at step 77500: 457.517550\n",
      "Average loss at step 77600: 460.699136\n",
      "Average loss at step 77700: 460.039005\n",
      "Average loss at step 77800: 459.787307\n",
      "Average loss at step 77900: 459.986070\n",
      "Average loss at step 78000: 458.532794\n",
      "Graph 78: 20 nodes\n",
      "Average loss at step 78100: 488.185461\n",
      "Average loss at step 78200: 444.992151\n",
      "Average loss at step 78300: 446.657425\n",
      "Average loss at step 78400: 447.084306\n",
      "Average loss at step 78500: 444.479817\n",
      "Average loss at step 78600: 443.878545\n",
      "Average loss at step 78700: 446.780667\n",
      "Average loss at step 78800: 445.106281\n",
      "Average loss at step 78900: 442.523849\n",
      "Average loss at step 79000: 443.269419\n",
      "Graph 79: 18 nodes\n",
      "Average loss at step 79100: 477.956063\n",
      "Average loss at step 79200: 459.998514\n",
      "Average loss at step 79300: 460.194584\n",
      "Average loss at step 79400: 457.556504\n",
      "Average loss at step 79500: 458.149384\n",
      "Average loss at step 79600: 455.709611\n",
      "Average loss at step 79700: 457.723889\n",
      "Average loss at step 79800: 457.049021\n",
      "Average loss at step 79900: 456.416249\n",
      "Average loss at step 80000: 455.804329\n",
      "Graph 80: 33 nodes\n",
      "Average loss at step 80100: 435.754776\n",
      "Average loss at step 80200: 420.874957\n",
      "Average loss at step 80300: 421.243258\n",
      "Average loss at step 80400: 418.355851\n",
      "Average loss at step 80500: 421.136238\n",
      "Average loss at step 80600: 418.729481\n",
      "Average loss at step 80700: 420.041030\n",
      "Average loss at step 80800: 421.306374\n",
      "Average loss at step 80900: 420.774494\n",
      "Average loss at step 81000: 419.721274\n",
      "Time: 51.8037269115\n",
      "Graph 81: 23 nodes\n",
      "Average loss at step 81100: 486.628801\n",
      "Average loss at step 81200: 459.721564\n",
      "Average loss at step 81300: 456.621417\n",
      "Average loss at step 81400: 457.388165\n",
      "Average loss at step 81500: 457.691148\n",
      "Average loss at step 81600: 456.500533\n",
      "Average loss at step 81700: 456.023413\n",
      "Average loss at step 81800: 456.949552\n",
      "Average loss at step 81900: 456.845995\n",
      "Average loss at step 82000: 456.948095\n",
      "Graph 82: 23 nodes\n",
      "Average loss at step 82100: 482.676525\n",
      "Average loss at step 82200: 457.733382\n",
      "Average loss at step 82300: 458.327600\n",
      "Average loss at step 82400: 455.383766\n",
      "Average loss at step 82500: 456.968591\n",
      "Average loss at step 82600: 454.464564\n",
      "Average loss at step 82700: 454.203398\n",
      "Average loss at step 82800: 455.542870\n",
      "Average loss at step 82900: 454.899328\n",
      "Average loss at step 83000: 455.320277\n",
      "Graph 83: 35 nodes\n",
      "Average loss at step 83100: 442.196337\n",
      "Average loss at step 83200: 427.764685\n",
      "Average loss at step 83300: 432.635520\n",
      "Average loss at step 83400: 427.607208\n",
      "Average loss at step 83500: 427.514424\n",
      "Average loss at step 83600: 425.080358\n",
      "Average loss at step 83700: 423.863312\n",
      "Average loss at step 83800: 425.343405\n",
      "Average loss at step 83900: 427.002691\n",
      "Average loss at step 84000: 427.086544\n",
      "Graph 84: 33 nodes\n",
      "Average loss at step 84100: 485.543310\n",
      "Average loss at step 84200: 437.751308\n",
      "Average loss at step 84300: 436.344125\n",
      "Average loss at step 84400: 437.988536\n",
      "Average loss at step 84500: 435.243633\n",
      "Average loss at step 84600: 434.926798\n",
      "Average loss at step 84700: 433.529246\n",
      "Average loss at step 84800: 435.230067\n",
      "Average loss at step 84900: 435.770526\n",
      "Average loss at step 85000: 436.448150\n",
      "Graph 85: 39 nodes\n",
      "Average loss at step 85100: 488.880354\n",
      "Average loss at step 85200: 451.754176\n",
      "Average loss at step 85300: 452.199328\n",
      "Average loss at step 85400: 453.752394\n",
      "Average loss at step 85500: 452.585078\n",
      "Average loss at step 85600: 450.827303\n",
      "Average loss at step 85700: 453.326530\n",
      "Average loss at step 85800: 451.778623\n",
      "Average loss at step 85900: 450.331831\n",
      "Average loss at step 86000: 452.371376\n",
      "Graph 86: 40 nodes\n",
      "Average loss at step 86100: 463.469398\n",
      "Average loss at step 86200: 448.958633\n",
      "Average loss at step 86300: 448.928929\n",
      "Average loss at step 86400: 445.083196\n",
      "Average loss at step 86500: 447.950725\n",
      "Average loss at step 86600: 446.621184\n",
      "Average loss at step 86700: 445.570154\n",
      "Average loss at step 86800: 447.935270\n",
      "Average loss at step 86900: 444.833980\n",
      "Average loss at step 87000: 442.934484\n",
      "Graph 87: 38 nodes\n",
      "Average loss at step 87100: 417.380420\n",
      "Average loss at step 87200: 403.901807\n",
      "Average loss at step 87300: 400.631176\n",
      "Average loss at step 87400: 401.298901\n",
      "Average loss at step 87500: 402.200833\n",
      "Average loss at step 87600: 398.707809\n",
      "Average loss at step 87700: 401.849107\n",
      "Average loss at step 87800: 397.827821\n",
      "Average loss at step 87900: 401.197614\n",
      "Average loss at step 88000: 397.968220\n",
      "Graph 88: 40 nodes\n",
      "Average loss at step 88100: 412.154896\n",
      "Average loss at step 88200: 402.147428\n",
      "Average loss at step 88300: 400.265826\n",
      "Average loss at step 88400: 399.136489\n",
      "Average loss at step 88500: 398.728724\n",
      "Average loss at step 88600: 401.221668\n",
      "Average loss at step 88700: 402.306185\n",
      "Average loss at step 88800: 400.762517\n",
      "Average loss at step 88900: 399.955595\n",
      "Average loss at step 89000: 398.531131\n",
      "Graph 89: 38 nodes\n",
      "Average loss at step 89100: 485.683825\n",
      "Average loss at step 89200: 452.055193\n",
      "Average loss at step 89300: 451.090661\n",
      "Average loss at step 89400: 450.717398\n",
      "Average loss at step 89500: 451.393484\n",
      "Average loss at step 89600: 449.842941\n",
      "Average loss at step 89700: 449.503852\n",
      "Average loss at step 89800: 452.018376\n",
      "Average loss at step 89900: 447.706216\n",
      "Average loss at step 90000: 450.489766\n",
      "Graph 90: 37 nodes\n",
      "Average loss at step 90100: 439.345942\n",
      "Average loss at step 90200: 423.812808\n",
      "Average loss at step 90300: 421.933832\n",
      "Average loss at step 90400: 421.993104\n",
      "Average loss at step 90500: 421.134452\n",
      "Average loss at step 90600: 417.804459\n",
      "Average loss at step 90700: 421.951334\n",
      "Average loss at step 90800: 421.949836\n",
      "Average loss at step 90900: 420.450850\n",
      "Average loss at step 91000: 420.018087\n",
      "Time: 52.8553931713\n",
      "Graph 91: 36 nodes\n",
      "Average loss at step 91100: 440.086565\n",
      "Average loss at step 91200: 429.822666\n",
      "Average loss at step 91300: 428.654491\n",
      "Average loss at step 91400: 428.898120\n",
      "Average loss at step 91500: 428.144927\n",
      "Average loss at step 91600: 426.090398\n",
      "Average loss at step 91700: 423.775563\n",
      "Average loss at step 91800: 425.644694\n",
      "Average loss at step 91900: 427.148848\n",
      "Average loss at step 92000: 425.143376\n",
      "Graph 92: 34 nodes\n",
      "Average loss at step 92100: 468.434482\n",
      "Average loss at step 92200: 452.958721\n",
      "Average loss at step 92300: 453.887379\n",
      "Average loss at step 92400: 455.542193\n",
      "Average loss at step 92500: 452.449126\n",
      "Average loss at step 92600: 453.859291\n",
      "Average loss at step 92700: 451.003262\n",
      "Average loss at step 92800: 452.222816\n",
      "Average loss at step 92900: 452.256841\n",
      "Average loss at step 93000: 452.889168\n",
      "Graph 93: 34 nodes\n",
      "Average loss at step 93100: 448.985505\n",
      "Average loss at step 93200: 433.905079\n",
      "Average loss at step 93300: 433.703366\n",
      "Average loss at step 93400: 434.744802\n",
      "Average loss at step 93500: 434.667012\n",
      "Average loss at step 93600: 434.036094\n",
      "Average loss at step 93700: 432.607869\n",
      "Average loss at step 93800: 433.812605\n",
      "Average loss at step 93900: 430.502792\n",
      "Average loss at step 94000: 433.067204\n",
      "Graph 94: 32 nodes\n",
      "Average loss at step 94100: 447.929891\n",
      "Average loss at step 94200: 435.617162\n",
      "Average loss at step 94300: 436.461341\n",
      "Average loss at step 94400: 432.740714\n",
      "Average loss at step 94500: 434.047045\n",
      "Average loss at step 94600: 434.243093\n",
      "Average loss at step 94700: 433.365286\n",
      "Average loss at step 94800: 432.756562\n",
      "Average loss at step 94900: 432.508774\n",
      "Average loss at step 95000: 432.327809\n",
      "Graph 95: 18 nodes\n",
      "Average loss at step 95100: 472.476442\n",
      "Average loss at step 95200: 461.991653\n",
      "Average loss at step 95300: 460.985251\n",
      "Average loss at step 95400: 461.459007\n",
      "Average loss at step 95500: 461.224517\n",
      "Average loss at step 95600: 460.466878\n",
      "Average loss at step 95700: 459.694539\n",
      "Average loss at step 95800: 459.877286\n",
      "Average loss at step 95900: 459.267203\n",
      "Average loss at step 96000: 457.918362\n",
      "Graph 96: 32 nodes\n",
      "Average loss at step 96100: 465.712587\n",
      "Average loss at step 96200: 446.610208\n",
      "Average loss at step 96300: 441.695723\n",
      "Average loss at step 96400: 442.303090\n",
      "Average loss at step 96500: 443.869581\n",
      "Average loss at step 96600: 445.195014\n",
      "Average loss at step 96700: 443.816287\n",
      "Average loss at step 96800: 443.044960\n",
      "Average loss at step 96900: 444.233030\n",
      "Average loss at step 97000: 442.103364\n",
      "Graph 97: 34 nodes\n",
      "Average loss at step 97100: 403.723811\n",
      "Average loss at step 97200: 392.274475\n",
      "Average loss at step 97300: 390.905463\n",
      "Average loss at step 97400: 392.716301\n",
      "Average loss at step 97500: 390.258446\n",
      "Average loss at step 97600: 390.767727\n",
      "Average loss at step 97700: 390.155946\n",
      "Average loss at step 97800: 388.445300\n",
      "Average loss at step 97900: 389.866917\n",
      "Average loss at step 98000: 389.393109\n",
      "Graph 98: 30 nodes\n",
      "Average loss at step 98100: 424.368224\n",
      "Average loss at step 98200: 385.735762\n",
      "Average loss at step 98300: 389.851945\n",
      "Average loss at step 98400: 389.132470\n",
      "Average loss at step 98500: 387.298226\n",
      "Average loss at step 98600: 385.235712\n",
      "Average loss at step 98700: 388.410563\n",
      "Average loss at step 98800: 387.647163\n",
      "Average loss at step 98900: 387.621169\n",
      "Average loss at step 99000: 389.506721\n",
      "Graph 99: 5 nodes\n",
      "Average loss at step 99100: 520.249762\n",
      "Average loss at step 99200: 486.157906\n",
      "Average loss at step 99300: 484.908807\n",
      "Average loss at step 99400: 483.398675\n",
      "Average loss at step 99500: 481.593478\n",
      "Average loss at step 99600: 481.568808\n",
      "Average loss at step 99700: 481.081949\n",
      "Average loss at step 99800: 480.705081\n",
      "Average loss at step 99900: 479.761482\n",
      "Average loss at step 100000: 480.511426\n",
      "Graph 100: 45 nodes\n",
      "Average loss at step 100100: 416.289483\n",
      "Average loss at step 100200: 397.241299\n",
      "Average loss at step 100300: 396.371757\n",
      "Average loss at step 100400: 396.102988\n",
      "Average loss at step 100500: 396.647118\n",
      "Average loss at step 100600: 394.817282\n",
      "Average loss at step 100700: 394.799474\n",
      "Average loss at step 100800: 395.293041\n",
      "Average loss at step 100900: 394.724689\n",
      "Average loss at step 101000: 392.571555\n",
      "Time: 51.3550739288\n",
      "Graph 101: 42 nodes\n",
      "Average loss at step 101100: 404.380582\n",
      "Average loss at step 101200: 393.916209\n",
      "Average loss at step 101300: 391.809608\n",
      "Average loss at step 101400: 390.324426\n",
      "Average loss at step 101500: 389.765936\n",
      "Average loss at step 101600: 391.297909\n",
      "Average loss at step 101700: 387.411906\n",
      "Average loss at step 101800: 385.424725\n",
      "Average loss at step 101900: 390.113306\n",
      "Average loss at step 102000: 389.655824\n",
      "Graph 102: 59 nodes\n",
      "Average loss at step 102100: 382.868922\n",
      "Average loss at step 102200: 370.789774\n",
      "Average loss at step 102300: 371.960991\n",
      "Average loss at step 102400: 370.885990\n",
      "Average loss at step 102500: 372.212687\n",
      "Average loss at step 102600: 372.466897\n",
      "Average loss at step 102700: 372.865360\n",
      "Average loss at step 102800: 372.517592\n",
      "Average loss at step 102900: 372.375428\n",
      "Average loss at step 103000: 371.706313\n",
      "Graph 103: 32 nodes\n",
      "Average loss at step 103100: 432.491552\n",
      "Average loss at step 103200: 415.282072\n",
      "Average loss at step 103300: 416.361198\n",
      "Average loss at step 103400: 418.835388\n",
      "Average loss at step 103500: 414.896075\n",
      "Average loss at step 103600: 414.590336\n",
      "Average loss at step 103700: 418.791340\n",
      "Average loss at step 103800: 413.676399\n",
      "Average loss at step 103900: 414.873148\n",
      "Average loss at step 104000: 412.329813\n",
      "Graph 104: 33 nodes\n",
      "Average loss at step 104100: 419.579620\n",
      "Average loss at step 104200: 410.526443\n",
      "Average loss at step 104300: 409.097049\n",
      "Average loss at step 104400: 410.411857\n",
      "Average loss at step 104500: 410.517773\n",
      "Average loss at step 104600: 412.218744\n",
      "Average loss at step 104700: 408.473553\n",
      "Average loss at step 104800: 408.224754\n",
      "Average loss at step 104900: 409.109713\n",
      "Average loss at step 105000: 410.976614\n",
      "Graph 105: 17 nodes\n",
      "Average loss at step 105100: 470.200898\n",
      "Average loss at step 105200: 457.398141\n",
      "Average loss at step 105300: 458.544310\n",
      "Average loss at step 105400: 458.828832\n",
      "Average loss at step 105500: 457.266994\n",
      "Average loss at step 105600: 457.031556\n",
      "Average loss at step 105700: 456.164091\n",
      "Average loss at step 105800: 456.286021\n",
      "Average loss at step 105900: 457.545728\n",
      "Average loss at step 106000: 457.238633\n",
      "Graph 106: 39 nodes\n",
      "Average loss at step 106100: 425.764494\n",
      "Average loss at step 106200: 416.717342\n",
      "Average loss at step 106300: 416.412880\n",
      "Average loss at step 106400: 416.639693\n",
      "Average loss at step 106500: 415.750758\n",
      "Average loss at step 106600: 418.090204\n",
      "Average loss at step 106700: 415.403055\n",
      "Average loss at step 106800: 415.101682\n",
      "Average loss at step 106900: 416.228538\n",
      "Average loss at step 107000: 413.448009\n",
      "Graph 107: 38 nodes\n",
      "Average loss at step 107100: 424.742630\n",
      "Average loss at step 107200: 414.368282\n",
      "Average loss at step 107300: 413.939372\n",
      "Average loss at step 107400: 414.006112\n",
      "Average loss at step 107500: 417.863758\n",
      "Average loss at step 107600: 416.173640\n",
      "Average loss at step 107700: 411.260446\n",
      "Average loss at step 107800: 416.075045\n",
      "Average loss at step 107900: 414.053895\n",
      "Average loss at step 108000: 414.593908\n",
      "Graph 108: 35 nodes\n",
      "Average loss at step 108100: 429.793306\n",
      "Average loss at step 108200: 420.779542\n",
      "Average loss at step 108300: 418.930519\n",
      "Average loss at step 108400: 423.000953\n",
      "Average loss at step 108500: 421.975644\n",
      "Average loss at step 108600: 421.415947\n",
      "Average loss at step 108700: 419.648236\n",
      "Average loss at step 108800: 419.751596\n",
      "Average loss at step 108900: 422.391799\n",
      "Average loss at step 109000: 420.411021\n",
      "Graph 109: 16 nodes\n",
      "Average loss at step 109100: 468.905687\n",
      "Average loss at step 109200: 460.444088\n",
      "Average loss at step 109300: 461.730581\n",
      "Average loss at step 109400: 460.272269\n",
      "Average loss at step 109500: 460.149027\n",
      "Average loss at step 109600: 460.697918\n",
      "Average loss at step 109700: 458.746187\n",
      "Average loss at step 109800: 459.579971\n",
      "Average loss at step 109900: 458.613117\n",
      "Average loss at step 110000: 459.440418\n",
      "Graph 110: 26 nodes\n",
      "Average loss at step 110100: 468.371248\n",
      "Average loss at step 110200: 448.708835\n",
      "Average loss at step 110300: 450.332370\n",
      "Average loss at step 110400: 449.198138\n",
      "Average loss at step 110500: 450.114958\n",
      "Average loss at step 110600: 450.935946\n",
      "Average loss at step 110700: 449.416367\n",
      "Average loss at step 110800: 449.472870\n",
      "Average loss at step 110900: 450.562574\n",
      "Average loss at step 111000: 449.005527\n",
      "Time: 50.6281969547\n",
      "Graph 111: 51 nodes\n",
      "Average loss at step 111100: 414.317673\n",
      "Average loss at step 111200: 388.384540\n",
      "Average loss at step 111300: 387.106155\n",
      "Average loss at step 111400: 386.075586\n",
      "Average loss at step 111500: 387.542084\n",
      "Average loss at step 111600: 384.215837\n",
      "Average loss at step 111700: 385.501561\n",
      "Average loss at step 111800: 382.750671\n",
      "Average loss at step 111900: 384.682982\n",
      "Average loss at step 112000: 385.596573\n",
      "Graph 112: 52 nodes\n",
      "Average loss at step 112100: 431.338073\n",
      "Average loss at step 112200: 390.110433\n",
      "Average loss at step 112300: 386.591340\n",
      "Average loss at step 112400: 391.062659\n",
      "Average loss at step 112500: 386.271502\n",
      "Average loss at step 112600: 384.965510\n",
      "Average loss at step 112700: 387.080449\n",
      "Average loss at step 112800: 388.321596\n",
      "Average loss at step 112900: 384.412052\n",
      "Average loss at step 113000: 385.406344\n",
      "Graph 113: 25 nodes\n",
      "Average loss at step 113100: 493.053007\n",
      "Average loss at step 113200: 448.748947\n",
      "Average loss at step 113300: 446.570155\n",
      "Average loss at step 113400: 447.729850\n",
      "Average loss at step 113500: 449.046081\n",
      "Average loss at step 113600: 447.041509\n",
      "Average loss at step 113700: 447.439010\n",
      "Average loss at step 113800: 447.492125\n",
      "Average loss at step 113900: 448.355272\n",
      "Average loss at step 114000: 445.437754\n",
      "Graph 114: 27 nodes\n",
      "Average loss at step 114100: 459.608145\n",
      "Average loss at step 114200: 445.572300\n",
      "Average loss at step 114300: 443.825509\n",
      "Average loss at step 114400: 442.339252\n",
      "Average loss at step 114500: 442.159633\n",
      "Average loss at step 114600: 443.656267\n",
      "Average loss at step 114700: 440.314275\n",
      "Average loss at step 114800: 442.019341\n",
      "Average loss at step 114900: 444.312759\n",
      "Average loss at step 115000: 441.955260\n",
      "Graph 115: 42 nodes\n",
      "Average loss at step 115100: 470.442643\n",
      "Average loss at step 115200: 454.454324\n",
      "Average loss at step 115300: 452.201295\n",
      "Average loss at step 115400: 450.857820\n",
      "Average loss at step 115500: 453.114947\n",
      "Average loss at step 115600: 452.148197\n",
      "Average loss at step 115700: 453.505434\n",
      "Average loss at step 115800: 449.309449\n",
      "Average loss at step 115900: 450.193759\n",
      "Average loss at step 116000: 452.532472\n",
      "Graph 116: 46 nodes\n",
      "Average loss at step 116100: 392.625918\n",
      "Average loss at step 116200: 384.588780\n",
      "Average loss at step 116300: 381.770291\n",
      "Average loss at step 116400: 383.431705\n",
      "Average loss at step 116500: 383.177957\n",
      "Average loss at step 116600: 380.353238\n",
      "Average loss at step 116700: 380.619295\n",
      "Average loss at step 116800: 379.214632\n",
      "Average loss at step 116900: 382.258300\n",
      "Average loss at step 117000: 379.280339\n",
      "Graph 117: 95 nodes\n",
      "Average loss at step 117100: 370.523569\n",
      "Average loss at step 117200: 344.707498\n",
      "Average loss at step 117300: 341.082048\n",
      "Average loss at step 117400: 341.083836\n",
      "Average loss at step 117500: 340.017655\n",
      "Average loss at step 117600: 342.314359\n",
      "Average loss at step 117700: 339.636902\n",
      "Average loss at step 117800: 340.242626\n",
      "Average loss at step 117900: 343.255781\n",
      "Average loss at step 118000: 339.858700\n",
      "Graph 118: 12 nodes\n",
      "Average loss at step 118100: 494.132854\n",
      "Average loss at step 118200: 465.940588\n",
      "Average loss at step 118300: 462.181132\n",
      "Average loss at step 118400: 462.931788\n",
      "Average loss at step 118500: 462.767325\n",
      "Average loss at step 118600: 464.043313\n",
      "Average loss at step 118700: 464.217561\n",
      "Average loss at step 118800: 461.148883\n",
      "Average loss at step 118900: 462.268778\n",
      "Average loss at step 119000: 461.770589\n",
      "Graph 119: 22 nodes\n",
      "Average loss at step 119100: 421.208678\n",
      "Average loss at step 119200: 412.172120\n",
      "Average loss at step 119300: 411.060729\n",
      "Average loss at step 119400: 410.666743\n",
      "Average loss at step 119500: 412.645522\n",
      "Average loss at step 119600: 409.368062\n",
      "Average loss at step 119700: 408.770761\n",
      "Average loss at step 119800: 411.017601\n",
      "Average loss at step 119900: 408.497055\n",
      "Average loss at step 120000: 410.336782\n",
      "Graph 120: 42 nodes\n",
      "Average loss at step 120100: 423.186262\n",
      "Average loss at step 120200: 415.651055\n",
      "Average loss at step 120300: 414.336679\n",
      "Average loss at step 120400: 413.435582\n",
      "Average loss at step 120500: 414.048740\n",
      "Average loss at step 120600: 413.289526\n",
      "Average loss at step 120700: 415.021966\n",
      "Average loss at step 120800: 415.498670\n",
      "Average loss at step 120900: 414.743114\n",
      "Average loss at step 121000: 414.415373\n",
      "Time: 51.4963469505\n",
      "Graph 121: 14 nodes\n",
      "Average loss at step 121100: 466.227861\n",
      "Average loss at step 121200: 456.510568\n",
      "Average loss at step 121300: 456.050832\n",
      "Average loss at step 121400: 454.263144\n",
      "Average loss at step 121500: 454.300316\n",
      "Average loss at step 121600: 454.356699\n",
      "Average loss at step 121700: 453.066201\n",
      "Average loss at step 121800: 454.002751\n",
      "Average loss at step 121900: 454.361276\n",
      "Average loss at step 122000: 453.574184\n",
      "Graph 122: 90 nodes\n",
      "Average loss at step 122100: 370.885573\n",
      "Average loss at step 122200: 354.666887\n",
      "Average loss at step 122300: 355.786571\n",
      "Average loss at step 122400: 355.536143\n",
      "Average loss at step 122500: 355.233285\n",
      "Average loss at step 122600: 355.354210\n",
      "Average loss at step 122700: 357.268755\n",
      "Average loss at step 122800: 351.677278\n",
      "Average loss at step 122900: 353.833954\n",
      "Average loss at step 123000: 356.391843\n",
      "Graph 123: 14 nodes\n",
      "Average loss at step 123100: 465.915340\n",
      "Average loss at step 123200: 451.842505\n",
      "Average loss at step 123300: 449.209054\n",
      "Average loss at step 123400: 450.226647\n",
      "Average loss at step 123500: 449.771413\n",
      "Average loss at step 123600: 447.306399\n",
      "Average loss at step 123700: 448.573325\n",
      "Average loss at step 123800: 449.579740\n",
      "Average loss at step 123900: 447.874279\n",
      "Average loss at step 124000: 450.130424\n",
      "Graph 124: 14 nodes\n",
      "Average loss at step 124100: 430.234453\n",
      "Average loss at step 124200: 424.755485\n",
      "Average loss at step 124300: 423.156572\n",
      "Average loss at step 124400: 420.895453\n",
      "Average loss at step 124500: 423.711585\n",
      "Average loss at step 124600: 420.661672\n",
      "Average loss at step 124700: 419.799285\n",
      "Average loss at step 124800: 418.788024\n",
      "Average loss at step 124900: 421.085166\n",
      "Average loss at step 125000: 421.106992\n",
      "Graph 125: 32 nodes\n",
      "Average loss at step 125100: 380.819248\n",
      "Average loss at step 125200: 354.669147\n",
      "Average loss at step 125300: 356.833279\n",
      "Average loss at step 125400: 356.231164\n",
      "Average loss at step 125500: 357.854218\n",
      "Average loss at step 125600: 357.063627\n",
      "Average loss at step 125700: 353.880592\n",
      "Average loss at step 125800: 358.466959\n",
      "Average loss at step 125900: 355.576702\n",
      "Average loss at step 126000: 353.069216\n",
      "Graph 126: 11 nodes\n",
      "Average loss at step 126100: 483.859093\n",
      "Average loss at step 126200: 455.667338\n",
      "Average loss at step 126300: 455.849147\n",
      "Average loss at step 126400: 452.929068\n",
      "Average loss at step 126500: 456.916338\n",
      "Average loss at step 126600: 454.049372\n",
      "Average loss at step 126700: 452.950310\n",
      "Average loss at step 126800: 454.525429\n",
      "Average loss at step 126900: 455.811356\n",
      "Average loss at step 127000: 454.140689\n",
      "Graph 127: 26 nodes\n",
      "Average loss at step 127100: 364.947924\n",
      "Average loss at step 127200: 353.708574\n",
      "Average loss at step 127300: 354.137118\n",
      "Average loss at step 127400: 351.751211\n",
      "Average loss at step 127500: 353.755697\n",
      "Average loss at step 127600: 354.056999\n",
      "Average loss at step 127700: 357.432374\n",
      "Average loss at step 127800: 352.012822\n",
      "Average loss at step 127900: 350.486377\n",
      "Average loss at step 128000: 352.871073\n",
      "Graph 128: 11 nodes\n",
      "Average loss at step 128100: 480.654203\n",
      "Average loss at step 128200: 464.541218\n",
      "Average loss at step 128300: 463.905801\n",
      "Average loss at step 128400: 461.814065\n",
      "Average loss at step 128500: 463.274533\n",
      "Average loss at step 128600: 462.218395\n",
      "Average loss at step 128700: 461.935956\n",
      "Average loss at step 128800: 462.277801\n",
      "Average loss at step 128900: 461.931787\n",
      "Average loss at step 129000: 462.197455\n",
      "Graph 129: 14 nodes\n",
      "Average loss at step 129100: 487.738067\n",
      "Average loss at step 129200: 472.713516\n",
      "Average loss at step 129300: 466.779244\n",
      "Average loss at step 129400: 468.205867\n",
      "Average loss at step 129500: 468.381029\n",
      "Average loss at step 129600: 466.879045\n",
      "Average loss at step 129700: 466.949564\n",
      "Average loss at step 129800: 466.865596\n",
      "Average loss at step 129900: 466.938054\n",
      "Average loss at step 130000: 466.937119\n",
      "Graph 130: 18 nodes\n",
      "Average loss at step 130100: 474.676293\n",
      "Average loss at step 130200: 456.853189\n",
      "Average loss at step 130300: 457.174046\n",
      "Average loss at step 130400: 457.166977\n",
      "Average loss at step 130500: 457.468586\n",
      "Average loss at step 130600: 456.232619\n",
      "Average loss at step 130700: 455.780240\n",
      "Average loss at step 130800: 457.694220\n",
      "Average loss at step 130900: 455.831662\n",
      "Average loss at step 131000: 456.911466\n",
      "Time: 50.3454780579\n",
      "Graph 131: 16 nodes\n",
      "Average loss at step 131100: 458.985245\n",
      "Average loss at step 131200: 450.772753\n",
      "Average loss at step 131300: 451.115770\n",
      "Average loss at step 131400: 450.158206\n",
      "Average loss at step 131500: 449.868435\n",
      "Average loss at step 131600: 450.528767\n",
      "Average loss at step 131700: 451.254817\n",
      "Average loss at step 131800: 448.171112\n",
      "Average loss at step 131900: 451.385847\n",
      "Average loss at step 132000: 449.316843\n",
      "Graph 132: 17 nodes\n",
      "Average loss at step 132100: 462.810157\n",
      "Average loss at step 132200: 455.212308\n",
      "Average loss at step 132300: 452.972195\n",
      "Average loss at step 132400: 454.853040\n",
      "Average loss at step 132500: 452.009896\n",
      "Average loss at step 132600: 453.939015\n",
      "Average loss at step 132700: 451.636002\n",
      "Average loss at step 132800: 452.912443\n",
      "Average loss at step 132900: 454.208598\n",
      "Average loss at step 133000: 452.287904\n",
      "Graph 133: 32 nodes\n",
      "Average loss at step 133100: 383.263092\n",
      "Average loss at step 133200: 370.469579\n",
      "Average loss at step 133300: 371.848346\n",
      "Average loss at step 133400: 372.971160\n",
      "Average loss at step 133500: 367.241037\n",
      "Average loss at step 133600: 368.875975\n",
      "Average loss at step 133700: 372.065184\n",
      "Average loss at step 133800: 371.621903\n",
      "Average loss at step 133900: 369.301083\n",
      "Average loss at step 134000: 370.307204\n",
      "Graph 134: 13 nodes\n",
      "Average loss at step 134100: 465.186114\n",
      "Average loss at step 134200: 458.071821\n",
      "Average loss at step 134300: 457.729254\n",
      "Average loss at step 134400: 456.780574\n",
      "Average loss at step 134500: 456.738379\n",
      "Average loss at step 134600: 458.773619\n",
      "Average loss at step 134700: 454.811587\n",
      "Average loss at step 134800: 456.805322\n",
      "Average loss at step 134900: 454.389157\n",
      "Average loss at step 135000: 454.487093\n",
      "Graph 135: 3 nodes\n",
      "Average loss at step 135100: 1210.896641\n",
      "Average loss at step 135200: 378.988184\n",
      "Average loss at step 135300: 330.919740\n",
      "Average loss at step 135400: 320.969526\n",
      "Average loss at step 135500: 317.819709\n",
      "Average loss at step 135600: 312.431901\n",
      "Average loss at step 135700: 311.302237\n",
      "Average loss at step 135800: 309.220430\n",
      "Average loss at step 135900: 310.009073\n",
      "Average loss at step 136000: 309.131634\n",
      "Graph 136: 22 nodes\n",
      "Average loss at step 136100: 470.877184\n",
      "Average loss at step 136200: 452.672620\n",
      "Average loss at step 136300: 452.076884\n",
      "Average loss at step 136400: 452.051043\n",
      "Average loss at step 136500: 452.819575\n",
      "Average loss at step 136600: 451.146014\n",
      "Average loss at step 136700: 452.351117\n",
      "Average loss at step 136800: 449.180009\n",
      "Average loss at step 136900: 451.506984\n",
      "Average loss at step 137000: 452.497302\n",
      "Graph 137: 16 nodes\n",
      "Average loss at step 137100: 457.666461\n",
      "Average loss at step 137200: 436.179720\n",
      "Average loss at step 137300: 433.028268\n",
      "Average loss at step 137400: 436.129869\n",
      "Average loss at step 137500: 434.616421\n",
      "Average loss at step 137600: 434.310815\n",
      "Average loss at step 137700: 431.629767\n",
      "Average loss at step 137800: 435.304923\n",
      "Average loss at step 137900: 433.366513\n",
      "Average loss at step 138000: 433.258257\n",
      "Graph 138: 38 nodes\n",
      "Average loss at step 138100: 403.045522\n",
      "Average loss at step 138200: 391.185343\n",
      "Average loss at step 138300: 393.052444\n",
      "Average loss at step 138400: 392.062739\n",
      "Average loss at step 138500: 391.431137\n",
      "Average loss at step 138600: 390.223665\n",
      "Average loss at step 138700: 393.480603\n",
      "Average loss at step 138800: 388.659862\n",
      "Average loss at step 138900: 389.291483\n",
      "Average loss at step 139000: 391.109740\n",
      "Graph 139: 13 nodes\n",
      "Average loss at step 139100: 452.727266\n",
      "Average loss at step 139200: 445.278767\n",
      "Average loss at step 139300: 442.430395\n",
      "Average loss at step 139400: 441.910245\n",
      "Average loss at step 139500: 442.060058\n",
      "Average loss at step 139600: 441.470147\n",
      "Average loss at step 139700: 441.000323\n",
      "Average loss at step 139800: 442.256320\n",
      "Average loss at step 139900: 443.374053\n",
      "Average loss at step 140000: 442.435217\n",
      "Graph 140: 12 nodes\n",
      "Average loss at step 140100: 477.401969\n",
      "Average loss at step 140200: 457.468576\n",
      "Average loss at step 140300: 459.870595\n",
      "Average loss at step 140400: 459.090843\n",
      "Average loss at step 140500: 456.508207\n",
      "Average loss at step 140600: 458.223729\n",
      "Average loss at step 140700: 458.399788\n",
      "Average loss at step 140800: 457.573615\n",
      "Average loss at step 140900: 458.789918\n",
      "Average loss at step 141000: 458.672443\n",
      "Time: 53.8561298847\n",
      "Graph 141: 14 nodes\n",
      "Average loss at step 141100: 456.487190\n",
      "Average loss at step 141200: 435.381750\n",
      "Average loss at step 141300: 436.189814\n",
      "Average loss at step 141400: 434.072523\n",
      "Average loss at step 141500: 435.556350\n",
      "Average loss at step 141600: 437.255085\n",
      "Average loss at step 141700: 433.034373\n",
      "Average loss at step 141800: 436.181426\n",
      "Average loss at step 141900: 436.795841\n",
      "Average loss at step 142000: 435.483527\n",
      "Graph 142: 39 nodes\n",
      "Average loss at step 142100: 373.004366\n",
      "Average loss at step 142200: 342.764799\n",
      "Average loss at step 142300: 341.499888\n",
      "Average loss at step 142400: 343.872286\n",
      "Average loss at step 142500: 341.484075\n",
      "Average loss at step 142600: 340.311825\n",
      "Average loss at step 142700: 344.532536\n",
      "Average loss at step 142800: 341.475903\n",
      "Average loss at step 142900: 338.912179\n",
      "Average loss at step 143000: 342.973934\n",
      "Graph 143: 19 nodes\n",
      "Average loss at step 143100: 474.413616\n",
      "Average loss at step 143200: 463.377172\n",
      "Average loss at step 143300: 461.356223\n",
      "Average loss at step 143400: 458.362937\n",
      "Average loss at step 143500: 460.549628\n",
      "Average loss at step 143600: 460.463064\n",
      "Average loss at step 143700: 460.649710\n",
      "Average loss at step 143800: 459.481151\n",
      "Average loss at step 143900: 458.265986\n",
      "Average loss at step 144000: 457.147909\n",
      "Graph 144: 20 nodes\n",
      "Average loss at step 144100: 456.491384\n",
      "Average loss at step 144200: 452.461978\n",
      "Average loss at step 144300: 449.812994\n",
      "Average loss at step 144400: 448.299351\n",
      "Average loss at step 144500: 450.271833\n",
      "Average loss at step 144600: 449.180033\n",
      "Average loss at step 144700: 449.753287\n",
      "Average loss at step 144800: 449.814914\n",
      "Average loss at step 144900: 450.969796\n",
      "Average loss at step 145000: 449.187340\n",
      "Graph 145: 39 nodes\n",
      "Average loss at step 145100: 401.894353\n",
      "Average loss at step 145200: 394.878216\n",
      "Average loss at step 145300: 397.230119\n",
      "Average loss at step 145400: 396.120782\n",
      "Average loss at step 145500: 393.277790\n",
      "Average loss at step 145600: 394.389651\n",
      "Average loss at step 145700: 394.572504\n",
      "Average loss at step 145800: 394.245085\n",
      "Average loss at step 145900: 392.520953\n",
      "Average loss at step 146000: 394.249295\n",
      "Graph 146: 40 nodes\n",
      "Average loss at step 146100: 377.398124\n",
      "Average loss at step 146200: 370.721419\n",
      "Average loss at step 146300: 369.919076\n",
      "Average loss at step 146400: 366.770707\n",
      "Average loss at step 146500: 371.116356\n",
      "Average loss at step 146600: 370.911081\n",
      "Average loss at step 146700: 369.736228\n",
      "Average loss at step 146800: 371.193544\n",
      "Average loss at step 146900: 369.109499\n",
      "Average loss at step 147000: 371.984202\n",
      "Graph 147: 39 nodes\n",
      "Average loss at step 147100: 382.510309\n",
      "Average loss at step 147200: 370.776916\n",
      "Average loss at step 147300: 376.692955\n",
      "Average loss at step 147400: 372.118179\n",
      "Average loss at step 147500: 373.225904\n",
      "Average loss at step 147600: 378.397804\n",
      "Average loss at step 147700: 372.433544\n",
      "Average loss at step 147800: 378.344965\n",
      "Average loss at step 147900: 376.753759\n",
      "Average loss at step 148000: 378.967003\n",
      "Graph 148: 39 nodes\n",
      "Average loss at step 148100: 385.081325\n",
      "Average loss at step 148200: 379.104672\n",
      "Average loss at step 148300: 383.250794\n",
      "Average loss at step 148400: 379.754622\n",
      "Average loss at step 148500: 382.814079\n",
      "Average loss at step 148600: 378.320930\n",
      "Average loss at step 148700: 380.238960\n",
      "Average loss at step 148800: 382.659389\n",
      "Average loss at step 148900: 380.852408\n",
      "Average loss at step 149000: 380.309893\n",
      "Graph 149: 29 nodes\n",
      "Average loss at step 149100: 401.663658\n",
      "Average loss at step 149200: 388.999595\n",
      "Average loss at step 149300: 385.317890\n",
      "Average loss at step 149400: 386.363679\n",
      "Average loss at step 149500: 383.562815\n",
      "Average loss at step 149600: 383.495849\n",
      "Average loss at step 149700: 386.816076\n",
      "Average loss at step 149800: 385.880115\n",
      "Average loss at step 149900: 385.024160\n",
      "Average loss at step 150000: 384.972915\n",
      "Graph 150: 22 nodes\n",
      "Average loss at step 150100: 401.061466\n",
      "Average loss at step 150200: 395.286080\n",
      "Average loss at step 150300: 392.976007\n",
      "Average loss at step 150400: 390.677974\n",
      "Average loss at step 150500: 392.717028\n",
      "Average loss at step 150600: 393.523171\n",
      "Average loss at step 150700: 391.018157\n",
      "Average loss at step 150800: 390.551831\n",
      "Average loss at step 150900: 388.640087\n",
      "Average loss at step 151000: 391.246471\n",
      "Time: 50.832971096\n",
      "Graph 151: 11 nodes\n",
      "Average loss at step 151100: 462.077094\n",
      "Average loss at step 151200: 453.215519\n",
      "Average loss at step 151300: 449.640295\n",
      "Average loss at step 151400: 449.515644\n",
      "Average loss at step 151500: 448.772590\n",
      "Average loss at step 151600: 451.009491\n",
      "Average loss at step 151700: 448.166164\n",
      "Average loss at step 151800: 447.832798\n",
      "Average loss at step 151900: 446.943078\n",
      "Average loss at step 152000: 448.075461\n",
      "Graph 152: 8 nodes\n",
      "Average loss at step 152100: 461.334117\n",
      "Average loss at step 152200: 458.261296\n",
      "Average loss at step 152300: 459.295927\n",
      "Average loss at step 152400: 458.915192\n",
      "Average loss at step 152500: 457.768822\n",
      "Average loss at step 152600: 456.662492\n",
      "Average loss at step 152700: 457.314913\n",
      "Average loss at step 152800: 455.569354\n",
      "Average loss at step 152900: 455.693279\n",
      "Average loss at step 153000: 456.089842\n",
      "Graph 153: 13 nodes\n",
      "Average loss at step 153100: 485.476910\n",
      "Average loss at step 153200: 473.017306\n",
      "Average loss at step 153300: 470.624090\n",
      "Average loss at step 153400: 470.398343\n",
      "Average loss at step 153500: 469.442079\n",
      "Average loss at step 153600: 469.332370\n",
      "Average loss at step 153700: 467.567558\n",
      "Average loss at step 153800: 469.036870\n",
      "Average loss at step 153900: 469.072013\n",
      "Average loss at step 154000: 467.125259\n",
      "Graph 154: 18 nodes\n",
      "Average loss at step 154100: 408.430960\n",
      "Average loss at step 154200: 400.708372\n",
      "Average loss at step 154300: 399.169411\n",
      "Average loss at step 154400: 397.479427\n",
      "Average loss at step 154500: 396.994791\n",
      "Average loss at step 154600: 399.476532\n",
      "Average loss at step 154700: 398.447045\n",
      "Average loss at step 154800: 395.287167\n",
      "Average loss at step 154900: 397.099289\n",
      "Average loss at step 155000: 396.993976\n",
      "Graph 155: 12 nodes\n",
      "Average loss at step 155100: 439.680076\n",
      "Average loss at step 155200: 431.280565\n",
      "Average loss at step 155300: 431.339226\n",
      "Average loss at step 155400: 432.054995\n",
      "Average loss at step 155500: 432.576455\n",
      "Average loss at step 155600: 429.926820\n",
      "Average loss at step 155700: 432.854419\n",
      "Average loss at step 155800: 429.321143\n",
      "Average loss at step 155900: 432.405641\n",
      "Average loss at step 156000: 432.326097\n",
      "Graph 156: 8 nodes\n",
      "Average loss at step 156100: 478.462710\n",
      "Average loss at step 156200: 470.259782\n",
      "Average loss at step 156300: 469.741048\n",
      "Average loss at step 156400: 468.456239\n",
      "Average loss at step 156500: 466.824977\n",
      "Average loss at step 156600: 466.945509\n",
      "Average loss at step 156700: 466.190857\n",
      "Average loss at step 156800: 468.118094\n",
      "Average loss at step 156900: 467.945648\n",
      "Average loss at step 157000: 467.084481\n",
      "Graph 157: 40 nodes\n",
      "Average loss at step 157100: 397.557675\n",
      "Average loss at step 157200: 391.666249\n",
      "Average loss at step 157300: 388.463557\n",
      "Average loss at step 157400: 388.354178\n",
      "Average loss at step 157500: 386.281511\n",
      "Average loss at step 157600: 385.493227\n",
      "Average loss at step 157700: 384.486041\n",
      "Average loss at step 157800: 388.281861\n",
      "Average loss at step 157900: 383.855750\n",
      "Average loss at step 158000: 385.441455\n",
      "Graph 158: 12 nodes\n",
      "Average loss at step 158100: 464.680754\n",
      "Average loss at step 158200: 448.201744\n",
      "Average loss at step 158300: 445.683326\n",
      "Average loss at step 158400: 444.823089\n",
      "Average loss at step 158500: 444.630758\n",
      "Average loss at step 158600: 442.262603\n",
      "Average loss at step 158700: 443.780826\n",
      "Average loss at step 158800: 443.554465\n",
      "Average loss at step 158900: 442.339835\n",
      "Average loss at step 159000: 442.987963\n",
      "Graph 159: 22 nodes\n",
      "Average loss at step 159100: 453.910458\n",
      "Average loss at step 159200: 424.511264\n",
      "Average loss at step 159300: 420.180915\n",
      "Average loss at step 159400: 422.335276\n",
      "Average loss at step 159500: 422.755771\n",
      "Average loss at step 159600: 423.845714\n",
      "Average loss at step 159700: 421.855072\n",
      "Average loss at step 159800: 425.359867\n",
      "Average loss at step 159900: 421.202380\n",
      "Average loss at step 160000: 422.235559\n",
      "Graph 160: 22 nodes\n",
      "Average loss at step 160100: 390.967762\n",
      "Average loss at step 160200: 385.492003\n",
      "Average loss at step 160300: 381.830263\n",
      "Average loss at step 160400: 384.663413\n",
      "Average loss at step 160500: 381.663562\n",
      "Average loss at step 160600: 382.005033\n",
      "Average loss at step 160700: 379.556845\n",
      "Average loss at step 160800: 380.566215\n",
      "Average loss at step 160900: 381.673181\n",
      "Average loss at step 161000: 378.646743\n",
      "Time: 53.1400060654\n",
      "Graph 161: 18 nodes\n",
      "Average loss at step 161100: 478.272407\n",
      "Average loss at step 161200: 461.304747\n",
      "Average loss at step 161300: 460.491631\n",
      "Average loss at step 161400: 460.125638\n",
      "Average loss at step 161500: 461.061557\n",
      "Average loss at step 161600: 458.126589\n",
      "Average loss at step 161700: 460.139512\n",
      "Average loss at step 161800: 459.578496\n",
      "Average loss at step 161900: 458.474519\n",
      "Average loss at step 162000: 458.467173\n",
      "Graph 162: 12 nodes\n",
      "Average loss at step 162100: 490.320626\n",
      "Average loss at step 162200: 462.369488\n",
      "Average loss at step 162300: 462.285348\n",
      "Average loss at step 162400: 461.893290\n",
      "Average loss at step 162500: 461.859998\n",
      "Average loss at step 162600: 461.687614\n",
      "Average loss at step 162700: 460.538091\n",
      "Average loss at step 162800: 462.815104\n",
      "Average loss at step 162900: 460.815431\n",
      "Average loss at step 163000: 461.659744\n",
      "Graph 163: 17 nodes\n",
      "Average loss at step 163100: 454.506086\n",
      "Average loss at step 163200: 448.293165\n",
      "Average loss at step 163300: 447.688498\n",
      "Average loss at step 163400: 447.483139\n",
      "Average loss at step 163500: 447.299419\n",
      "Average loss at step 163600: 448.943182\n",
      "Average loss at step 163700: 446.811587\n",
      "Average loss at step 163800: 448.772136\n",
      "Average loss at step 163900: 447.417873\n",
      "Average loss at step 164000: 447.001892\n",
      "Graph 164: 14 nodes\n",
      "Average loss at step 164100: 464.914652\n",
      "Average loss at step 164200: 455.417235\n",
      "Average loss at step 164300: 455.230678\n",
      "Average loss at step 164400: 453.946337\n",
      "Average loss at step 164500: 455.423913\n",
      "Average loss at step 164600: 455.086559\n",
      "Average loss at step 164700: 454.146389\n",
      "Average loss at step 164800: 454.071815\n",
      "Average loss at step 164900: 451.233342\n",
      "Average loss at step 165000: 454.026437\n",
      "Graph 165: 22 nodes\n",
      "Average loss at step 165100: 464.724294\n",
      "Average loss at step 165200: 458.380226\n",
      "Average loss at step 165300: 458.725720\n",
      "Average loss at step 165400: 455.572796\n",
      "Average loss at step 165500: 457.083144\n",
      "Average loss at step 165600: 456.054664\n",
      "Average loss at step 165700: 455.158716\n",
      "Average loss at step 165800: 455.232127\n",
      "Average loss at step 165900: 453.693792\n",
      "Average loss at step 166000: 455.830989\n",
      "Graph 166: 43 nodes\n",
      "Average loss at step 166100: 428.485048\n",
      "Average loss at step 166200: 421.285350\n",
      "Average loss at step 166300: 420.593619\n",
      "Average loss at step 166400: 419.964042\n",
      "Average loss at step 166500: 420.494670\n",
      "Average loss at step 166600: 420.713192\n",
      "Average loss at step 166700: 417.903769\n",
      "Average loss at step 166800: 423.028714\n",
      "Average loss at step 166900: 420.197641\n",
      "Average loss at step 167000: 418.892075\n",
      "Graph 167: 42 nodes\n",
      "Average loss at step 167100: 401.456674\n",
      "Average loss at step 167200: 393.332126\n",
      "Average loss at step 167300: 397.638100\n",
      "Average loss at step 167400: 397.139862\n",
      "Average loss at step 167500: 394.557416\n",
      "Average loss at step 167600: 394.735952\n",
      "Average loss at step 167700: 397.286245\n",
      "Average loss at step 167800: 394.581697\n",
      "Average loss at step 167900: 393.584148\n",
      "Average loss at step 168000: 398.382980\n",
      "Graph 168: 44 nodes\n",
      "Average loss at step 168100: 377.975746\n",
      "Average loss at step 168200: 376.712430\n",
      "Average loss at step 168300: 373.619245\n",
      "Average loss at step 168400: 375.290299\n",
      "Average loss at step 168500: 374.549589\n",
      "Average loss at step 168600: 372.172256\n",
      "Average loss at step 168700: 378.597099\n",
      "Average loss at step 168800: 373.454617\n",
      "Average loss at step 168900: 377.434144\n",
      "Average loss at step 169000: 376.133339\n",
      "Graph 169: 24 nodes\n",
      "Average loss at step 169100: 457.319660\n",
      "Average loss at step 169200: 451.071948\n",
      "Average loss at step 169300: 451.587518\n",
      "Average loss at step 169400: 452.112745\n",
      "Average loss at step 169500: 452.384514\n",
      "Average loss at step 169600: 450.426223\n",
      "Average loss at step 169700: 450.513491\n",
      "Average loss at step 169800: 451.584185\n",
      "Average loss at step 169900: 450.084208\n",
      "Average loss at step 170000: 452.110959\n",
      "Graph 170: 48 nodes\n",
      "Average loss at step 170100: 399.792252\n",
      "Average loss at step 170200: 393.966485\n",
      "Average loss at step 170300: 396.804867\n",
      "Average loss at step 170400: 395.145499\n",
      "Average loss at step 170500: 394.753134\n",
      "Average loss at step 170600: 396.576741\n",
      "Average loss at step 170700: 395.454316\n",
      "Average loss at step 170800: 391.929444\n",
      "Average loss at step 170900: 392.671510\n",
      "Average loss at step 171000: 393.588298\n",
      "Time: 54.1699881554\n",
      "Graph 171: 25 nodes\n",
      "Average loss at step 171100: 447.052934\n",
      "Average loss at step 171200: 442.990855\n",
      "Average loss at step 171300: 443.382993\n",
      "Average loss at step 171400: 443.148993\n",
      "Average loss at step 171500: 443.438508\n",
      "Average loss at step 171600: 443.778076\n",
      "Average loss at step 171700: 443.781660\n",
      "Average loss at step 171800: 442.603454\n",
      "Average loss at step 171900: 439.398099\n",
      "Average loss at step 172000: 440.195851\n",
      "Graph 172: 46 nodes\n",
      "Average loss at step 172100: 386.412526\n",
      "Average loss at step 172200: 386.083851\n",
      "Average loss at step 172300: 383.126044\n",
      "Average loss at step 172400: 385.513815\n",
      "Average loss at step 172500: 382.597529\n",
      "Average loss at step 172600: 385.801864\n",
      "Average loss at step 172700: 384.600212\n",
      "Average loss at step 172800: 383.170520\n",
      "Average loss at step 172900: 383.714585\n",
      "Average loss at step 173000: 383.248685\n",
      "Graph 173: 48 nodes\n",
      "Average loss at step 173100: 400.919546\n",
      "Average loss at step 173200: 395.651799\n",
      "Average loss at step 173300: 394.466294\n",
      "Average loss at step 173400: 397.975038\n",
      "Average loss at step 173500: 393.008413\n",
      "Average loss at step 173600: 396.935991\n",
      "Average loss at step 173700: 394.547341\n",
      "Average loss at step 173800: 395.082253\n",
      "Average loss at step 173900: 392.432617\n",
      "Average loss at step 174000: 393.492729\n",
      "Graph 174: 25 nodes\n",
      "Average loss at step 174100: 468.495450\n",
      "Average loss at step 174200: 455.528068\n",
      "Average loss at step 174300: 453.173457\n",
      "Average loss at step 174400: 452.215186\n",
      "Average loss at step 174500: 454.577373\n",
      "Average loss at step 174600: 452.695131\n",
      "Average loss at step 174700: 451.437231\n",
      "Average loss at step 174800: 451.465536\n",
      "Average loss at step 174900: 451.707330\n",
      "Average loss at step 175000: 452.394708\n",
      "Graph 175: 48 nodes\n",
      "Average loss at step 175100: 404.713171\n",
      "Average loss at step 175200: 399.705176\n",
      "Average loss at step 175300: 401.295182\n",
      "Average loss at step 175400: 397.281474\n",
      "Average loss at step 175500: 401.033779\n",
      "Average loss at step 175600: 398.144109\n",
      "Average loss at step 175700: 402.452492\n",
      "Average loss at step 175800: 396.974286\n",
      "Average loss at step 175900: 398.007204\n",
      "Average loss at step 176000: 395.945240\n",
      "Graph 176: 44 nodes\n",
      "Average loss at step 176100: 375.305910\n",
      "Average loss at step 176200: 372.259362\n",
      "Average loss at step 176300: 369.699304\n",
      "Average loss at step 176400: 370.917066\n",
      "Average loss at step 176500: 370.197539\n",
      "Average loss at step 176600: 369.597763\n",
      "Average loss at step 176700: 369.128295\n",
      "Average loss at step 176800: 370.004105\n",
      "Average loss at step 176900: 369.114334\n",
      "Average loss at step 177000: 368.791944\n",
      "Graph 177: 42 nodes\n",
      "Average loss at step 177100: 385.610840\n",
      "Average loss at step 177200: 376.185867\n",
      "Average loss at step 177300: 382.704418\n",
      "Average loss at step 177400: 378.427873\n",
      "Average loss at step 177500: 382.023784\n",
      "Average loss at step 177600: 378.756862\n",
      "Average loss at step 177700: 376.683766\n",
      "Average loss at step 177800: 376.640614\n",
      "Average loss at step 177900: 376.791380\n",
      "Average loss at step 178000: 380.102183\n",
      "Graph 178: 40 nodes\n",
      "Average loss at step 178100: 400.412086\n",
      "Average loss at step 178200: 390.492508\n",
      "Average loss at step 178300: 391.206595\n",
      "Average loss at step 178400: 388.297325\n",
      "Average loss at step 178500: 389.264063\n",
      "Average loss at step 178600: 390.820277\n",
      "Average loss at step 178700: 388.304419\n",
      "Average loss at step 178800: 388.898484\n",
      "Average loss at step 178900: 386.734175\n",
      "Average loss at step 179000: 387.743531\n",
      "Graph 179: 38 nodes\n",
      "Average loss at step 179100: 397.798714\n",
      "Average loss at step 179200: 392.362018\n",
      "Average loss at step 179300: 389.524494\n",
      "Average loss at step 179400: 394.471597\n",
      "Average loss at step 179500: 391.770390\n",
      "Average loss at step 179600: 391.632140\n",
      "Average loss at step 179700: 392.629313\n",
      "Average loss at step 179800: 393.194644\n",
      "Average loss at step 179900: 389.870846\n",
      "Average loss at step 180000: 390.233717\n",
      "Graph 180: 40 nodes\n",
      "Average loss at step 180100: 400.838022\n",
      "Average loss at step 180200: 398.210606\n",
      "Average loss at step 180300: 393.337349\n",
      "Average loss at step 180400: 391.195391\n",
      "Average loss at step 180500: 394.526727\n",
      "Average loss at step 180600: 391.015451\n",
      "Average loss at step 180700: 396.583214\n",
      "Average loss at step 180800: 392.076264\n",
      "Average loss at step 180900: 394.052275\n",
      "Average loss at step 181000: 395.000473\n",
      "Time: 57.4359688759\n",
      "Graph 181: 41 nodes\n",
      "Average loss at step 181100: 396.023348\n",
      "Average loss at step 181200: 393.133987\n",
      "Average loss at step 181300: 390.468421\n",
      "Average loss at step 181400: 391.917879\n",
      "Average loss at step 181500: 391.487885\n",
      "Average loss at step 181600: 389.892663\n",
      "Average loss at step 181700: 391.078739\n",
      "Average loss at step 181800: 391.887701\n",
      "Average loss at step 181900: 386.584052\n",
      "Average loss at step 182000: 392.565406\n",
      "Graph 182: 42 nodes\n",
      "Average loss at step 182100: 383.984869\n",
      "Average loss at step 182200: 378.477201\n",
      "Average loss at step 182300: 377.686888\n",
      "Average loss at step 182400: 381.057558\n",
      "Average loss at step 182500: 377.957105\n",
      "Average loss at step 182600: 378.891965\n",
      "Average loss at step 182700: 379.548023\n",
      "Average loss at step 182800: 379.496632\n",
      "Average loss at step 182900: 380.929779\n",
      "Average loss at step 183000: 376.638331\n",
      "Graph 183: 40 nodes\n",
      "Average loss at step 183100: 369.992281\n",
      "Average loss at step 183200: 371.752827\n",
      "Average loss at step 183300: 370.351142\n",
      "Average loss at step 183400: 366.671821\n",
      "Average loss at step 183500: 370.358313\n",
      "Average loss at step 183600: 370.309052\n",
      "Average loss at step 183700: 370.602070\n",
      "Average loss at step 183800: 373.572575\n",
      "Average loss at step 183900: 370.454835\n",
      "Average loss at step 184000: 371.695185\n",
      "Graph 184: 44 nodes\n",
      "Average loss at step 184100: 373.831481\n",
      "Average loss at step 184200: 370.767252\n",
      "Average loss at step 184300: 370.257570\n",
      "Average loss at step 184400: 374.615989\n",
      "Average loss at step 184500: 374.491920\n",
      "Average loss at step 184600: 374.072766\n",
      "Average loss at step 184700: 371.886307\n",
      "Average loss at step 184800: 371.491653\n",
      "Average loss at step 184900: 372.509512\n",
      "Average loss at step 185000: 372.844342\n",
      "Graph 185: 24 nodes\n",
      "Average loss at step 185100: 429.156504\n",
      "Average loss at step 185200: 427.193509\n",
      "Average loss at step 185300: 426.000677\n",
      "Average loss at step 185400: 423.422071\n",
      "Average loss at step 185500: 423.715419\n",
      "Average loss at step 185600: 422.658094\n",
      "Average loss at step 185700: 423.985127\n",
      "Average loss at step 185800: 422.794373\n",
      "Average loss at step 185900: 422.389925\n",
      "Average loss at step 186000: 424.845567\n",
      "Graph 186: 44 nodes\n",
      "Average loss at step 186100: 379.532308\n",
      "Average loss at step 186200: 379.759492\n",
      "Average loss at step 186300: 380.416779\n",
      "Average loss at step 186400: 375.738369\n",
      "Average loss at step 186500: 381.689674\n",
      "Average loss at step 186600: 379.684168\n",
      "Average loss at step 186700: 377.773134\n",
      "Average loss at step 186800: 376.872755\n",
      "Average loss at step 186900: 381.116107\n",
      "Average loss at step 187000: 381.090234\n",
      "Graph 187: 20 nodes\n",
      "Average loss at step 187100: 438.801672\n",
      "Average loss at step 187200: 434.936152\n",
      "Average loss at step 187300: 435.550412\n",
      "Average loss at step 187400: 432.441453\n",
      "Average loss at step 187500: 436.464485\n",
      "Average loss at step 187600: 435.318628\n",
      "Average loss at step 187700: 434.337169\n",
      "Average loss at step 187800: 435.556502\n",
      "Average loss at step 187900: 436.241328\n",
      "Average loss at step 188000: 435.087564\n",
      "Graph 188: 42 nodes\n",
      "Average loss at step 188100: 388.863695\n",
      "Average loss at step 188200: 382.377711\n",
      "Average loss at step 188300: 382.409250\n",
      "Average loss at step 188400: 381.427623\n",
      "Average loss at step 188500: 379.560073\n",
      "Average loss at step 188600: 380.875542\n",
      "Average loss at step 188700: 380.941208\n",
      "Average loss at step 188800: 382.789441\n",
      "Average loss at step 188900: 379.517981\n",
      "Average loss at step 189000: 379.091006\n",
      "Graph 189: 27 nodes\n",
      "Average loss at step 189100: 459.415969\n",
      "Average loss at step 189200: 456.217263\n",
      "Average loss at step 189300: 457.185169\n",
      "Average loss at step 189400: 455.235656\n",
      "Average loss at step 189500: 454.967281\n",
      "Average loss at step 189600: 453.676747\n",
      "Average loss at step 189700: 454.327677\n",
      "Average loss at step 189800: 455.142579\n",
      "Average loss at step 189900: 455.148940\n",
      "Average loss at step 190000: 455.362471\n",
      "Graph 190: 48 nodes\n",
      "Average loss at step 190100: 401.266639\n",
      "Average loss at step 190200: 393.680071\n",
      "Average loss at step 190300: 396.906743\n",
      "Average loss at step 190400: 396.755337\n",
      "Average loss at step 190500: 395.084717\n",
      "Average loss at step 190600: 396.811919\n",
      "Average loss at step 190700: 395.589683\n",
      "Average loss at step 190800: 397.928241\n",
      "Average loss at step 190900: 395.002641\n",
      "Average loss at step 191000: 395.618154\n",
      "Time: 51.967979908\n",
      "Graph 191: 31 nodes\n",
      "Average loss at step 191100: 432.620835\n",
      "Average loss at step 191200: 428.393255\n",
      "Average loss at step 191300: 425.653551\n",
      "Average loss at step 191400: 430.541902\n",
      "Average loss at step 191500: 424.985603\n",
      "Average loss at step 191600: 424.939611\n",
      "Average loss at step 191700: 426.507880\n",
      "Average loss at step 191800: 425.826995\n",
      "Average loss at step 191900: 423.183014\n",
      "Average loss at step 192000: 425.848760\n",
      "Graph 192: 30 nodes\n",
      "Average loss at step 192100: 437.457051\n",
      "Average loss at step 192200: 436.240013\n",
      "Average loss at step 192300: 438.748470\n",
      "Average loss at step 192400: 436.645098\n",
      "Average loss at step 192500: 436.460151\n",
      "Average loss at step 192600: 437.752368\n",
      "Average loss at step 192700: 437.787017\n",
      "Average loss at step 192800: 438.179880\n",
      "Average loss at step 192900: 436.174832\n",
      "Average loss at step 193000: 435.926176\n",
      "Graph 193: 46 nodes\n",
      "Average loss at step 193100: 373.654962\n",
      "Average loss at step 193200: 369.153847\n",
      "Average loss at step 193300: 372.765172\n",
      "Average loss at step 193400: 368.030244\n",
      "Average loss at step 193500: 370.214776\n",
      "Average loss at step 193600: 369.482911\n",
      "Average loss at step 193700: 368.897121\n",
      "Average loss at step 193800: 371.639206\n",
      "Average loss at step 193900: 367.996792\n",
      "Average loss at step 194000: 371.783776\n",
      "Graph 194: 47 nodes\n",
      "Average loss at step 194100: 402.894804\n",
      "Average loss at step 194200: 400.501283\n",
      "Average loss at step 194300: 397.957324\n",
      "Average loss at step 194400: 400.628709\n",
      "Average loss at step 194500: 402.926392\n",
      "Average loss at step 194600: 398.981948\n",
      "Average loss at step 194700: 397.100727\n",
      "Average loss at step 194800: 399.649317\n",
      "Average loss at step 194900: 398.960725\n",
      "Average loss at step 195000: 400.242997\n",
      "Graph 195: 50 nodes\n",
      "Average loss at step 195100: 373.296098\n",
      "Average loss at step 195200: 368.614103\n",
      "Average loss at step 195300: 369.560428\n",
      "Average loss at step 195400: 371.196705\n",
      "Average loss at step 195500: 368.548767\n",
      "Average loss at step 195600: 368.390379\n",
      "Average loss at step 195700: 366.438240\n",
      "Average loss at step 195800: 369.767242\n",
      "Average loss at step 195900: 365.826429\n",
      "Average loss at step 196000: 366.448313\n",
      "Graph 196: 40 nodes\n",
      "Average loss at step 196100: 397.967232\n",
      "Average loss at step 196200: 394.071883\n",
      "Average loss at step 196300: 397.193664\n",
      "Average loss at step 196400: 396.562195\n",
      "Average loss at step 196500: 395.361588\n",
      "Average loss at step 196600: 395.350757\n",
      "Average loss at step 196700: 396.300265\n",
      "Average loss at step 196800: 392.603485\n",
      "Average loss at step 196900: 394.969637\n",
      "Average loss at step 197000: 394.260258\n",
      "Graph 197: 55 nodes\n",
      "Average loss at step 197100: 377.045298\n",
      "Average loss at step 197200: 372.636111\n",
      "Average loss at step 197300: 375.715569\n",
      "Average loss at step 197400: 371.803198\n",
      "Average loss at step 197500: 372.483678\n",
      "Average loss at step 197600: 372.744998\n",
      "Average loss at step 197700: 372.296796\n",
      "Average loss at step 197800: 372.911302\n",
      "Average loss at step 197900: 370.600835\n",
      "Average loss at step 198000: 368.757670\n",
      "Graph 198: 62 nodes\n",
      "Average loss at step 198100: 387.039934\n",
      "Average loss at step 198200: 384.869333\n",
      "Average loss at step 198300: 387.801080\n",
      "Average loss at step 198400: 387.036206\n",
      "Average loss at step 198500: 382.439850\n",
      "Average loss at step 198600: 383.617819\n",
      "Average loss at step 198700: 383.450190\n",
      "Average loss at step 198800: 382.519735\n",
      "Average loss at step 198900: 381.623501\n",
      "Average loss at step 199000: 383.147635\n",
      "Graph 199: 34 nodes\n",
      "Average loss at step 199100: 437.342048\n",
      "Average loss at step 199200: 431.068168\n",
      "Average loss at step 199300: 433.775029\n",
      "Average loss at step 199400: 432.007185\n",
      "Average loss at step 199500: 432.732607\n",
      "Average loss at step 199600: 431.072341\n",
      "Average loss at step 199700: 432.054266\n",
      "Average loss at step 199800: 432.046781\n",
      "Average loss at step 199900: 431.737842\n",
      "Average loss at step 200000: 429.307532\n",
      "Graph 200: 29 nodes\n",
      "Average loss at step 200100: 462.530637\n",
      "Average loss at step 200200: 462.649515\n",
      "Average loss at step 200300: 460.428190\n",
      "Average loss at step 200400: 461.012807\n",
      "Average loss at step 200500: 461.897422\n",
      "Average loss at step 200600: 462.009459\n",
      "Average loss at step 200700: 458.965077\n",
      "Average loss at step 200800: 460.144836\n",
      "Average loss at step 200900: 461.574375\n",
      "Average loss at step 201000: 460.093973\n",
      "Time: 51.5655868053\n",
      "Graph 201: 25 nodes\n",
      "Average loss at step 201100: 459.290164\n",
      "Average loss at step 201200: 455.679985\n",
      "Average loss at step 201300: 455.771483\n",
      "Average loss at step 201400: 456.616363\n",
      "Average loss at step 201500: 455.803790\n",
      "Average loss at step 201600: 455.584646\n",
      "Average loss at step 201700: 453.414551\n",
      "Average loss at step 201800: 454.722646\n",
      "Average loss at step 201900: 455.863729\n",
      "Average loss at step 202000: 455.547137\n",
      "Graph 202: 56 nodes\n",
      "Average loss at step 202100: 395.815711\n",
      "Average loss at step 202200: 390.730542\n",
      "Average loss at step 202300: 392.578214\n",
      "Average loss at step 202400: 389.966104\n",
      "Average loss at step 202500: 388.706033\n",
      "Average loss at step 202600: 388.340209\n",
      "Average loss at step 202700: 390.506008\n",
      "Average loss at step 202800: 390.592850\n",
      "Average loss at step 202900: 392.108390\n",
      "Average loss at step 203000: 388.780052\n",
      "Graph 203: 57 nodes\n",
      "Average loss at step 203100: 398.838132\n",
      "Average loss at step 203200: 396.722148\n",
      "Average loss at step 203300: 397.791898\n",
      "Average loss at step 203400: 396.680008\n",
      "Average loss at step 203500: 397.680874\n",
      "Average loss at step 203600: 395.492846\n",
      "Average loss at step 203700: 396.649210\n",
      "Average loss at step 203800: 396.964336\n",
      "Average loss at step 203900: 395.556763\n",
      "Average loss at step 204000: 397.565811\n",
      "Graph 204: 27 nodes\n",
      "Average loss at step 204100: 389.622866\n",
      "Average loss at step 204200: 388.306992\n",
      "Average loss at step 204300: 386.039411\n",
      "Average loss at step 204400: 385.428360\n",
      "Average loss at step 204500: 384.659924\n",
      "Average loss at step 204600: 387.645316\n",
      "Average loss at step 204700: 380.954059\n",
      "Average loss at step 204800: 384.896875\n",
      "Average loss at step 204900: 384.721757\n",
      "Average loss at step 205000: 383.591772\n",
      "Graph 205: 22 nodes\n",
      "Average loss at step 205100: 430.555879\n",
      "Average loss at step 205200: 429.184998\n",
      "Average loss at step 205300: 430.456419\n",
      "Average loss at step 205400: 431.294540\n",
      "Average loss at step 205500: 427.500422\n",
      "Average loss at step 205600: 428.575505\n",
      "Average loss at step 205700: 430.261198\n",
      "Average loss at step 205800: 426.502443\n",
      "Average loss at step 205900: 427.516992\n",
      "Average loss at step 206000: 426.753730\n",
      "Graph 206: 24 nodes\n",
      "Average loss at step 206100: 459.706028\n",
      "Average loss at step 206200: 458.195219\n",
      "Average loss at step 206300: 457.832039\n",
      "Average loss at step 206400: 457.731083\n",
      "Average loss at step 206500: 457.239521\n",
      "Average loss at step 206600: 459.922057\n",
      "Average loss at step 206700: 456.292630\n",
      "Average loss at step 206800: 456.835796\n",
      "Average loss at step 206900: 456.409916\n",
      "Average loss at step 207000: 455.996956\n",
      "Graph 207: 23 nodes\n",
      "Average loss at step 207100: 428.417788\n",
      "Average loss at step 207200: 421.706254\n",
      "Average loss at step 207300: 421.738512\n",
      "Average loss at step 207400: 421.846807\n",
      "Average loss at step 207500: 421.702660\n",
      "Average loss at step 207600: 420.766304\n",
      "Average loss at step 207700: 417.996257\n",
      "Average loss at step 207800: 418.864943\n",
      "Average loss at step 207900: 419.707475\n",
      "Average loss at step 208000: 422.510931\n",
      "Graph 208: 57 nodes\n",
      "Average loss at step 208100: 398.671943\n",
      "Average loss at step 208200: 393.525036\n",
      "Average loss at step 208300: 390.191423\n",
      "Average loss at step 208400: 391.812969\n",
      "Average loss at step 208500: 391.877068\n",
      "Average loss at step 208600: 391.391621\n",
      "Average loss at step 208700: 390.530607\n",
      "Average loss at step 208800: 391.834044\n",
      "Average loss at step 208900: 391.582835\n",
      "Average loss at step 209000: 391.043946\n",
      "Graph 209: 24 nodes\n",
      "Average loss at step 209100: 438.402337\n",
      "Average loss at step 209200: 434.273909\n",
      "Average loss at step 209300: 435.654208\n",
      "Average loss at step 209400: 435.073866\n",
      "Average loss at step 209500: 434.191783\n",
      "Average loss at step 209600: 433.938419\n",
      "Average loss at step 209700: 432.179235\n",
      "Average loss at step 209800: 433.816888\n",
      "Average loss at step 209900: 431.896954\n",
      "Average loss at step 210000: 431.599270\n",
      "Graph 210: 24 nodes\n",
      "Average loss at step 210100: 425.877556\n",
      "Average loss at step 210200: 421.425218\n",
      "Average loss at step 210300: 423.088536\n",
      "Average loss at step 210400: 418.695061\n",
      "Average loss at step 210500: 422.936777\n",
      "Average loss at step 210600: 424.246732\n",
      "Average loss at step 210700: 419.116239\n",
      "Average loss at step 210800: 421.518756\n",
      "Average loss at step 210900: 421.683832\n",
      "Average loss at step 211000: 421.765737\n",
      "Time: 52.8402187824\n",
      "Graph 211: 23 nodes\n",
      "Average loss at step 211100: 436.451481\n",
      "Average loss at step 211200: 436.708809\n",
      "Average loss at step 211300: 434.457624\n",
      "Average loss at step 211400: 432.835980\n",
      "Average loss at step 211500: 432.270100\n",
      "Average loss at step 211600: 434.362104\n",
      "Average loss at step 211700: 435.369584\n",
      "Average loss at step 211800: 433.406967\n",
      "Average loss at step 211900: 432.711884\n",
      "Average loss at step 212000: 433.068211\n",
      "Graph 212: 25 nodes\n",
      "Average loss at step 212100: 393.648297\n",
      "Average loss at step 212200: 394.367492\n",
      "Average loss at step 212300: 392.421076\n",
      "Average loss at step 212400: 394.190417\n",
      "Average loss at step 212500: 397.398537\n",
      "Average loss at step 212600: 396.265511\n",
      "Average loss at step 212700: 396.247327\n",
      "Average loss at step 212800: 393.915621\n",
      "Average loss at step 212900: 393.607188\n",
      "Average loss at step 213000: 393.711301\n",
      "Graph 213: 23 nodes\n",
      "Average loss at step 213100: 410.573803\n",
      "Average loss at step 213200: 408.026457\n",
      "Average loss at step 213300: 403.463750\n",
      "Average loss at step 213400: 404.215429\n",
      "Average loss at step 213500: 403.634866\n",
      "Average loss at step 213600: 404.286981\n",
      "Average loss at step 213700: 406.211354\n",
      "Average loss at step 213800: 405.458063\n",
      "Average loss at step 213900: 404.811127\n",
      "Average loss at step 214000: 402.629046\n",
      "Graph 214: 48 nodes\n",
      "Average loss at step 214100: 375.719966\n",
      "Average loss at step 214200: 368.064474\n",
      "Average loss at step 214300: 364.102481\n",
      "Average loss at step 214400: 369.984119\n",
      "Average loss at step 214500: 367.480774\n",
      "Average loss at step 214600: 369.518986\n",
      "Average loss at step 214700: 367.891405\n",
      "Average loss at step 214800: 367.505212\n",
      "Average loss at step 214900: 368.849648\n",
      "Average loss at step 215000: 366.153978\n",
      "Graph 215: 27 nodes\n",
      "Average loss at step 215100: 458.894556\n",
      "Average loss at step 215200: 455.615713\n",
      "Average loss at step 215300: 455.677004\n",
      "Average loss at step 215400: 454.920468\n",
      "Average loss at step 215500: 455.154382\n",
      "Average loss at step 215600: 454.334198\n",
      "Average loss at step 215700: 453.219091\n",
      "Average loss at step 215800: 452.984530\n",
      "Average loss at step 215900: 452.823383\n",
      "Average loss at step 216000: 455.821723\n",
      "Graph 216: 15 nodes\n",
      "Average loss at step 216100: 417.170139\n",
      "Average loss at step 216200: 413.473729\n",
      "Average loss at step 216300: 414.126662\n",
      "Average loss at step 216400: 416.727970\n",
      "Average loss at step 216500: 415.838954\n",
      "Average loss at step 216600: 409.624822\n",
      "Average loss at step 216700: 414.651598\n",
      "Average loss at step 216800: 412.522897\n",
      "Average loss at step 216900: 411.472428\n",
      "Average loss at step 217000: 410.440194\n",
      "Graph 217: 44 nodes\n",
      "Average loss at step 217100: 394.166329\n",
      "Average loss at step 217200: 391.154276\n",
      "Average loss at step 217300: 388.290114\n",
      "Average loss at step 217400: 389.045446\n",
      "Average loss at step 217500: 389.710649\n",
      "Average loss at step 217600: 391.724964\n",
      "Average loss at step 217700: 390.635103\n",
      "Average loss at step 217800: 391.227715\n",
      "Average loss at step 217900: 389.711883\n",
      "Average loss at step 218000: 388.555681\n",
      "Graph 218: 29 nodes\n",
      "Average loss at step 218100: 426.813112\n",
      "Average loss at step 218200: 428.926752\n",
      "Average loss at step 218300: 424.353067\n",
      "Average loss at step 218400: 428.748766\n",
      "Average loss at step 218500: 426.757590\n",
      "Average loss at step 218600: 423.496269\n",
      "Average loss at step 218700: 426.476721\n",
      "Average loss at step 218800: 427.101403\n",
      "Average loss at step 218900: 427.833859\n",
      "Average loss at step 219000: 424.057856\n",
      "Graph 219: 21 nodes\n",
      "Average loss at step 219100: 452.607740\n",
      "Average loss at step 219200: 452.899949\n",
      "Average loss at step 219300: 452.505236\n",
      "Average loss at step 219400: 451.841964\n",
      "Average loss at step 219500: 450.929047\n",
      "Average loss at step 219600: 451.336397\n",
      "Average loss at step 219700: 450.881800\n",
      "Average loss at step 219800: 449.591619\n",
      "Average loss at step 219900: 449.680842\n",
      "Average loss at step 220000: 449.667913\n",
      "Graph 220: 34 nodes\n",
      "Average loss at step 220100: 409.679709\n",
      "Average loss at step 220200: 405.910483\n",
      "Average loss at step 220300: 405.515648\n",
      "Average loss at step 220400: 405.214708\n",
      "Average loss at step 220500: 402.974491\n",
      "Average loss at step 220600: 403.641706\n",
      "Average loss at step 220700: 400.480492\n",
      "Average loss at step 220800: 404.647253\n",
      "Average loss at step 220900: 402.941941\n",
      "Average loss at step 221000: 403.538747\n",
      "Time: 51.2063119411\n",
      "Graph 221: 30 nodes\n",
      "Average loss at step 221100: 379.912299\n",
      "Average loss at step 221200: 377.845891\n",
      "Average loss at step 221300: 380.009799\n",
      "Average loss at step 221400: 374.723381\n",
      "Average loss at step 221500: 374.285554\n",
      "Average loss at step 221600: 376.899982\n",
      "Average loss at step 221700: 378.682395\n",
      "Average loss at step 221800: 373.813076\n",
      "Average loss at step 221900: 376.886452\n",
      "Average loss at step 222000: 374.261047\n",
      "Graph 222: 40 nodes\n",
      "Average loss at step 222100: 434.305310\n",
      "Average loss at step 222200: 430.487547\n",
      "Average loss at step 222300: 429.049299\n",
      "Average loss at step 222400: 427.248945\n",
      "Average loss at step 222500: 426.720356\n",
      "Average loss at step 222600: 428.865741\n",
      "Average loss at step 222700: 426.820721\n",
      "Average loss at step 222800: 426.317372\n",
      "Average loss at step 222900: 427.661493\n",
      "Average loss at step 223000: 427.526897\n",
      "Graph 223: 54 nodes\n",
      "Average loss at step 223100: 379.583422\n",
      "Average loss at step 223200: 378.943290\n",
      "Average loss at step 223300: 379.316587\n",
      "Average loss at step 223400: 380.990060\n",
      "Average loss at step 223500: 380.972567\n",
      "Average loss at step 223600: 383.191266\n",
      "Average loss at step 223700: 380.998984\n",
      "Average loss at step 223800: 381.776031\n",
      "Average loss at step 223900: 379.348825\n",
      "Average loss at step 224000: 378.752761\n",
      "Graph 224: 18 nodes\n",
      "Average loss at step 224100: 451.383761\n",
      "Average loss at step 224200: 446.771776\n",
      "Average loss at step 224300: 448.141936\n",
      "Average loss at step 224400: 448.803511\n",
      "Average loss at step 224500: 445.914603\n",
      "Average loss at step 224600: 446.649944\n",
      "Average loss at step 224700: 444.938983\n",
      "Average loss at step 224800: 447.831591\n",
      "Average loss at step 224900: 443.018677\n",
      "Average loss at step 225000: 445.979483\n",
      "Graph 225: 36 nodes\n",
      "Average loss at step 225100: 403.324260\n",
      "Average loss at step 225200: 395.794546\n",
      "Average loss at step 225300: 400.110916\n",
      "Average loss at step 225400: 401.302547\n",
      "Average loss at step 225500: 399.445283\n",
      "Average loss at step 225600: 398.616178\n",
      "Average loss at step 225700: 398.080529\n",
      "Average loss at step 225800: 398.046251\n",
      "Average loss at step 225900: 399.531484\n",
      "Average loss at step 226000: 399.843656\n",
      "Graph 226: 37 nodes\n",
      "Average loss at step 226100: 388.012261\n",
      "Average loss at step 226200: 389.877563\n",
      "Average loss at step 226300: 387.743778\n",
      "Average loss at step 226400: 388.812346\n",
      "Average loss at step 226500: 384.861220\n",
      "Average loss at step 226600: 387.752995\n",
      "Average loss at step 226700: 389.676991\n",
      "Average loss at step 226800: 387.989512\n",
      "Average loss at step 226900: 387.373895\n",
      "Average loss at step 227000: 385.295835\n",
      "Graph 227: 34 nodes\n",
      "Average loss at step 227100: 387.178063\n",
      "Average loss at step 227200: 390.601301\n",
      "Average loss at step 227300: 386.501269\n",
      "Average loss at step 227400: 384.578405\n",
      "Average loss at step 227500: 390.796785\n",
      "Average loss at step 227600: 386.535319\n",
      "Average loss at step 227700: 386.563136\n",
      "Average loss at step 227800: 385.843570\n",
      "Average loss at step 227900: 382.881050\n",
      "Average loss at step 228000: 388.531755\n",
      "Graph 228: 23 nodes\n",
      "Average loss at step 228100: 424.360035\n",
      "Average loss at step 228200: 422.511955\n",
      "Average loss at step 228300: 422.643008\n",
      "Average loss at step 228400: 422.509138\n",
      "Average loss at step 228500: 422.120682\n",
      "Average loss at step 228600: 420.047313\n",
      "Average loss at step 228700: 422.330585\n",
      "Average loss at step 228800: 422.698824\n",
      "Average loss at step 228900: 421.294275\n",
      "Average loss at step 229000: 423.801067\n",
      "Graph 229: 32 nodes\n",
      "Average loss at step 229100: 429.941148\n",
      "Average loss at step 229200: 425.212134\n",
      "Average loss at step 229300: 425.075062\n",
      "Average loss at step 229400: 424.510048\n",
      "Average loss at step 229500: 423.210432\n",
      "Average loss at step 229600: 428.399500\n",
      "Average loss at step 229700: 425.460489\n",
      "Average loss at step 229800: 424.393676\n",
      "Average loss at step 229900: 424.341347\n",
      "Average loss at step 230000: 424.986212\n",
      "Graph 230: 33 nodes\n",
      "Average loss at step 230100: 407.903467\n",
      "Average loss at step 230200: 406.674345\n",
      "Average loss at step 230300: 405.087258\n",
      "Average loss at step 230400: 406.104980\n",
      "Average loss at step 230500: 402.905877\n",
      "Average loss at step 230600: 403.763063\n",
      "Average loss at step 230700: 403.868576\n",
      "Average loss at step 230800: 404.656029\n",
      "Average loss at step 230900: 403.923482\n",
      "Average loss at step 231000: 408.428741\n",
      "Time: 56.4778511524\n",
      "Graph 231: 29 nodes\n",
      "Average loss at step 231100: 417.952865\n",
      "Average loss at step 231200: 417.568048\n",
      "Average loss at step 231300: 413.973358\n",
      "Average loss at step 231400: 415.430936\n",
      "Average loss at step 231500: 415.846211\n",
      "Average loss at step 231600: 415.902479\n",
      "Average loss at step 231700: 417.533575\n",
      "Average loss at step 231800: 414.384436\n",
      "Average loss at step 231900: 413.921590\n",
      "Average loss at step 232000: 419.449407\n",
      "Graph 232: 39 nodes\n",
      "Average loss at step 232100: 410.756188\n",
      "Average loss at step 232200: 410.067672\n",
      "Average loss at step 232300: 411.411389\n",
      "Average loss at step 232400: 408.546184\n",
      "Average loss at step 232500: 410.957924\n",
      "Average loss at step 232600: 409.432223\n",
      "Average loss at step 232700: 409.319396\n",
      "Average loss at step 232800: 409.557391\n",
      "Average loss at step 232900: 407.738926\n",
      "Average loss at step 233000: 407.168005\n",
      "Graph 233: 34 nodes\n",
      "Average loss at step 233100: 392.995264\n",
      "Average loss at step 233200: 390.133252\n",
      "Average loss at step 233300: 391.086278\n",
      "Average loss at step 233400: 388.619289\n",
      "Average loss at step 233500: 393.162808\n",
      "Average loss at step 233600: 392.103685\n",
      "Average loss at step 233700: 390.107005\n",
      "Average loss at step 233800: 392.893213\n",
      "Average loss at step 233900: 391.435529\n",
      "Average loss at step 234000: 387.527576\n",
      "Graph 234: 36 nodes\n",
      "Average loss at step 234100: 419.306857\n",
      "Average loss at step 234200: 417.555475\n",
      "Average loss at step 234300: 415.410746\n",
      "Average loss at step 234400: 412.930753\n",
      "Average loss at step 234500: 412.312712\n",
      "Average loss at step 234600: 416.935689\n",
      "Average loss at step 234700: 413.990540\n",
      "Average loss at step 234800: 413.046690\n",
      "Average loss at step 234900: 414.247816\n",
      "Average loss at step 235000: 414.391724\n",
      "Graph 235: 21 nodes\n",
      "Average loss at step 235100: 446.894536\n",
      "Average loss at step 235200: 444.700511\n",
      "Average loss at step 235300: 445.420672\n",
      "Average loss at step 235400: 444.499308\n",
      "Average loss at step 235500: 441.601612\n",
      "Average loss at step 235600: 443.143190\n",
      "Average loss at step 235700: 441.356265\n",
      "Average loss at step 235800: 442.083654\n",
      "Average loss at step 235900: 440.939025\n",
      "Average loss at step 236000: 440.110091\n",
      "Graph 236: 6 nodes\n",
      "Average loss at step 236100: 485.050608\n",
      "Average loss at step 236200: 480.205081\n",
      "Average loss at step 236300: 478.299835\n",
      "Average loss at step 236400: 476.842846\n",
      "Average loss at step 236500: 477.649496\n",
      "Average loss at step 236600: 475.822754\n",
      "Average loss at step 236700: 476.617316\n",
      "Average loss at step 236800: 475.113770\n",
      "Average loss at step 236900: 475.673127\n",
      "Average loss at step 237000: 475.757102\n",
      "Graph 237: 22 nodes\n",
      "Average loss at step 237100: 411.639034\n",
      "Average loss at step 237200: 404.869684\n",
      "Average loss at step 237300: 407.244290\n",
      "Average loss at step 237400: 407.815886\n",
      "Average loss at step 237500: 404.803916\n",
      "Average loss at step 237600: 408.213669\n",
      "Average loss at step 237700: 403.508607\n",
      "Average loss at step 237800: 403.111690\n",
      "Average loss at step 237900: 403.279506\n",
      "Average loss at step 238000: 404.985424\n",
      "Graph 238: 18 nodes\n",
      "Average loss at step 238100: 407.443571\n",
      "Average loss at step 238200: 403.739283\n",
      "Average loss at step 238300: 400.016752\n",
      "Average loss at step 238400: 403.622431\n",
      "Average loss at step 238500: 396.458302\n",
      "Average loss at step 238600: 403.699069\n",
      "Average loss at step 238700: 402.141056\n",
      "Average loss at step 238800: 401.980740\n",
      "Average loss at step 238900: 400.275017\n",
      "Average loss at step 239000: 402.103813\n",
      "Graph 239: 18 nodes\n",
      "Average loss at step 239100: 403.053063\n",
      "Average loss at step 239200: 400.019009\n",
      "Average loss at step 239300: 399.747903\n",
      "Average loss at step 239400: 400.054467\n",
      "Average loss at step 239500: 402.396671\n",
      "Average loss at step 239600: 401.102196\n",
      "Average loss at step 239700: 400.019305\n",
      "Average loss at step 239800: 399.088758\n",
      "Average loss at step 239900: 400.579244\n",
      "Average loss at step 240000: 398.489269\n",
      "Graph 240: 34 nodes\n",
      "Average loss at step 240100: 447.403836\n",
      "Average loss at step 240200: 447.791949\n",
      "Average loss at step 240300: 444.713053\n",
      "Average loss at step 240400: 446.533271\n",
      "Average loss at step 240500: 446.190155\n",
      "Average loss at step 240600: 444.515550\n",
      "Average loss at step 240700: 442.336646\n",
      "Average loss at step 240800: 444.421504\n",
      "Average loss at step 240900: 445.110907\n",
      "Average loss at step 241000: 444.127621\n",
      "Time: 50.2428839207\n",
      "Graph 241: 32 nodes\n",
      "Average loss at step 241100: 450.556754\n",
      "Average loss at step 241200: 449.987161\n",
      "Average loss at step 241300: 447.506107\n",
      "Average loss at step 241400: 448.586203\n",
      "Average loss at step 241500: 451.079584\n",
      "Average loss at step 241600: 450.358743\n",
      "Average loss at step 241700: 449.628482\n",
      "Average loss at step 241800: 448.519083\n",
      "Average loss at step 241900: 447.115483\n",
      "Average loss at step 242000: 448.219644\n",
      "Graph 242: 33 nodes\n",
      "Average loss at step 242100: 452.250569\n",
      "Average loss at step 242200: 451.616256\n",
      "Average loss at step 242300: 448.912782\n",
      "Average loss at step 242400: 449.987510\n",
      "Average loss at step 242500: 452.008358\n",
      "Average loss at step 242600: 449.290933\n",
      "Average loss at step 242700: 448.655437\n",
      "Average loss at step 242800: 449.739128\n",
      "Average loss at step 242900: 448.864723\n",
      "Average loss at step 243000: 446.878610\n",
      "Graph 243: 28 nodes\n",
      "Average loss at step 243100: 421.266131\n",
      "Average loss at step 243200: 419.733223\n",
      "Average loss at step 243300: 419.728416\n",
      "Average loss at step 243400: 418.277214\n",
      "Average loss at step 243500: 418.072366\n",
      "Average loss at step 243600: 414.485062\n",
      "Average loss at step 243700: 419.470348\n",
      "Average loss at step 243800: 416.603594\n",
      "Average loss at step 243900: 418.219265\n",
      "Average loss at step 244000: 416.683953\n",
      "Graph 244: 40 nodes\n",
      "Average loss at step 244100: 429.630927\n",
      "Average loss at step 244200: 422.967692\n",
      "Average loss at step 244300: 425.603956\n",
      "Average loss at step 244400: 423.329226\n",
      "Average loss at step 244500: 426.256611\n",
      "Average loss at step 244600: 426.759464\n",
      "Average loss at step 244700: 427.130112\n",
      "Average loss at step 244800: 428.005826\n",
      "Average loss at step 244900: 423.732666\n",
      "Average loss at step 245000: 424.776021\n",
      "Graph 245: 42 nodes\n",
      "Average loss at step 245100: 451.938353\n",
      "Average loss at step 245200: 448.686115\n",
      "Average loss at step 245300: 449.010520\n",
      "Average loss at step 245400: 449.404397\n",
      "Average loss at step 245500: 451.866096\n",
      "Average loss at step 245600: 448.754420\n",
      "Average loss at step 245700: 447.220443\n",
      "Average loss at step 245800: 448.804258\n",
      "Average loss at step 245900: 449.039078\n",
      "Average loss at step 246000: 451.094151\n",
      "Graph 246: 39 nodes\n",
      "Average loss at step 246100: 374.611593\n",
      "Average loss at step 246200: 370.620911\n",
      "Average loss at step 246300: 368.749423\n",
      "Average loss at step 246400: 367.678812\n",
      "Average loss at step 246500: 367.611781\n",
      "Average loss at step 246600: 368.359901\n",
      "Average loss at step 246700: 368.191141\n",
      "Average loss at step 246800: 366.853825\n",
      "Average loss at step 246900: 365.724109\n",
      "Average loss at step 247000: 363.997468\n",
      "Graph 247: 28 nodes\n",
      "Average loss at step 247100: 397.679764\n",
      "Average loss at step 247200: 392.600481\n",
      "Average loss at step 247300: 389.694110\n",
      "Average loss at step 247400: 392.071360\n",
      "Average loss at step 247500: 387.744541\n",
      "Average loss at step 247600: 390.937464\n",
      "Average loss at step 247700: 389.317578\n",
      "Average loss at step 247800: 390.688481\n",
      "Average loss at step 247900: 392.048080\n",
      "Average loss at step 248000: 388.481226\n",
      "Graph 248: 29 nodes\n",
      "Average loss at step 248100: 402.251855\n",
      "Average loss at step 248200: 401.687477\n",
      "Average loss at step 248300: 399.729552\n",
      "Average loss at step 248400: 398.906392\n",
      "Average loss at step 248500: 400.139892\n",
      "Average loss at step 248600: 397.694611\n",
      "Average loss at step 248700: 398.300454\n",
      "Average loss at step 248800: 400.427709\n",
      "Average loss at step 248900: 400.640127\n",
      "Average loss at step 249000: 398.433011\n",
      "Graph 249: 30 nodes\n",
      "Average loss at step 249100: 393.556640\n",
      "Average loss at step 249200: 388.863063\n",
      "Average loss at step 249300: 388.323137\n",
      "Average loss at step 249400: 387.478081\n",
      "Average loss at step 249500: 388.282365\n",
      "Average loss at step 249600: 385.100948\n",
      "Average loss at step 249700: 385.636542\n",
      "Average loss at step 249800: 386.579517\n",
      "Average loss at step 249900: 388.582681\n",
      "Average loss at step 250000: 385.917633\n",
      "Graph 250: 14 nodes\n",
      "Average loss at step 250100: 466.212063\n",
      "Average loss at step 250200: 462.828283\n",
      "Average loss at step 250300: 462.226800\n",
      "Average loss at step 250400: 460.874692\n",
      "Average loss at step 250500: 461.068871\n",
      "Average loss at step 250600: 461.310272\n",
      "Average loss at step 250700: 461.233596\n",
      "Average loss at step 250800: 459.965847\n",
      "Average loss at step 250900: 461.316274\n",
      "Average loss at step 251000: 460.075160\n",
      "Time: 50.5966050625\n",
      "Graph 251: 32 nodes\n",
      "Average loss at step 251100: 450.666373\n",
      "Average loss at step 251200: 450.618178\n",
      "Average loss at step 251300: 449.629357\n",
      "Average loss at step 251400: 446.153372\n",
      "Average loss at step 251500: 448.071726\n",
      "Average loss at step 251600: 448.913045\n",
      "Average loss at step 251700: 447.719100\n",
      "Average loss at step 251800: 447.466044\n",
      "Average loss at step 251900: 448.932131\n",
      "Average loss at step 252000: 446.560461\n",
      "Graph 252: 35 nodes\n",
      "Average loss at step 252100: 407.243706\n",
      "Average loss at step 252200: 405.220602\n",
      "Average loss at step 252300: 405.484785\n",
      "Average loss at step 252400: 403.894398\n",
      "Average loss at step 252500: 403.002120\n",
      "Average loss at step 252600: 401.188133\n",
      "Average loss at step 252700: 405.966169\n",
      "Average loss at step 252800: 404.544071\n",
      "Average loss at step 252900: 403.834443\n",
      "Average loss at step 253000: 403.349617\n",
      "Graph 253: 24 nodes\n",
      "Average loss at step 253100: 380.422457\n",
      "Average loss at step 253200: 374.278233\n",
      "Average loss at step 253300: 371.731003\n",
      "Average loss at step 253400: 371.100233\n",
      "Average loss at step 253500: 371.763765\n",
      "Average loss at step 253600: 372.165239\n",
      "Average loss at step 253700: 371.823330\n",
      "Average loss at step 253800: 371.415595\n",
      "Average loss at step 253900: 368.465833\n",
      "Average loss at step 254000: 370.658979\n",
      "Graph 254: 17 nodes\n",
      "Average loss at step 254100: 460.723101\n",
      "Average loss at step 254200: 455.837807\n",
      "Average loss at step 254300: 455.089579\n",
      "Average loss at step 254400: 456.501089\n",
      "Average loss at step 254500: 455.558542\n",
      "Average loss at step 254600: 454.571098\n",
      "Average loss at step 254700: 454.612900\n",
      "Average loss at step 254800: 455.417726\n",
      "Average loss at step 254900: 455.781255\n",
      "Average loss at step 255000: 452.720812\n",
      "Graph 255: 18 nodes\n",
      "Average loss at step 255100: 462.413107\n",
      "Average loss at step 255200: 459.097322\n",
      "Average loss at step 255300: 459.004336\n",
      "Average loss at step 255400: 457.886846\n",
      "Average loss at step 255500: 458.145674\n",
      "Average loss at step 255600: 459.701869\n",
      "Average loss at step 255700: 458.261634\n",
      "Average loss at step 255800: 459.331933\n",
      "Average loss at step 255900: 458.762208\n",
      "Average loss at step 256000: 459.340259\n",
      "Graph 256: 16 nodes\n",
      "Average loss at step 256100: 463.928760\n",
      "Average loss at step 256200: 464.384203\n",
      "Average loss at step 256300: 462.607390\n",
      "Average loss at step 256400: 463.470717\n",
      "Average loss at step 256500: 462.448468\n",
      "Average loss at step 256600: 462.694667\n",
      "Average loss at step 256700: 462.329441\n",
      "Average loss at step 256800: 461.148880\n",
      "Average loss at step 256900: 462.729320\n",
      "Average loss at step 257000: 460.604571\n",
      "Graph 257: 19 nodes\n",
      "Average loss at step 257100: 456.744014\n",
      "Average loss at step 257200: 457.832625\n",
      "Average loss at step 257300: 455.991180\n",
      "Average loss at step 257400: 456.847641\n",
      "Average loss at step 257500: 455.082802\n",
      "Average loss at step 257600: 454.274051\n",
      "Average loss at step 257700: 455.645348\n",
      "Average loss at step 257800: 458.421048\n",
      "Average loss at step 257900: 455.988402\n",
      "Average loss at step 258000: 455.717512\n",
      "Graph 258: 19 nodes\n",
      "Average loss at step 258100: 457.190910\n",
      "Average loss at step 258200: 455.279541\n",
      "Average loss at step 258300: 456.295424\n",
      "Average loss at step 258400: 455.446193\n",
      "Average loss at step 258500: 455.985126\n",
      "Average loss at step 258600: 453.566758\n",
      "Average loss at step 258700: 454.330245\n",
      "Average loss at step 258800: 455.253598\n",
      "Average loss at step 258900: 455.258625\n",
      "Average loss at step 259000: 455.830379\n",
      "Graph 259: 21 nodes\n",
      "Average loss at step 259100: 455.479503\n",
      "Average loss at step 259200: 451.484215\n",
      "Average loss at step 259300: 451.881316\n",
      "Average loss at step 259400: 450.930926\n",
      "Average loss at step 259500: 449.631080\n",
      "Average loss at step 259600: 450.442904\n",
      "Average loss at step 259700: 449.244882\n",
      "Average loss at step 259800: 449.003046\n",
      "Average loss at step 259900: 452.235194\n",
      "Average loss at step 260000: 449.438211\n",
      "Graph 260: 23 nodes\n",
      "Average loss at step 260100: 432.502657\n",
      "Average loss at step 260200: 431.023075\n",
      "Average loss at step 260300: 428.892256\n",
      "Average loss at step 260400: 427.253648\n",
      "Average loss at step 260500: 430.696618\n",
      "Average loss at step 260600: 426.559370\n",
      "Average loss at step 260700: 428.701285\n",
      "Average loss at step 260800: 430.511941\n",
      "Average loss at step 260900: 430.715021\n",
      "Average loss at step 261000: 428.313892\n",
      "Time: 53.5275409222\n",
      "Graph 261: 25 nodes\n",
      "Average loss at step 261100: 442.496880\n",
      "Average loss at step 261200: 438.108165\n",
      "Average loss at step 261300: 439.055894\n",
      "Average loss at step 261400: 438.497427\n",
      "Average loss at step 261500: 436.386561\n",
      "Average loss at step 261600: 435.521329\n",
      "Average loss at step 261700: 437.276663\n",
      "Average loss at step 261800: 439.301710\n",
      "Average loss at step 261900: 439.535918\n",
      "Average loss at step 262000: 437.265253\n",
      "Graph 262: 27 nodes\n",
      "Average loss at step 262100: 431.212859\n",
      "Average loss at step 262200: 434.326957\n",
      "Average loss at step 262300: 431.314162\n",
      "Average loss at step 262400: 431.548886\n",
      "Average loss at step 262500: 427.392354\n",
      "Average loss at step 262600: 431.243597\n",
      "Average loss at step 262700: 429.949656\n",
      "Average loss at step 262800: 429.528217\n",
      "Average loss at step 262900: 429.237216\n",
      "Average loss at step 263000: 430.858031\n",
      "Graph 263: 24 nodes\n",
      "Average loss at step 263100: 445.425700\n",
      "Average loss at step 263200: 443.431956\n",
      "Average loss at step 263300: 440.982809\n",
      "Average loss at step 263400: 441.369081\n",
      "Average loss at step 263500: 444.100343\n",
      "Average loss at step 263600: 443.908580\n",
      "Average loss at step 263700: 442.903653\n",
      "Average loss at step 263800: 443.306349\n",
      "Average loss at step 263900: 441.239460\n",
      "Average loss at step 264000: 443.483070\n",
      "Graph 264: 24 nodes\n",
      "Average loss at step 264100: 438.465687\n",
      "Average loss at step 264200: 436.373370\n",
      "Average loss at step 264300: 437.937151\n",
      "Average loss at step 264400: 436.987413\n",
      "Average loss at step 264500: 436.666975\n",
      "Average loss at step 264600: 436.192310\n",
      "Average loss at step 264700: 435.307341\n",
      "Average loss at step 264800: 437.206193\n",
      "Average loss at step 264900: 437.398284\n",
      "Average loss at step 265000: 436.590885\n",
      "Graph 265: 32 nodes\n",
      "Average loss at step 265100: 448.301715\n",
      "Average loss at step 265200: 443.568518\n",
      "Average loss at step 265300: 445.364581\n",
      "Average loss at step 265400: 442.791063\n",
      "Average loss at step 265500: 446.807591\n",
      "Average loss at step 265600: 443.821835\n",
      "Average loss at step 265700: 445.220034\n",
      "Average loss at step 265800: 444.686096\n",
      "Average loss at step 265900: 443.527691\n",
      "Average loss at step 266000: 445.316826\n",
      "Graph 266: 31 nodes\n",
      "Average loss at step 266100: 447.695487\n",
      "Average loss at step 266200: 446.377999\n",
      "Average loss at step 266300: 448.111378\n",
      "Average loss at step 266400: 446.053817\n",
      "Average loss at step 266500: 446.217011\n",
      "Average loss at step 266600: 447.162410\n",
      "Average loss at step 266700: 447.107966\n",
      "Average loss at step 266800: 445.551737\n",
      "Average loss at step 266900: 444.940543\n",
      "Average loss at step 267000: 446.475615\n",
      "Graph 267: 33 nodes\n",
      "Average loss at step 267100: 422.226127\n",
      "Average loss at step 267200: 421.015392\n",
      "Average loss at step 267300: 420.775539\n",
      "Average loss at step 267400: 421.017126\n",
      "Average loss at step 267500: 422.549844\n",
      "Average loss at step 267600: 423.271127\n",
      "Average loss at step 267700: 421.102093\n",
      "Average loss at step 267800: 419.269982\n",
      "Average loss at step 267900: 419.529688\n",
      "Average loss at step 268000: 418.214241\n",
      "Graph 268: 34 nodes\n",
      "Average loss at step 268100: 422.091278\n",
      "Average loss at step 268200: 421.474884\n",
      "Average loss at step 268300: 421.302698\n",
      "Average loss at step 268400: 419.524638\n",
      "Average loss at step 268500: 419.949799\n",
      "Average loss at step 268600: 421.045870\n",
      "Average loss at step 268700: 423.978744\n",
      "Average loss at step 268800: 424.090850\n",
      "Average loss at step 268900: 422.369145\n",
      "Average loss at step 269000: 421.763064\n",
      "Graph 269: 32 nodes\n",
      "Average loss at step 269100: 451.465310\n",
      "Average loss at step 269200: 449.071224\n",
      "Average loss at step 269300: 449.470023\n",
      "Average loss at step 269400: 448.033409\n",
      "Average loss at step 269500: 449.168580\n",
      "Average loss at step 269600: 448.315095\n",
      "Average loss at step 269700: 450.257721\n",
      "Average loss at step 269800: 448.910867\n",
      "Average loss at step 269900: 448.725399\n",
      "Average loss at step 270000: 451.210691\n",
      "Graph 270: 32 nodes\n",
      "Average loss at step 270100: 431.660302\n",
      "Average loss at step 270200: 428.847832\n",
      "Average loss at step 270300: 430.601259\n",
      "Average loss at step 270400: 427.301556\n",
      "Average loss at step 270500: 428.124184\n",
      "Average loss at step 270600: 430.738580\n",
      "Average loss at step 270700: 427.432216\n",
      "Average loss at step 270800: 429.823386\n",
      "Average loss at step 270900: 426.337000\n",
      "Average loss at step 271000: 429.929542\n",
      "Time: 53.8344709873\n",
      "Graph 271: 44 nodes\n",
      "Average loss at step 271100: 411.788612\n",
      "Average loss at step 271200: 406.121509\n",
      "Average loss at step 271300: 405.950263\n",
      "Average loss at step 271400: 403.494691\n",
      "Average loss at step 271500: 402.258773\n",
      "Average loss at step 271600: 402.193385\n",
      "Average loss at step 271700: 403.020432\n",
      "Average loss at step 271800: 400.483971\n",
      "Average loss at step 271900: 400.766906\n",
      "Average loss at step 272000: 400.277948\n",
      "Graph 272: 40 nodes\n",
      "Average loss at step 272100: 440.784092\n",
      "Average loss at step 272200: 438.260607\n",
      "Average loss at step 272300: 436.629459\n",
      "Average loss at step 272400: 436.435625\n",
      "Average loss at step 272500: 437.352439\n",
      "Average loss at step 272600: 436.478302\n",
      "Average loss at step 272700: 437.645078\n",
      "Average loss at step 272800: 434.475187\n",
      "Average loss at step 272900: 434.909092\n",
      "Average loss at step 273000: 436.362223\n",
      "Graph 273: 40 nodes\n",
      "Average loss at step 273100: 426.129121\n",
      "Average loss at step 273200: 424.027551\n",
      "Average loss at step 273300: 421.595243\n",
      "Average loss at step 273400: 421.691990\n",
      "Average loss at step 273500: 420.563125\n",
      "Average loss at step 273600: 421.544019\n",
      "Average loss at step 273700: 418.726890\n",
      "Average loss at step 273800: 420.626147\n",
      "Average loss at step 273900: 419.080323\n",
      "Average loss at step 274000: 418.022157\n",
      "Graph 274: 38 nodes\n",
      "Average loss at step 274100: 432.741246\n",
      "Average loss at step 274200: 426.949213\n",
      "Average loss at step 274300: 428.293294\n",
      "Average loss at step 274400: 427.611199\n",
      "Average loss at step 274500: 427.247154\n",
      "Average loss at step 274600: 426.387149\n",
      "Average loss at step 274700: 423.229515\n",
      "Average loss at step 274800: 424.264178\n",
      "Average loss at step 274900: 422.841689\n",
      "Average loss at step 275000: 425.820140\n",
      "Graph 275: 30 nodes\n",
      "Average loss at step 275100: 389.512982\n",
      "Average loss at step 275200: 387.029988\n",
      "Average loss at step 275300: 386.249100\n",
      "Average loss at step 275400: 383.049426\n",
      "Average loss at step 275500: 382.786387\n",
      "Average loss at step 275600: 384.954300\n",
      "Average loss at step 275700: 387.216717\n",
      "Average loss at step 275800: 381.433613\n",
      "Average loss at step 275900: 381.101232\n",
      "Average loss at step 276000: 385.091385\n",
      "Graph 276: 38 nodes\n",
      "Average loss at step 276100: 417.456511\n",
      "Average loss at step 276200: 414.784062\n",
      "Average loss at step 276300: 412.381127\n",
      "Average loss at step 276400: 412.681188\n",
      "Average loss at step 276500: 409.378747\n",
      "Average loss at step 276600: 413.258223\n",
      "Average loss at step 276700: 414.238734\n",
      "Average loss at step 276800: 413.729628\n",
      "Average loss at step 276900: 412.474787\n",
      "Average loss at step 277000: 411.847800\n",
      "Graph 277: 44 nodes\n",
      "Average loss at step 277100: 408.095883\n",
      "Average loss at step 277200: 406.436576\n",
      "Average loss at step 277300: 403.902167\n",
      "Average loss at step 277400: 404.639264\n",
      "Average loss at step 277500: 404.296115\n",
      "Average loss at step 277600: 405.041599\n",
      "Average loss at step 277700: 407.803024\n",
      "Average loss at step 277800: 406.515771\n",
      "Average loss at step 277900: 404.317953\n",
      "Average loss at step 278000: 401.711396\n",
      "Graph 278: 60 nodes\n",
      "Average loss at step 278100: 372.143652\n",
      "Average loss at step 278200: 369.806630\n",
      "Average loss at step 278300: 369.948999\n",
      "Average loss at step 278400: 367.849639\n",
      "Average loss at step 278500: 365.440040\n",
      "Average loss at step 278600: 364.717134\n",
      "Average loss at step 278700: 366.852797\n",
      "Average loss at step 278800: 362.587524\n",
      "Average loss at step 278900: 367.065421\n",
      "Average loss at step 279000: 366.179969\n",
      "Graph 279: 28 nodes\n",
      "Average loss at step 279100: 456.974705\n",
      "Average loss at step 279200: 452.676898\n",
      "Average loss at step 279300: 452.927507\n",
      "Average loss at step 279400: 454.480688\n",
      "Average loss at step 279500: 451.789654\n",
      "Average loss at step 279600: 453.536255\n",
      "Average loss at step 279700: 451.658607\n",
      "Average loss at step 279800: 452.250389\n",
      "Average loss at step 279900: 453.894915\n",
      "Average loss at step 280000: 452.962362\n",
      "Graph 280: 16 nodes\n",
      "Average loss at step 280100: 458.609209\n",
      "Average loss at step 280200: 457.493156\n",
      "Average loss at step 280300: 454.658888\n",
      "Average loss at step 280400: 452.906559\n",
      "Average loss at step 280500: 455.647544\n",
      "Average loss at step 280600: 456.796435\n",
      "Average loss at step 280700: 456.126915\n",
      "Average loss at step 280800: 456.626267\n",
      "Average loss at step 280900: 454.133247\n",
      "Average loss at step 281000: 455.046717\n",
      "Time: 52.7755060196\n",
      "Graph 281: 50 nodes\n",
      "Average loss at step 281100: 389.777192\n",
      "Average loss at step 281200: 388.122929\n",
      "Average loss at step 281300: 383.367446\n",
      "Average loss at step 281400: 387.954407\n",
      "Average loss at step 281500: 383.300283\n",
      "Average loss at step 281600: 382.672197\n",
      "Average loss at step 281700: 385.142937\n",
      "Average loss at step 281800: 382.886351\n",
      "Average loss at step 281900: 383.501002\n",
      "Average loss at step 282000: 380.815248\n",
      "Graph 282: 52 nodes\n",
      "Average loss at step 282100: 383.232454\n",
      "Average loss at step 282200: 380.567143\n",
      "Average loss at step 282300: 380.237423\n",
      "Average loss at step 282400: 378.396870\n",
      "Average loss at step 282500: 375.880680\n",
      "Average loss at step 282600: 380.044185\n",
      "Average loss at step 282700: 381.300911\n",
      "Average loss at step 282800: 378.079302\n",
      "Average loss at step 282900: 377.011453\n",
      "Average loss at step 283000: 379.801788\n",
      "Graph 283: 48 nodes\n",
      "Average loss at step 283100: 383.712782\n",
      "Average loss at step 283200: 384.156409\n",
      "Average loss at step 283300: 382.359709\n",
      "Average loss at step 283400: 383.251481\n",
      "Average loss at step 283500: 383.683473\n",
      "Average loss at step 283600: 384.524672\n",
      "Average loss at step 283700: 382.430410\n",
      "Average loss at step 283800: 382.023883\n",
      "Average loss at step 283900: 382.439897\n",
      "Average loss at step 284000: 382.776953\n",
      "Graph 284: 50 nodes\n",
      "Average loss at step 284100: 391.630311\n",
      "Average loss at step 284200: 388.783452\n",
      "Average loss at step 284300: 389.458125\n",
      "Average loss at step 284400: 388.048934\n",
      "Average loss at step 284500: 387.789304\n",
      "Average loss at step 284600: 387.763980\n",
      "Average loss at step 284700: 389.311046\n",
      "Average loss at step 284800: 389.232951\n",
      "Average loss at step 284900: 386.873633\n",
      "Average loss at step 285000: 387.301906\n",
      "Graph 285: 42 nodes\n",
      "Average loss at step 285100: 454.535290\n",
      "Average loss at step 285200: 451.230544\n",
      "Average loss at step 285300: 450.444658\n",
      "Average loss at step 285400: 449.638406\n",
      "Average loss at step 285500: 448.280989\n",
      "Average loss at step 285600: 450.462159\n",
      "Average loss at step 285700: 449.964166\n",
      "Average loss at step 285800: 447.125753\n",
      "Average loss at step 285900: 448.506763\n",
      "Average loss at step 286000: 449.532335\n",
      "Graph 286: 43 nodes\n",
      "Average loss at step 286100: 455.201272\n",
      "Average loss at step 286200: 454.622585\n",
      "Average loss at step 286300: 454.529291\n",
      "Average loss at step 286400: 452.916100\n",
      "Average loss at step 286500: 451.841983\n",
      "Average loss at step 286600: 454.221003\n",
      "Average loss at step 286700: 452.940370\n",
      "Average loss at step 286800: 451.789762\n",
      "Average loss at step 286900: 452.718777\n",
      "Average loss at step 287000: 452.741704\n",
      "Graph 287: 41 nodes\n",
      "Average loss at step 287100: 455.237002\n",
      "Average loss at step 287200: 453.450377\n",
      "Average loss at step 287300: 452.381549\n",
      "Average loss at step 287400: 450.441133\n",
      "Average loss at step 287500: 451.899387\n",
      "Average loss at step 287600: 450.789062\n",
      "Average loss at step 287700: 451.516668\n",
      "Average loss at step 287800: 449.822602\n",
      "Average loss at step 287900: 451.493977\n",
      "Average loss at step 288000: 449.365780\n",
      "Graph 288: 39 nodes\n",
      "Average loss at step 288100: 419.636842\n",
      "Average loss at step 288200: 417.878285\n",
      "Average loss at step 288300: 418.840184\n",
      "Average loss at step 288400: 416.400299\n",
      "Average loss at step 288500: 420.554006\n",
      "Average loss at step 288600: 418.213522\n",
      "Average loss at step 288700: 418.063455\n",
      "Average loss at step 288800: 420.537330\n",
      "Average loss at step 288900: 417.199568\n",
      "Average loss at step 289000: 417.029397\n",
      "Graph 289: 38 nodes\n",
      "Average loss at step 289100: 446.826471\n",
      "Average loss at step 289200: 444.407872\n",
      "Average loss at step 289300: 444.435103\n",
      "Average loss at step 289400: 446.328454\n",
      "Average loss at step 289500: 443.188548\n",
      "Average loss at step 289600: 444.811192\n",
      "Average loss at step 289700: 445.910861\n",
      "Average loss at step 289800: 444.759008\n",
      "Average loss at step 289900: 445.084606\n",
      "Average loss at step 290000: 445.046612\n",
      "Graph 290: 62 nodes\n",
      "Average loss at step 290100: 396.337281\n",
      "Average loss at step 290200: 391.756797\n",
      "Average loss at step 290300: 389.496876\n",
      "Average loss at step 290400: 389.837814\n",
      "Average loss at step 290500: 389.392532\n",
      "Average loss at step 290600: 385.438237\n",
      "Average loss at step 290700: 386.424885\n",
      "Average loss at step 290800: 387.553805\n",
      "Average loss at step 290900: 386.250160\n",
      "Average loss at step 291000: 388.654655\n",
      "Time: 53.2627701759\n",
      "Graph 291: 60 nodes\n",
      "Average loss at step 291100: 392.367697\n",
      "Average loss at step 291200: 390.460652\n",
      "Average loss at step 291300: 392.061618\n",
      "Average loss at step 291400: 388.754333\n",
      "Average loss at step 291500: 389.481166\n",
      "Average loss at step 291600: 386.115841\n",
      "Average loss at step 291700: 389.811323\n",
      "Average loss at step 291800: 389.407441\n",
      "Average loss at step 291900: 387.619428\n",
      "Average loss at step 292000: 387.351486\n",
      "Graph 292: 96 nodes\n",
      "Average loss at step 292100: 347.447554\n",
      "Average loss at step 292200: 343.519658\n",
      "Average loss at step 292300: 343.614215\n",
      "Average loss at step 292400: 342.499319\n",
      "Average loss at step 292500: 339.492244\n",
      "Average loss at step 292600: 339.675168\n",
      "Average loss at step 292700: 341.103558\n",
      "Average loss at step 292800: 336.943111\n",
      "Average loss at step 292900: 337.694737\n",
      "Average loss at step 293000: 342.772537\n",
      "Graph 293: 54 nodes\n",
      "Average loss at step 293100: 376.989679\n",
      "Average loss at step 293200: 378.387767\n",
      "Average loss at step 293300: 375.598715\n",
      "Average loss at step 293400: 375.318920\n",
      "Average loss at step 293500: 377.567492\n",
      "Average loss at step 293600: 374.850071\n",
      "Average loss at step 293700: 375.590823\n",
      "Average loss at step 293800: 373.886234\n",
      "Average loss at step 293900: 372.628845\n",
      "Average loss at step 294000: 373.791112\n",
      "Graph 294: 123 nodes\n",
      "Average loss at step 294100: 334.845213\n",
      "Average loss at step 294200: 326.251669\n",
      "Average loss at step 294300: 326.235451\n",
      "Average loss at step 294400: 325.037844\n",
      "Average loss at step 294500: 326.742446\n",
      "Average loss at step 294600: 324.696992\n",
      "Average loss at step 294700: 323.800233\n",
      "Average loss at step 294800: 324.658554\n",
      "Average loss at step 294900: 324.833158\n",
      "Average loss at step 295000: 325.408518\n",
      "Graph 295: 125 nodes\n",
      "Average loss at step 295100: 329.744296\n",
      "Average loss at step 295200: 329.108449\n",
      "Average loss at step 295300: 327.672316\n",
      "Average loss at step 295400: 326.865931\n",
      "Average loss at step 295500: 327.620851\n",
      "Average loss at step 295600: 329.774400\n",
      "Average loss at step 295700: 325.343141\n",
      "Average loss at step 295800: 329.095451\n",
      "Average loss at step 295900: 329.677255\n",
      "Average loss at step 296000: 331.070151\n",
      "Graph 296: 121 nodes\n",
      "Average loss at step 296100: 341.302946\n",
      "Average loss at step 296200: 340.703071\n",
      "Average loss at step 296300: 337.693827\n",
      "Average loss at step 296400: 338.757842\n",
      "Average loss at step 296500: 338.046838\n",
      "Average loss at step 296600: 338.333649\n",
      "Average loss at step 296700: 338.954828\n",
      "Average loss at step 296800: 339.039238\n",
      "Average loss at step 296900: 339.357607\n",
      "Average loss at step 297000: 338.905515\n",
      "Graph 297: 24 nodes\n",
      "Average loss at step 297100: 430.345076\n",
      "Average loss at step 297200: 425.164291\n",
      "Average loss at step 297300: 424.138691\n",
      "Average loss at step 297400: 419.273637\n",
      "Average loss at step 297500: 420.393910\n",
      "Average loss at step 297600: 420.080912\n",
      "Average loss at step 297700: 416.169086\n",
      "Average loss at step 297800: 420.041651\n",
      "Average loss at step 297900: 418.705250\n",
      "Average loss at step 298000: 421.067629\n",
      "Graph 298: 41 nodes\n",
      "Average loss at step 298100: 447.649516\n",
      "Average loss at step 298200: 446.476702\n",
      "Average loss at step 298300: 445.169768\n",
      "Average loss at step 298400: 444.802144\n",
      "Average loss at step 298500: 445.326898\n",
      "Average loss at step 298600: 447.091968\n",
      "Average loss at step 298700: 445.865409\n",
      "Average loss at step 298800: 443.236775\n",
      "Average loss at step 298900: 445.162816\n",
      "Average loss at step 299000: 445.444088\n",
      "Graph 299: 49 nodes\n",
      "Average loss at step 299100: 383.284002\n",
      "Average loss at step 299200: 378.076404\n",
      "Average loss at step 299300: 374.337562\n",
      "Average loss at step 299400: 376.920984\n",
      "Average loss at step 299500: 376.979439\n",
      "Average loss at step 299600: 371.452573\n",
      "Average loss at step 299700: 375.863331\n",
      "Average loss at step 299800: 373.046472\n",
      "Average loss at step 299900: 372.468218\n",
      "Average loss at step 300000: 375.642293\n",
      "Graph 300: 44 nodes\n",
      "Average loss at step 300100: 389.156143\n",
      "Average loss at step 300200: 389.319862\n",
      "Average loss at step 300300: 389.630005\n",
      "Average loss at step 300400: 389.105212\n",
      "Average loss at step 300500: 386.212054\n",
      "Average loss at step 300600: 387.856989\n",
      "Average loss at step 300700: 386.173661\n",
      "Average loss at step 300800: 388.640368\n",
      "Average loss at step 300900: 390.091824\n",
      "Average loss at step 301000: 385.017099\n",
      "Time: 54.2872731686\n",
      "Graph 301: 42 nodes\n",
      "Average loss at step 301100: 391.793709\n",
      "Average loss at step 301200: 388.587577\n",
      "Average loss at step 301300: 390.677261\n",
      "Average loss at step 301400: 392.152177\n",
      "Average loss at step 301500: 387.973838\n",
      "Average loss at step 301600: 390.685065\n",
      "Average loss at step 301700: 390.527817\n",
      "Average loss at step 301800: 390.941244\n",
      "Average loss at step 301900: 389.414410\n",
      "Average loss at step 302000: 389.708194\n",
      "Graph 302: 41 nodes\n",
      "Average loss at step 302100: 389.124251\n",
      "Average loss at step 302200: 388.786353\n",
      "Average loss at step 302300: 389.792319\n",
      "Average loss at step 302400: 390.653208\n",
      "Average loss at step 302500: 386.818556\n",
      "Average loss at step 302600: 386.862075\n",
      "Average loss at step 302700: 387.570640\n",
      "Average loss at step 302800: 387.783899\n",
      "Average loss at step 302900: 388.439171\n",
      "Average loss at step 303000: 391.727520\n",
      "Graph 303: 42 nodes\n",
      "Average loss at step 303100: 393.697571\n",
      "Average loss at step 303200: 386.776060\n",
      "Average loss at step 303300: 389.770360\n",
      "Average loss at step 303400: 389.462964\n",
      "Average loss at step 303500: 387.859590\n",
      "Average loss at step 303600: 386.601450\n",
      "Average loss at step 303700: 387.441320\n",
      "Average loss at step 303800: 387.476314\n",
      "Average loss at step 303900: 386.870293\n",
      "Average loss at step 304000: 385.982682\n",
      "Graph 304: 46 nodes\n",
      "Average loss at step 304100: 391.894321\n",
      "Average loss at step 304200: 388.745389\n",
      "Average loss at step 304300: 385.211233\n",
      "Average loss at step 304400: 385.272871\n",
      "Average loss at step 304500: 387.090625\n",
      "Average loss at step 304600: 384.515849\n",
      "Average loss at step 304700: 387.505521\n",
      "Average loss at step 304800: 385.133964\n",
      "Average loss at step 304900: 388.069044\n",
      "Average loss at step 305000: 388.238851\n",
      "Graph 305: 55 nodes\n",
      "Average loss at step 305100: 395.081961\n",
      "Average loss at step 305200: 392.141287\n",
      "Average loss at step 305300: 391.994204\n",
      "Average loss at step 305400: 395.116779\n",
      "Average loss at step 305500: 393.642216\n",
      "Average loss at step 305600: 395.362490\n",
      "Average loss at step 305700: 394.429987\n",
      "Average loss at step 305800: 388.342992\n",
      "Average loss at step 305900: 391.674730\n",
      "Average loss at step 306000: 390.918722\n",
      "Graph 306: 50 nodes\n",
      "Average loss at step 306100: 383.829670\n",
      "Average loss at step 306200: 378.475706\n",
      "Average loss at step 306300: 377.896028\n",
      "Average loss at step 306400: 375.701266\n",
      "Average loss at step 306500: 381.700240\n",
      "Average loss at step 306600: 379.453225\n",
      "Average loss at step 306700: 377.371613\n",
      "Average loss at step 306800: 378.784487\n",
      "Average loss at step 306900: 377.755000\n",
      "Average loss at step 307000: 375.744140\n",
      "Graph 307: 42 nodes\n",
      "Average loss at step 307100: 389.382137\n",
      "Average loss at step 307200: 390.197893\n",
      "Average loss at step 307300: 390.165593\n",
      "Average loss at step 307400: 386.560665\n",
      "Average loss at step 307500: 386.945928\n",
      "Average loss at step 307600: 389.047937\n",
      "Average loss at step 307700: 393.137636\n",
      "Average loss at step 307800: 389.175007\n",
      "Average loss at step 307900: 386.095058\n",
      "Average loss at step 308000: 388.470666\n",
      "Graph 308: 48 nodes\n",
      "Average loss at step 308100: 386.318707\n",
      "Average loss at step 308200: 386.654484\n",
      "Average loss at step 308300: 382.158630\n",
      "Average loss at step 308400: 385.771635\n",
      "Average loss at step 308500: 383.914031\n",
      "Average loss at step 308600: 383.553265\n",
      "Average loss at step 308700: 388.494020\n",
      "Average loss at step 308800: 384.860564\n",
      "Average loss at step 308900: 383.679205\n",
      "Average loss at step 309000: 380.744688\n",
      "Graph 309: 51 nodes\n",
      "Average loss at step 309100: 375.671293\n",
      "Average loss at step 309200: 372.199035\n",
      "Average loss at step 309300: 369.739828\n",
      "Average loss at step 309400: 374.524887\n",
      "Average loss at step 309500: 373.508931\n",
      "Average loss at step 309600: 370.351471\n",
      "Average loss at step 309700: 371.033248\n",
      "Average loss at step 309800: 375.782492\n",
      "Average loss at step 309900: 370.069285\n",
      "Average loss at step 310000: 370.149293\n",
      "Graph 310: 24 nodes\n",
      "Average loss at step 310100: 392.795768\n",
      "Average loss at step 310200: 392.556701\n",
      "Average loss at step 310300: 390.135653\n",
      "Average loss at step 310400: 390.182635\n",
      "Average loss at step 310500: 387.246833\n",
      "Average loss at step 310600: 390.551241\n",
      "Average loss at step 310700: 389.797001\n",
      "Average loss at step 310800: 391.598647\n",
      "Average loss at step 310900: 388.979780\n",
      "Average loss at step 311000: 386.762256\n",
      "Time: 56.3178949356\n",
      "Graph 311: 51 nodes\n",
      "Average loss at step 311100: 404.980672\n",
      "Average loss at step 311200: 399.901343\n",
      "Average loss at step 311300: 398.051589\n",
      "Average loss at step 311400: 395.914885\n",
      "Average loss at step 311500: 400.390066\n",
      "Average loss at step 311600: 398.805509\n",
      "Average loss at step 311700: 398.483875\n",
      "Average loss at step 311800: 399.763755\n",
      "Average loss at step 311900: 398.212430\n",
      "Average loss at step 312000: 399.252301\n",
      "Graph 312: 52 nodes\n",
      "Average loss at step 312100: 373.648786\n",
      "Average loss at step 312200: 373.178350\n",
      "Average loss at step 312300: 371.082389\n",
      "Average loss at step 312400: 374.905717\n",
      "Average loss at step 312500: 374.006759\n",
      "Average loss at step 312600: 373.745292\n",
      "Average loss at step 312700: 370.498237\n",
      "Average loss at step 312800: 374.169424\n",
      "Average loss at step 312900: 372.101302\n",
      "Average loss at step 313000: 368.834264\n",
      "Graph 313: 22 nodes\n",
      "Average loss at step 313100: 416.915791\n",
      "Average loss at step 313200: 419.613925\n",
      "Average loss at step 313300: 415.162295\n",
      "Average loss at step 313400: 417.909050\n",
      "Average loss at step 313500: 416.373091\n",
      "Average loss at step 313600: 413.643643\n",
      "Average loss at step 313700: 416.278452\n",
      "Average loss at step 313800: 415.095521\n",
      "Average loss at step 313900: 418.086967\n",
      "Average loss at step 314000: 419.690270\n",
      "Graph 314: 44 nodes\n",
      "Average loss at step 314100: 403.144292\n",
      "Average loss at step 314200: 401.043226\n",
      "Average loss at step 314300: 399.659166\n",
      "Average loss at step 314400: 398.518137\n",
      "Average loss at step 314500: 400.101744\n",
      "Average loss at step 314600: 401.344091\n",
      "Average loss at step 314700: 401.729984\n",
      "Average loss at step 314800: 394.155648\n",
      "Average loss at step 314900: 399.490407\n",
      "Average loss at step 315000: 397.755128\n",
      "Graph 315: 44 nodes\n",
      "Average loss at step 315100: 406.404348\n",
      "Average loss at step 315200: 401.812849\n",
      "Average loss at step 315300: 402.344254\n",
      "Average loss at step 315400: 401.967058\n",
      "Average loss at step 315500: 401.041839\n",
      "Average loss at step 315600: 402.943621\n",
      "Average loss at step 315700: 400.739277\n",
      "Average loss at step 315800: 399.674837\n",
      "Average loss at step 315900: 402.101114\n",
      "Average loss at step 316000: 398.667607\n",
      "Graph 316: 31 nodes\n",
      "Average loss at step 316100: 426.423992\n",
      "Average loss at step 316200: 422.994772\n",
      "Average loss at step 316300: 423.271819\n",
      "Average loss at step 316400: 420.180204\n",
      "Average loss at step 316500: 425.705183\n",
      "Average loss at step 316600: 419.992996\n",
      "Average loss at step 316700: 424.896243\n",
      "Average loss at step 316800: 421.127429\n",
      "Average loss at step 316900: 421.685516\n",
      "Average loss at step 317000: 419.495281\n",
      "Graph 317: 27 nodes\n",
      "Average loss at step 317100: 439.089465\n",
      "Average loss at step 317200: 434.769716\n",
      "Average loss at step 317300: 436.019049\n",
      "Average loss at step 317400: 434.604353\n",
      "Average loss at step 317500: 435.631058\n",
      "Average loss at step 317600: 432.191628\n",
      "Average loss at step 317700: 435.038637\n",
      "Average loss at step 317800: 433.976657\n",
      "Average loss at step 317900: 433.637566\n",
      "Average loss at step 318000: 435.350740\n",
      "Graph 318: 25 nodes\n",
      "Average loss at step 318100: 457.170908\n",
      "Average loss at step 318200: 455.809053\n",
      "Average loss at step 318300: 454.520298\n",
      "Average loss at step 318400: 455.061507\n",
      "Average loss at step 318500: 454.819458\n",
      "Average loss at step 318600: 453.176142\n",
      "Average loss at step 318700: 453.158936\n",
      "Average loss at step 318800: 454.671779\n",
      "Average loss at step 318900: 455.129374\n",
      "Average loss at step 319000: 454.303822\n",
      "Graph 319: 38 nodes\n",
      "Average loss at step 319100: 380.615238\n",
      "Average loss at step 319200: 373.590113\n",
      "Average loss at step 319300: 372.809288\n",
      "Average loss at step 319400: 369.790595\n",
      "Average loss at step 319500: 369.651201\n",
      "Average loss at step 319600: 373.524188\n",
      "Average loss at step 319700: 369.762546\n",
      "Average loss at step 319800: 369.374655\n",
      "Average loss at step 319900: 369.522121\n",
      "Average loss at step 320000: 369.840620\n",
      "Graph 320: 21 nodes\n",
      "Average loss at step 320100: 453.152480\n",
      "Average loss at step 320200: 447.701483\n",
      "Average loss at step 320300: 447.195394\n",
      "Average loss at step 320400: 444.674724\n",
      "Average loss at step 320500: 445.332615\n",
      "Average loss at step 320600: 444.544866\n",
      "Average loss at step 320700: 444.226413\n",
      "Average loss at step 320800: 444.248226\n",
      "Average loss at step 320900: 444.252920\n",
      "Average loss at step 321000: 445.721777\n",
      "Time: 54.6938939095\n",
      "Graph 321: 29 nodes\n",
      "Average loss at step 321100: 434.304186\n",
      "Average loss at step 321200: 435.019633\n",
      "Average loss at step 321300: 431.894824\n",
      "Average loss at step 321400: 434.275771\n",
      "Average loss at step 321500: 434.410743\n",
      "Average loss at step 321600: 432.960709\n",
      "Average loss at step 321700: 432.331756\n",
      "Average loss at step 321800: 432.546846\n",
      "Average loss at step 321900: 433.623372\n",
      "Average loss at step 322000: 432.046978\n",
      "Graph 322: 20 nodes\n",
      "Average loss at step 322100: 459.245715\n",
      "Average loss at step 322200: 459.633554\n",
      "Average loss at step 322300: 457.735231\n",
      "Average loss at step 322400: 455.586544\n",
      "Average loss at step 322500: 456.381327\n",
      "Average loss at step 322600: 456.747153\n",
      "Average loss at step 322700: 454.837667\n",
      "Average loss at step 322800: 454.666939\n",
      "Average loss at step 322900: 457.468905\n",
      "Average loss at step 323000: 455.775115\n",
      "Graph 323: 21 nodes\n",
      "Average loss at step 323100: 461.813003\n",
      "Average loss at step 323200: 459.847705\n",
      "Average loss at step 323300: 459.359074\n",
      "Average loss at step 323400: 459.071473\n",
      "Average loss at step 323500: 461.170639\n",
      "Average loss at step 323600: 457.856776\n",
      "Average loss at step 323700: 459.212084\n",
      "Average loss at step 323800: 458.197415\n",
      "Average loss at step 323900: 456.582497\n",
      "Average loss at step 324000: 458.884607\n",
      "Graph 324: 50 nodes\n",
      "Average loss at step 324100: 391.620028\n",
      "Average loss at step 324200: 384.154201\n",
      "Average loss at step 324300: 385.742759\n",
      "Average loss at step 324400: 385.635281\n",
      "Average loss at step 324500: 381.177220\n",
      "Average loss at step 324600: 387.175807\n",
      "Average loss at step 324700: 388.222559\n",
      "Average loss at step 324800: 385.338552\n",
      "Average loss at step 324900: 385.911805\n",
      "Average loss at step 325000: 383.478724\n",
      "Graph 325: 46 nodes\n",
      "Average loss at step 325100: 389.159882\n",
      "Average loss at step 325200: 390.726246\n",
      "Average loss at step 325300: 390.215888\n",
      "Average loss at step 325400: 389.831616\n",
      "Average loss at step 325500: 388.776370\n",
      "Average loss at step 325600: 386.461021\n",
      "Average loss at step 325700: 387.955126\n",
      "Average loss at step 325800: 389.483017\n",
      "Average loss at step 325900: 385.617995\n",
      "Average loss at step 326000: 386.796484\n",
      "Graph 326: 49 nodes\n",
      "Average loss at step 326100: 393.235755\n",
      "Average loss at step 326200: 389.393671\n",
      "Average loss at step 326300: 389.881295\n",
      "Average loss at step 326400: 386.732968\n",
      "Average loss at step 326500: 388.878956\n",
      "Average loss at step 326600: 389.179428\n",
      "Average loss at step 326700: 388.388073\n",
      "Average loss at step 326800: 389.830847\n",
      "Average loss at step 326900: 389.501200\n",
      "Average loss at step 327000: 389.941279\n",
      "Graph 327: 27 nodes\n",
      "Average loss at step 327100: 458.995811\n",
      "Average loss at step 327200: 456.737791\n",
      "Average loss at step 327300: 456.154612\n",
      "Average loss at step 327400: 454.514649\n",
      "Average loss at step 327500: 457.210366\n",
      "Average loss at step 327600: 456.015696\n",
      "Average loss at step 327700: 456.689822\n",
      "Average loss at step 327800: 456.373792\n",
      "Average loss at step 327900: 456.098797\n",
      "Average loss at step 328000: 454.939876\n",
      "Graph 328: 12 nodes\n",
      "Average loss at step 328100: 470.535518\n",
      "Average loss at step 328200: 467.557838\n",
      "Average loss at step 328300: 466.944727\n",
      "Average loss at step 328400: 466.512484\n",
      "Average loss at step 328500: 466.883011\n",
      "Average loss at step 328600: 465.856307\n",
      "Average loss at step 328700: 464.344590\n",
      "Average loss at step 328800: 465.780887\n",
      "Average loss at step 328900: 464.765579\n",
      "Average loss at step 329000: 464.512705\n",
      "Graph 329: 14 nodes\n",
      "Average loss at step 329100: 417.504256\n",
      "Average loss at step 329200: 413.136216\n",
      "Average loss at step 329300: 412.225189\n",
      "Average loss at step 329400: 409.441046\n",
      "Average loss at step 329500: 410.199736\n",
      "Average loss at step 329600: 408.541775\n",
      "Average loss at step 329700: 407.284656\n",
      "Average loss at step 329800: 409.608819\n",
      "Average loss at step 329900: 406.690339\n",
      "Average loss at step 330000: 408.486631\n",
      "Graph 330: 23 nodes\n",
      "Average loss at step 330100: 449.367111\n",
      "Average loss at step 330200: 444.750623\n",
      "Average loss at step 330300: 442.383739\n",
      "Average loss at step 330400: 443.199389\n",
      "Average loss at step 330500: 443.486541\n",
      "Average loss at step 330600: 441.593378\n",
      "Average loss at step 330700: 440.451639\n",
      "Average loss at step 330800: 442.891797\n",
      "Average loss at step 330900: 442.062302\n",
      "Average loss at step 331000: 438.670294\n",
      "Time: 60.8769118786\n",
      "Graph 331: 24 nodes\n",
      "Average loss at step 331100: 444.917442\n",
      "Average loss at step 331200: 442.429942\n",
      "Average loss at step 331300: 442.629003\n",
      "Average loss at step 331400: 440.480045\n",
      "Average loss at step 331500: 441.379270\n",
      "Average loss at step 331600: 442.721735\n",
      "Average loss at step 331700: 442.651484\n",
      "Average loss at step 331800: 441.316012\n",
      "Average loss at step 331900: 441.227496\n",
      "Average loss at step 332000: 442.848176\n",
      "Graph 332: 33 nodes\n",
      "Average loss at step 332100: 456.920691\n",
      "Average loss at step 332200: 452.338679\n",
      "Average loss at step 332300: 452.532682\n",
      "Average loss at step 332400: 453.689514\n",
      "Average loss at step 332500: 451.373582\n",
      "Average loss at step 332600: 453.499793\n",
      "Average loss at step 332700: 451.592975\n",
      "Average loss at step 332800: 452.079557\n",
      "Average loss at step 332900: 452.898400\n",
      "Average loss at step 333000: 453.442363\n",
      "Graph 333: 27 nodes\n",
      "Average loss at step 333100: 453.282418\n",
      "Average loss at step 333200: 449.544330\n",
      "Average loss at step 333300: 450.247908\n",
      "Average loss at step 333400: 450.825638\n",
      "Average loss at step 333500: 450.783111\n",
      "Average loss at step 333600: 450.134327\n",
      "Average loss at step 333700: 451.484505\n",
      "Average loss at step 333800: 450.480413\n",
      "Average loss at step 333900: 451.308005\n",
      "Average loss at step 334000: 449.522262\n",
      "Graph 334: 25 nodes\n",
      "Average loss at step 334100: 433.817756\n",
      "Average loss at step 334200: 428.384902\n",
      "Average loss at step 334300: 431.600201\n",
      "Average loss at step 334400: 429.944822\n",
      "Average loss at step 334500: 431.219801\n",
      "Average loss at step 334600: 427.629240\n",
      "Average loss at step 334700: 427.694247\n",
      "Average loss at step 334800: 430.395700\n",
      "Average loss at step 334900: 430.710123\n",
      "Average loss at step 335000: 424.860572\n",
      "Graph 335: 28 nodes\n",
      "Average loss at step 335100: 439.421911\n",
      "Average loss at step 335200: 439.847873\n",
      "Average loss at step 335300: 436.142085\n",
      "Average loss at step 335400: 437.295985\n",
      "Average loss at step 335500: 440.910345\n",
      "Average loss at step 335600: 440.313704\n",
      "Average loss at step 335700: 438.497813\n",
      "Average loss at step 335800: 436.860180\n",
      "Average loss at step 335900: 436.589099\n",
      "Average loss at step 336000: 436.817724\n",
      "Graph 336: 28 nodes\n",
      "Average loss at step 336100: 450.253043\n",
      "Average loss at step 336200: 450.159122\n",
      "Average loss at step 336300: 445.609543\n",
      "Average loss at step 336400: 448.079015\n",
      "Average loss at step 336500: 446.765974\n",
      "Average loss at step 336600: 446.107441\n",
      "Average loss at step 336700: 446.976495\n",
      "Average loss at step 336800: 446.415021\n",
      "Average loss at step 336900: 443.573241\n",
      "Average loss at step 337000: 446.254831\n",
      "Graph 337: 12 nodes\n",
      "Average loss at step 337100: 467.066269\n",
      "Average loss at step 337200: 463.539920\n",
      "Average loss at step 337300: 462.565379\n",
      "Average loss at step 337400: 462.692479\n",
      "Average loss at step 337500: 460.208389\n",
      "Average loss at step 337600: 462.028673\n",
      "Average loss at step 337700: 461.909757\n",
      "Average loss at step 337800: 461.103620\n",
      "Average loss at step 337900: 460.261393\n",
      "Average loss at step 338000: 459.855248\n",
      "Graph 338: 26 nodes\n",
      "Average loss at step 338100: 449.490055\n",
      "Average loss at step 338200: 447.329956\n",
      "Average loss at step 338300: 447.140998\n",
      "Average loss at step 338400: 445.607832\n",
      "Average loss at step 338500: 445.381963\n",
      "Average loss at step 338600: 445.786713\n",
      "Average loss at step 338700: 445.219494\n",
      "Average loss at step 338800: 446.188635\n",
      "Average loss at step 338900: 446.818535\n",
      "Average loss at step 339000: 446.332952\n",
      "Graph 339: 28 nodes\n",
      "Average loss at step 339100: 437.977712\n",
      "Average loss at step 339200: 435.389207\n",
      "Average loss at step 339300: 436.890660\n",
      "Average loss at step 339400: 434.242344\n",
      "Average loss at step 339500: 434.639219\n",
      "Average loss at step 339600: 433.810557\n",
      "Average loss at step 339700: 429.280356\n",
      "Average loss at step 339800: 434.298817\n",
      "Average loss at step 339900: 432.886676\n",
      "Average loss at step 340000: 435.140744\n",
      "Graph 340: 28 nodes\n",
      "Average loss at step 340100: 455.795833\n",
      "Average loss at step 340200: 451.532960\n",
      "Average loss at step 340300: 450.520416\n",
      "Average loss at step 340400: 450.546240\n",
      "Average loss at step 340500: 450.257372\n",
      "Average loss at step 340600: 450.794333\n",
      "Average loss at step 340700: 448.018199\n",
      "Average loss at step 340800: 451.196801\n",
      "Average loss at step 340900: 451.859083\n",
      "Average loss at step 341000: 450.244250\n",
      "Time: 54.3339109421\n",
      "Graph 341: 26 nodes\n",
      "Average loss at step 341100: 444.427242\n",
      "Average loss at step 341200: 445.131199\n",
      "Average loss at step 341300: 443.425589\n",
      "Average loss at step 341400: 441.622351\n",
      "Average loss at step 341500: 442.595872\n",
      "Average loss at step 341600: 442.780038\n",
      "Average loss at step 341700: 442.712121\n",
      "Average loss at step 341800: 442.619876\n",
      "Average loss at step 341900: 443.783521\n",
      "Average loss at step 342000: 441.597448\n",
      "Graph 342: 24 nodes\n",
      "Average loss at step 342100: 451.806215\n",
      "Average loss at step 342200: 451.712611\n",
      "Average loss at step 342300: 451.135353\n",
      "Average loss at step 342400: 450.594625\n",
      "Average loss at step 342500: 448.548825\n",
      "Average loss at step 342600: 449.657549\n",
      "Average loss at step 342700: 450.744302\n",
      "Average loss at step 342800: 449.419929\n",
      "Average loss at step 342900: 450.539098\n",
      "Average loss at step 343000: 451.603864\n",
      "Graph 343: 23 nodes\n",
      "Average loss at step 343100: 458.690686\n",
      "Average loss at step 343200: 456.187650\n",
      "Average loss at step 343300: 455.127016\n",
      "Average loss at step 343400: 453.989165\n",
      "Average loss at step 343500: 452.343328\n",
      "Average loss at step 343600: 453.644416\n",
      "Average loss at step 343700: 452.558788\n",
      "Average loss at step 343800: 453.095217\n",
      "Average loss at step 343900: 453.184830\n",
      "Average loss at step 344000: 452.693454\n",
      "Graph 344: 27 nodes\n",
      "Average loss at step 344100: 434.813918\n",
      "Average loss at step 344200: 429.228887\n",
      "Average loss at step 344300: 430.500087\n",
      "Average loss at step 344400: 431.855767\n",
      "Average loss at step 344500: 432.502808\n",
      "Average loss at step 344600: 433.568252\n",
      "Average loss at step 344700: 431.928728\n",
      "Average loss at step 344800: 431.017734\n",
      "Average loss at step 344900: 432.088843\n",
      "Average loss at step 345000: 431.016970\n",
      "Graph 345: 39 nodes\n",
      "Average loss at step 345100: 450.814668\n",
      "Average loss at step 345200: 448.391570\n",
      "Average loss at step 345300: 448.279088\n",
      "Average loss at step 345400: 448.192295\n",
      "Average loss at step 345500: 448.040224\n",
      "Average loss at step 345600: 447.805368\n",
      "Average loss at step 345700: 447.983354\n",
      "Average loss at step 345800: 447.823371\n",
      "Average loss at step 345900: 446.760328\n",
      "Average loss at step 346000: 447.122990\n",
      "Graph 346: 42 nodes\n",
      "Average loss at step 346100: 396.795085\n",
      "Average loss at step 346200: 397.353682\n",
      "Average loss at step 346300: 400.705160\n",
      "Average loss at step 346400: 397.198131\n",
      "Average loss at step 346500: 399.043824\n",
      "Average loss at step 346600: 395.344769\n",
      "Average loss at step 346700: 397.896607\n",
      "Average loss at step 346800: 396.792624\n",
      "Average loss at step 346900: 394.507817\n",
      "Average loss at step 347000: 394.987358\n",
      "Graph 347: 42 nodes\n",
      "Average loss at step 347100: 398.458783\n",
      "Average loss at step 347200: 396.799555\n",
      "Average loss at step 347300: 396.551925\n",
      "Average loss at step 347400: 397.311682\n",
      "Average loss at step 347500: 396.372203\n",
      "Average loss at step 347600: 397.190614\n",
      "Average loss at step 347700: 396.871076\n",
      "Average loss at step 347800: 395.618989\n",
      "Average loss at step 347900: 394.706902\n",
      "Average loss at step 348000: 393.740486\n",
      "Graph 348: 64 nodes\n",
      "Average loss at step 348100: 373.839269\n",
      "Average loss at step 348200: 372.827319\n",
      "Average loss at step 348300: 374.092782\n",
      "Average loss at step 348400: 370.360623\n",
      "Average loss at step 348500: 369.938208\n",
      "Average loss at step 348600: 370.837059\n",
      "Average loss at step 348700: 370.469483\n",
      "Average loss at step 348800: 370.280062\n",
      "Average loss at step 348900: 370.618983\n",
      "Average loss at step 349000: 373.006783\n",
      "Graph 349: 38 nodes\n",
      "Average loss at step 349100: 392.031735\n",
      "Average loss at step 349200: 391.572625\n",
      "Average loss at step 349300: 388.114483\n",
      "Average loss at step 349400: 388.268588\n",
      "Average loss at step 349500: 390.492682\n",
      "Average loss at step 349600: 389.397053\n",
      "Average loss at step 349700: 391.584840\n",
      "Average loss at step 349800: 387.911188\n",
      "Average loss at step 349900: 390.996335\n",
      "Average loss at step 350000: 388.385211\n",
      "Graph 350: 22 nodes\n",
      "Average loss at step 350100: 409.674955\n",
      "Average loss at step 350200: 404.011152\n",
      "Average loss at step 350300: 406.063085\n",
      "Average loss at step 350400: 405.888637\n",
      "Average loss at step 350500: 404.656817\n",
      "Average loss at step 350600: 401.969455\n",
      "Average loss at step 350700: 405.290001\n",
      "Average loss at step 350800: 405.356992\n",
      "Average loss at step 350900: 405.018433\n",
      "Average loss at step 351000: 403.442669\n",
      "Time: 54.770275116\n",
      "Graph 351: 9 nodes\n",
      "Average loss at step 351100: 317.516325\n",
      "Average loss at step 351200: 294.639004\n",
      "Average loss at step 351300: 293.515622\n",
      "Average loss at step 351400: 288.718888\n",
      "Average loss at step 351500: 292.001995\n",
      "Average loss at step 351600: 289.485209\n",
      "Average loss at step 351700: 287.249871\n",
      "Average loss at step 351800: 292.455568\n",
      "Average loss at step 351900: 287.026027\n",
      "Average loss at step 352000: 285.591447\n",
      "Graph 352: 26 nodes\n",
      "Average loss at step 352100: 456.022121\n",
      "Average loss at step 352200: 445.273052\n",
      "Average loss at step 352300: 445.862562\n",
      "Average loss at step 352400: 444.246775\n",
      "Average loss at step 352500: 444.090707\n",
      "Average loss at step 352600: 445.792575\n",
      "Average loss at step 352700: 445.180014\n",
      "Average loss at step 352800: 441.670090\n",
      "Average loss at step 352900: 442.273228\n",
      "Average loss at step 353000: 444.763169\n",
      "Graph 353: 19 nodes\n",
      "Average loss at step 353100: 388.149658\n",
      "Average loss at step 353200: 387.214901\n",
      "Average loss at step 353300: 381.262489\n",
      "Average loss at step 353400: 382.853934\n",
      "Average loss at step 353500: 383.495691\n",
      "Average loss at step 353600: 381.584472\n",
      "Average loss at step 353700: 382.865589\n",
      "Average loss at step 353800: 379.959692\n",
      "Average loss at step 353900: 386.057050\n",
      "Average loss at step 354000: 383.439118\n",
      "Graph 354: 66 nodes\n",
      "Average loss at step 354100: 360.329086\n",
      "Average loss at step 354200: 357.597061\n",
      "Average loss at step 354300: 353.574473\n",
      "Average loss at step 354400: 354.655037\n",
      "Average loss at step 354500: 355.111964\n",
      "Average loss at step 354600: 352.972357\n",
      "Average loss at step 354700: 355.114123\n",
      "Average loss at step 354800: 353.821662\n",
      "Average loss at step 354900: 357.011043\n",
      "Average loss at step 355000: 353.443251\n",
      "Graph 355: 27 nodes\n",
      "Average loss at step 355100: 444.992888\n",
      "Average loss at step 355200: 442.432263\n",
      "Average loss at step 355300: 441.310400\n",
      "Average loss at step 355400: 440.235048\n",
      "Average loss at step 355500: 439.607670\n",
      "Average loss at step 355600: 440.649140\n",
      "Average loss at step 355700: 438.972109\n",
      "Average loss at step 355800: 439.163138\n",
      "Average loss at step 355900: 441.228965\n",
      "Average loss at step 356000: 441.353936\n",
      "Graph 356: 34 nodes\n",
      "Average loss at step 356100: 398.790795\n",
      "Average loss at step 356200: 397.230428\n",
      "Average loss at step 356300: 395.914671\n",
      "Average loss at step 356400: 395.244272\n",
      "Average loss at step 356500: 395.461214\n",
      "Average loss at step 356600: 393.926058\n",
      "Average loss at step 356700: 391.426318\n",
      "Average loss at step 356800: 394.213564\n",
      "Average loss at step 356900: 396.046131\n",
      "Average loss at step 357000: 396.425662\n",
      "Graph 357: 28 nodes\n",
      "Average loss at step 357100: 404.511083\n",
      "Average loss at step 357200: 402.050478\n",
      "Average loss at step 357300: 399.518469\n",
      "Average loss at step 357400: 400.849166\n",
      "Average loss at step 357500: 401.294032\n",
      "Average loss at step 357600: 401.036693\n",
      "Average loss at step 357700: 401.411864\n",
      "Average loss at step 357800: 401.786303\n",
      "Average loss at step 357900: 402.153001\n",
      "Average loss at step 358000: 401.193826\n",
      "Graph 358: 26 nodes\n",
      "Average loss at step 358100: 393.082220\n",
      "Average loss at step 358200: 390.403060\n",
      "Average loss at step 358300: 389.857112\n",
      "Average loss at step 358400: 392.047624\n",
      "Average loss at step 358500: 385.584961\n",
      "Average loss at step 358600: 389.557143\n",
      "Average loss at step 358700: 387.495850\n",
      "Average loss at step 358800: 387.780031\n",
      "Average loss at step 358900: 388.775865\n",
      "Average loss at step 359000: 389.075250\n",
      "Graph 359: 26 nodes\n",
      "Average loss at step 359100: 401.287328\n",
      "Average loss at step 359200: 397.191020\n",
      "Average loss at step 359300: 396.614372\n",
      "Average loss at step 359400: 395.039979\n",
      "Average loss at step 359500: 398.210400\n",
      "Average loss at step 359600: 397.296563\n",
      "Average loss at step 359700: 397.348553\n",
      "Average loss at step 359800: 395.423768\n",
      "Average loss at step 359900: 400.023460\n",
      "Average loss at step 360000: 397.916518\n",
      "Graph 360: 28 nodes\n",
      "Average loss at step 360100: 382.661598\n",
      "Average loss at step 360200: 381.283557\n",
      "Average loss at step 360300: 379.870308\n",
      "Average loss at step 360400: 378.821645\n",
      "Average loss at step 360500: 380.293871\n",
      "Average loss at step 360600: 383.809826\n",
      "Average loss at step 360700: 382.093099\n",
      "Average loss at step 360800: 381.426215\n",
      "Average loss at step 360900: 383.085105\n",
      "Average loss at step 361000: 378.222433\n",
      "Time: 56.0700111389\n",
      "Graph 361: 19 nodes\n",
      "Average loss at step 361100: 439.847117\n",
      "Average loss at step 361200: 434.216468\n",
      "Average loss at step 361300: 431.761457\n",
      "Average loss at step 361400: 432.668300\n",
      "Average loss at step 361500: 431.639739\n",
      "Average loss at step 361600: 433.193058\n",
      "Average loss at step 361700: 431.828952\n",
      "Average loss at step 361800: 431.229125\n",
      "Average loss at step 361900: 433.962985\n",
      "Average loss at step 362000: 430.863502\n",
      "Graph 362: 20 nodes\n",
      "Average loss at step 362100: 429.185824\n",
      "Average loss at step 362200: 427.547472\n",
      "Average loss at step 362300: 428.190351\n",
      "Average loss at step 362400: 429.318892\n",
      "Average loss at step 362500: 427.895599\n",
      "Average loss at step 362600: 427.597993\n",
      "Average loss at step 362700: 428.563534\n",
      "Average loss at step 362800: 426.685996\n",
      "Average loss at step 362900: 430.209879\n",
      "Average loss at step 363000: 426.170008\n",
      "Graph 363: 20 nodes\n",
      "Average loss at step 363100: 409.300741\n",
      "Average loss at step 363200: 408.347123\n",
      "Average loss at step 363300: 402.984314\n",
      "Average loss at step 363400: 403.597376\n",
      "Average loss at step 363500: 402.545183\n",
      "Average loss at step 363600: 404.114338\n",
      "Average loss at step 363700: 404.268409\n",
      "Average loss at step 363800: 404.575104\n",
      "Average loss at step 363900: 403.214734\n",
      "Average loss at step 364000: 404.154258\n",
      "Graph 364: 42 nodes\n",
      "Average loss at step 364100: 402.280650\n",
      "Average loss at step 364200: 400.107087\n",
      "Average loss at step 364300: 400.045875\n",
      "Average loss at step 364400: 395.975851\n",
      "Average loss at step 364500: 398.668719\n",
      "Average loss at step 364600: 395.997690\n",
      "Average loss at step 364700: 398.614606\n",
      "Average loss at step 364800: 390.006530\n",
      "Average loss at step 364900: 393.895715\n",
      "Average loss at step 365000: 390.830282\n",
      "Graph 365: 42 nodes\n",
      "Average loss at step 365100: 400.532351\n",
      "Average loss at step 365200: 398.428933\n",
      "Average loss at step 365300: 399.779479\n",
      "Average loss at step 365400: 401.264172\n",
      "Average loss at step 365500: 399.662675\n",
      "Average loss at step 365600: 397.980139\n",
      "Average loss at step 365700: 400.446852\n",
      "Average loss at step 365800: 398.257163\n",
      "Average loss at step 365900: 395.656394\n",
      "Average loss at step 366000: 401.394419\n",
      "Graph 366: 15 nodes\n",
      "Average loss at step 366100: 424.346266\n",
      "Average loss at step 366200: 422.369574\n",
      "Average loss at step 366300: 420.683121\n",
      "Average loss at step 366400: 420.621519\n",
      "Average loss at step 366500: 423.124854\n",
      "Average loss at step 366600: 419.665199\n",
      "Average loss at step 366700: 420.642364\n",
      "Average loss at step 366800: 419.495588\n",
      "Average loss at step 366900: 419.386389\n",
      "Average loss at step 367000: 421.620013\n",
      "Graph 367: 11 nodes\n",
      "Average loss at step 367100: 433.173554\n",
      "Average loss at step 367200: 437.156801\n",
      "Average loss at step 367300: 435.825810\n",
      "Average loss at step 367400: 433.809595\n",
      "Average loss at step 367500: 433.804781\n",
      "Average loss at step 367600: 434.326796\n",
      "Average loss at step 367700: 431.754817\n",
      "Average loss at step 367800: 431.454870\n",
      "Average loss at step 367900: 432.766422\n",
      "Average loss at step 368000: 433.924392\n",
      "Graph 368: 10 nodes\n",
      "Average loss at step 368100: 423.248265\n",
      "Average loss at step 368200: 422.567048\n",
      "Average loss at step 368300: 421.464629\n",
      "Average loss at step 368400: 419.930149\n",
      "Average loss at step 368500: 421.341583\n",
      "Average loss at step 368600: 417.296449\n",
      "Average loss at step 368700: 419.189206\n",
      "Average loss at step 368800: 418.682359\n",
      "Average loss at step 368900: 418.179371\n",
      "Average loss at step 369000: 421.834731\n",
      "Graph 369: 12 nodes\n",
      "Average loss at step 369100: 419.306237\n",
      "Average loss at step 369200: 417.008100\n",
      "Average loss at step 369300: 416.286293\n",
      "Average loss at step 369400: 415.819886\n",
      "Average loss at step 369500: 416.992301\n",
      "Average loss at step 369600: 418.468581\n",
      "Average loss at step 369700: 415.409803\n",
      "Average loss at step 369800: 414.451526\n",
      "Average loss at step 369900: 419.965686\n",
      "Average loss at step 370000: 415.107149\n",
      "Graph 370: 30 nodes\n",
      "Average loss at step 370100: 399.911711\n",
      "Average loss at step 370200: 395.911838\n",
      "Average loss at step 370300: 394.799285\n",
      "Average loss at step 370400: 393.691048\n",
      "Average loss at step 370500: 395.691656\n",
      "Average loss at step 370600: 394.129137\n",
      "Average loss at step 370700: 394.490344\n",
      "Average loss at step 370800: 394.628869\n",
      "Average loss at step 370900: 395.456151\n",
      "Average loss at step 371000: 392.349556\n",
      "Time: 50.1917889118\n",
      "Graph 371: 15 nodes\n",
      "Average loss at step 371100: 410.056745\n",
      "Average loss at step 371200: 408.954918\n",
      "Average loss at step 371300: 409.454142\n",
      "Average loss at step 371400: 410.717775\n",
      "Average loss at step 371500: 410.409116\n",
      "Average loss at step 371600: 409.332941\n",
      "Average loss at step 371700: 410.620456\n",
      "Average loss at step 371800: 408.185667\n",
      "Average loss at step 371900: 408.365608\n",
      "Average loss at step 372000: 410.147690\n",
      "Graph 372: 10 nodes\n",
      "Average loss at step 372100: 459.850805\n",
      "Average loss at step 372200: 457.264054\n",
      "Average loss at step 372300: 453.728427\n",
      "Average loss at step 372400: 453.132488\n",
      "Average loss at step 372500: 452.894754\n",
      "Average loss at step 372600: 450.907176\n",
      "Average loss at step 372700: 452.338068\n",
      "Average loss at step 372800: 452.182785\n",
      "Average loss at step 372900: 451.480597\n",
      "Average loss at step 373000: 452.520137\n",
      "Graph 373: 18 nodes\n",
      "Average loss at step 373100: 422.948164\n",
      "Average loss at step 373200: 419.169807\n",
      "Average loss at step 373300: 417.764528\n",
      "Average loss at step 373400: 417.920653\n",
      "Average loss at step 373500: 414.049595\n",
      "Average loss at step 373600: 416.027738\n",
      "Average loss at step 373700: 417.967900\n",
      "Average loss at step 373800: 418.062124\n",
      "Average loss at step 373900: 418.029735\n",
      "Average loss at step 374000: 414.680812\n",
      "Graph 374: 22 nodes\n",
      "Average loss at step 374100: 407.405698\n",
      "Average loss at step 374200: 405.967210\n",
      "Average loss at step 374300: 401.092010\n",
      "Average loss at step 374400: 405.386625\n",
      "Average loss at step 374500: 403.304053\n",
      "Average loss at step 374600: 404.204708\n",
      "Average loss at step 374700: 404.332487\n",
      "Average loss at step 374800: 403.754467\n",
      "Average loss at step 374900: 401.242880\n",
      "Average loss at step 375000: 404.397418\n",
      "Graph 375: 12 nodes\n",
      "Average loss at step 375100: 426.198665\n",
      "Average loss at step 375200: 423.587747\n",
      "Average loss at step 375300: 423.334851\n",
      "Average loss at step 375400: 423.367431\n",
      "Average loss at step 375500: 423.251840\n",
      "Average loss at step 375600: 423.886347\n",
      "Average loss at step 375700: 422.400938\n",
      "Average loss at step 375800: 422.193299\n",
      "Average loss at step 375900: 423.377334\n",
      "Average loss at step 376000: 424.961810\n",
      "Graph 376: 36 nodes\n",
      "Average loss at step 376100: 440.167315\n",
      "Average loss at step 376200: 439.013891\n",
      "Average loss at step 376300: 436.123428\n",
      "Average loss at step 376400: 436.704144\n",
      "Average loss at step 376500: 436.225289\n",
      "Average loss at step 376600: 436.940647\n",
      "Average loss at step 376700: 434.808319\n",
      "Average loss at step 376800: 435.266418\n",
      "Average loss at step 376900: 437.168548\n",
      "Average loss at step 377000: 434.364991\n",
      "Graph 377: 30 nodes\n",
      "Average loss at step 377100: 394.491344\n",
      "Average loss at step 377200: 394.345798\n",
      "Average loss at step 377300: 392.717835\n",
      "Average loss at step 377400: 390.610245\n",
      "Average loss at step 377500: 392.504729\n",
      "Average loss at step 377600: 388.525207\n",
      "Average loss at step 377700: 389.736604\n",
      "Average loss at step 377800: 391.366964\n",
      "Average loss at step 377900: 391.186135\n",
      "Average loss at step 378000: 390.092077\n",
      "Graph 378: 30 nodes\n",
      "Average loss at step 378100: 395.242159\n",
      "Average loss at step 378200: 396.233764\n",
      "Average loss at step 378300: 394.195797\n",
      "Average loss at step 378400: 392.371045\n",
      "Average loss at step 378500: 393.459795\n",
      "Average loss at step 378600: 394.822483\n",
      "Average loss at step 378700: 397.759985\n",
      "Average loss at step 378800: 397.031531\n",
      "Average loss at step 378900: 394.768450\n",
      "Average loss at step 379000: 394.122767\n",
      "Graph 379: 36 nodes\n",
      "Average loss at step 379100: 420.918872\n",
      "Average loss at step 379200: 420.059269\n",
      "Average loss at step 379300: 419.691726\n",
      "Average loss at step 379400: 421.879078\n",
      "Average loss at step 379500: 417.978102\n",
      "Average loss at step 379600: 419.313522\n",
      "Average loss at step 379700: 417.892018\n",
      "Average loss at step 379800: 417.947306\n",
      "Average loss at step 379900: 416.617609\n",
      "Average loss at step 380000: 418.146562\n",
      "Graph 380: 33 nodes\n",
      "Average loss at step 380100: 459.779459\n",
      "Average loss at step 380200: 457.517290\n",
      "Average loss at step 380300: 456.186258\n",
      "Average loss at step 380400: 458.293788\n",
      "Average loss at step 380500: 454.822346\n",
      "Average loss at step 380600: 455.390346\n",
      "Average loss at step 380700: 455.154495\n",
      "Average loss at step 380800: 455.690512\n",
      "Average loss at step 380900: 454.928560\n",
      "Average loss at step 381000: 454.578461\n",
      "Time: 49.7992761135\n",
      "Graph 381: 48 nodes\n",
      "Average loss at step 381100: 391.250556\n",
      "Average loss at step 381200: 384.556975\n",
      "Average loss at step 381300: 383.713838\n",
      "Average loss at step 381400: 380.497413\n",
      "Average loss at step 381500: 385.086512\n",
      "Average loss at step 381600: 381.843791\n",
      "Average loss at step 381700: 382.668930\n",
      "Average loss at step 381800: 385.890296\n",
      "Average loss at step 381900: 384.808089\n",
      "Average loss at step 382000: 382.397721\n",
      "Graph 382: 27 nodes\n",
      "Average loss at step 382100: 445.929538\n",
      "Average loss at step 382200: 443.201228\n",
      "Average loss at step 382300: 442.494274\n",
      "Average loss at step 382400: 443.489088\n",
      "Average loss at step 382500: 442.825662\n",
      "Average loss at step 382600: 441.349693\n",
      "Average loss at step 382700: 444.738341\n",
      "Average loss at step 382800: 440.028251\n",
      "Average loss at step 382900: 440.868467\n",
      "Average loss at step 383000: 440.468762\n",
      "Graph 383: 27 nodes\n",
      "Average loss at step 383100: 435.261858\n",
      "Average loss at step 383200: 434.195087\n",
      "Average loss at step 383300: 431.576552\n",
      "Average loss at step 383400: 433.128219\n",
      "Average loss at step 383500: 433.005123\n",
      "Average loss at step 383600: 434.016247\n",
      "Average loss at step 383700: 432.817317\n",
      "Average loss at step 383800: 430.423218\n",
      "Average loss at step 383900: 432.233155\n",
      "Average loss at step 384000: 432.466504\n",
      "Graph 384: 16 nodes\n",
      "Average loss at step 384100: 431.262928\n",
      "Average loss at step 384200: 428.732101\n",
      "Average loss at step 384300: 427.348749\n",
      "Average loss at step 384400: 426.336545\n",
      "Average loss at step 384500: 427.014387\n",
      "Average loss at step 384600: 426.198031\n",
      "Average loss at step 384700: 427.102270\n",
      "Average loss at step 384800: 423.401508\n",
      "Average loss at step 384900: 426.804140\n",
      "Average loss at step 385000: 423.794430\n",
      "Graph 385: 7 nodes\n",
      "Average loss at step 385100: 477.054854\n",
      "Average loss at step 385200: 472.842172\n",
      "Average loss at step 385300: 470.587862\n",
      "Average loss at step 385400: 470.974811\n",
      "Average loss at step 385500: 470.331827\n",
      "Average loss at step 385600: 468.497326\n",
      "Average loss at step 385700: 469.365015\n",
      "Average loss at step 385800: 467.766136\n",
      "Average loss at step 385900: 468.920603\n",
      "Average loss at step 386000: 468.144058\n",
      "Graph 386: 16 nodes\n",
      "Average loss at step 386100: 430.468746\n",
      "Average loss at step 386200: 425.110295\n",
      "Average loss at step 386300: 424.825718\n",
      "Average loss at step 386400: 423.585410\n",
      "Average loss at step 386500: 423.498317\n",
      "Average loss at step 386600: 422.149444\n",
      "Average loss at step 386700: 423.851222\n",
      "Average loss at step 386800: 423.698099\n",
      "Average loss at step 386900: 424.037836\n",
      "Average loss at step 387000: 421.537585\n",
      "Graph 387: 35 nodes\n",
      "Average loss at step 387100: 416.123473\n",
      "Average loss at step 387200: 418.193084\n",
      "Average loss at step 387300: 416.275692\n",
      "Average loss at step 387400: 415.570874\n",
      "Average loss at step 387500: 416.782574\n",
      "Average loss at step 387600: 415.327724\n",
      "Average loss at step 387700: 416.286131\n",
      "Average loss at step 387800: 414.304064\n",
      "Average loss at step 387900: 415.795018\n",
      "Average loss at step 388000: 415.081551\n",
      "Graph 388: 28 nodes\n",
      "Average loss at step 388100: 442.866704\n",
      "Average loss at step 388200: 440.376796\n",
      "Average loss at step 388300: 438.963296\n",
      "Average loss at step 388400: 437.978675\n",
      "Average loss at step 388500: 439.741520\n",
      "Average loss at step 388600: 438.031637\n",
      "Average loss at step 388700: 438.836732\n",
      "Average loss at step 388800: 439.911307\n",
      "Average loss at step 388900: 440.924703\n",
      "Average loss at step 389000: 437.285086\n",
      "Graph 389: 40 nodes\n",
      "Average loss at step 389100: 440.627664\n",
      "Average loss at step 389200: 437.343776\n",
      "Average loss at step 389300: 437.041100\n",
      "Average loss at step 389400: 436.416759\n",
      "Average loss at step 389500: 434.837757\n",
      "Average loss at step 389600: 438.686805\n",
      "Average loss at step 389700: 435.951566\n",
      "Average loss at step 389800: 431.381380\n",
      "Average loss at step 389900: 435.385786\n",
      "Average loss at step 390000: 434.936014\n",
      "Graph 390: 26 nodes\n",
      "Average loss at step 390100: 450.964414\n",
      "Average loss at step 390200: 448.780697\n",
      "Average loss at step 390300: 448.618367\n",
      "Average loss at step 390400: 447.807728\n",
      "Average loss at step 390500: 447.874935\n",
      "Average loss at step 390600: 448.139356\n",
      "Average loss at step 390700: 448.871148\n",
      "Average loss at step 390800: 449.244617\n",
      "Average loss at step 390900: 447.304189\n",
      "Average loss at step 391000: 446.875987\n",
      "Time: 51.4652011395\n",
      "Graph 391: 48 nodes\n",
      "Average loss at step 391100: 401.860747\n",
      "Average loss at step 391200: 396.857916\n",
      "Average loss at step 391300: 393.352886\n",
      "Average loss at step 391400: 396.816400\n",
      "Average loss at step 391500: 397.885131\n",
      "Average loss at step 391600: 396.713908\n",
      "Average loss at step 391700: 397.666827\n",
      "Average loss at step 391800: 395.144464\n",
      "Average loss at step 391900: 394.906619\n",
      "Average loss at step 392000: 392.858936\n",
      "Graph 392: 15 nodes\n",
      "Average loss at step 392100: 457.412333\n",
      "Average loss at step 392200: 454.947505\n",
      "Average loss at step 392300: 454.003829\n",
      "Average loss at step 392400: 454.764328\n",
      "Average loss at step 392500: 456.017109\n",
      "Average loss at step 392600: 454.977634\n",
      "Average loss at step 392700: 453.315046\n",
      "Average loss at step 392800: 454.084886\n",
      "Average loss at step 392900: 454.179886\n",
      "Average loss at step 393000: 455.868445\n",
      "Graph 393: 11 nodes\n",
      "Average loss at step 393100: 459.911423\n",
      "Average loss at step 393200: 456.500414\n",
      "Average loss at step 393300: 456.424723\n",
      "Average loss at step 393400: 458.035077\n",
      "Average loss at step 393500: 456.382901\n",
      "Average loss at step 393600: 458.059437\n",
      "Average loss at step 393700: 458.009254\n",
      "Average loss at step 393800: 456.114115\n",
      "Average loss at step 393900: 456.210980\n",
      "Average loss at step 394000: 455.713154\n",
      "Graph 394: 50 nodes\n",
      "Average loss at step 394100: 394.474619\n",
      "Average loss at step 394200: 392.033515\n",
      "Average loss at step 394300: 390.199220\n",
      "Average loss at step 394400: 391.543850\n",
      "Average loss at step 394500: 388.353223\n",
      "Average loss at step 394600: 394.039130\n",
      "Average loss at step 394700: 391.858151\n",
      "Average loss at step 394800: 393.224563\n",
      "Average loss at step 394900: 392.521525\n",
      "Average loss at step 395000: 391.668177\n",
      "Graph 395: 18 nodes\n",
      "Average loss at step 395100: 417.338703\n",
      "Average loss at step 395200: 412.079029\n",
      "Average loss at step 395300: 417.271237\n",
      "Average loss at step 395400: 416.600492\n",
      "Average loss at step 395500: 416.167917\n",
      "Average loss at step 395600: 415.032801\n",
      "Average loss at step 395700: 414.529314\n",
      "Average loss at step 395800: 413.570341\n",
      "Average loss at step 395900: 412.985543\n",
      "Average loss at step 396000: 413.374000\n",
      "Graph 396: 19 nodes\n",
      "Average loss at step 396100: 421.948459\n",
      "Average loss at step 396200: 417.756529\n",
      "Average loss at step 396300: 418.567493\n",
      "Average loss at step 396400: 420.448085\n",
      "Average loss at step 396500: 417.783273\n",
      "Average loss at step 396600: 417.177870\n",
      "Average loss at step 396700: 415.737441\n",
      "Average loss at step 396800: 415.933927\n",
      "Average loss at step 396900: 416.955490\n",
      "Average loss at step 397000: 417.319413\n",
      "Graph 397: 14 nodes\n",
      "Average loss at step 397100: 414.488470\n",
      "Average loss at step 397200: 411.807188\n",
      "Average loss at step 397300: 411.711493\n",
      "Average loss at step 397400: 408.392060\n",
      "Average loss at step 397500: 407.842206\n",
      "Average loss at step 397600: 405.829585\n",
      "Average loss at step 397700: 406.588660\n",
      "Average loss at step 397800: 408.502547\n",
      "Average loss at step 397900: 405.591020\n",
      "Average loss at step 398000: 407.354695\n",
      "Graph 398: 21 nodes\n",
      "Average loss at step 398100: 441.721871\n",
      "Average loss at step 398200: 439.163786\n",
      "Average loss at step 398300: 438.588193\n",
      "Average loss at step 398400: 437.577812\n",
      "Average loss at step 398500: 437.353893\n",
      "Average loss at step 398600: 436.591763\n",
      "Average loss at step 398700: 436.940349\n",
      "Average loss at step 398800: 438.614018\n",
      "Average loss at step 398900: 438.202619\n",
      "Average loss at step 399000: 438.771580\n",
      "Graph 399: 37 nodes\n",
      "Average loss at step 399100: 358.962524\n",
      "Average loss at step 399200: 356.078904\n",
      "Average loss at step 399300: 355.215775\n",
      "Average loss at step 399400: 352.871742\n",
      "Average loss at step 399500: 353.787987\n",
      "Average loss at step 399600: 352.400636\n",
      "Average loss at step 399700: 355.068399\n",
      "Average loss at step 399800: 350.567357\n",
      "Average loss at step 399900: 352.895129\n",
      "Average loss at step 400000: 357.883840\n",
      "Graph 400: 48 nodes\n",
      "Average loss at step 400100: 362.516902\n",
      "Average loss at step 400200: 358.324056\n",
      "Average loss at step 400300: 359.793697\n",
      "Average loss at step 400400: 357.446912\n",
      "Average loss at step 400500: 355.771956\n",
      "Average loss at step 400600: 357.404149\n",
      "Average loss at step 400700: 360.407377\n",
      "Average loss at step 400800: 359.482973\n",
      "Average loss at step 400900: 360.187433\n",
      "Average loss at step 401000: 358.563202\n",
      "Time: 53.7145769596\n",
      "Graph 401: 25 nodes\n",
      "Average loss at step 401100: 412.400056\n",
      "Average loss at step 401200: 414.127317\n",
      "Average loss at step 401300: 410.946121\n",
      "Average loss at step 401400: 409.334694\n",
      "Average loss at step 401500: 413.772274\n",
      "Average loss at step 401600: 410.058466\n",
      "Average loss at step 401700: 410.542582\n",
      "Average loss at step 401800: 411.255633\n",
      "Average loss at step 401900: 410.250107\n",
      "Average loss at step 402000: 412.404735\n",
      "Graph 402: 28 nodes\n",
      "Average loss at step 402100: 398.954528\n",
      "Average loss at step 402200: 395.932488\n",
      "Average loss at step 402300: 396.528110\n",
      "Average loss at step 402400: 393.204995\n",
      "Average loss at step 402500: 399.289263\n",
      "Average loss at step 402600: 396.678818\n",
      "Average loss at step 402700: 395.510299\n",
      "Average loss at step 402800: 395.770401\n",
      "Average loss at step 402900: 393.273445\n",
      "Average loss at step 403000: 392.838683\n",
      "Graph 403: 33 nodes\n",
      "Average loss at step 403100: 402.669286\n",
      "Average loss at step 403200: 398.354057\n",
      "Average loss at step 403300: 396.986585\n",
      "Average loss at step 403400: 397.038414\n",
      "Average loss at step 403500: 393.313414\n",
      "Average loss at step 403600: 398.371416\n",
      "Average loss at step 403700: 396.661536\n",
      "Average loss at step 403800: 396.684510\n",
      "Average loss at step 403900: 398.160725\n",
      "Average loss at step 404000: 396.990911\n",
      "Graph 404: 29 nodes\n",
      "Average loss at step 404100: 403.108609\n",
      "Average loss at step 404200: 404.651589\n",
      "Average loss at step 404300: 402.453349\n",
      "Average loss at step 404400: 402.394513\n",
      "Average loss at step 404500: 401.362336\n",
      "Average loss at step 404600: 400.052447\n",
      "Average loss at step 404700: 401.439502\n",
      "Average loss at step 404800: 396.847678\n",
      "Average loss at step 404900: 401.627023\n",
      "Average loss at step 405000: 398.023346\n",
      "Graph 405: 33 nodes\n",
      "Average loss at step 405100: 418.048008\n",
      "Average loss at step 405200: 417.877674\n",
      "Average loss at step 405300: 416.854895\n",
      "Average loss at step 405400: 417.342047\n",
      "Average loss at step 405500: 415.983883\n",
      "Average loss at step 405600: 416.319852\n",
      "Average loss at step 405700: 414.805802\n",
      "Average loss at step 405800: 414.446358\n",
      "Average loss at step 405900: 416.213416\n",
      "Average loss at step 406000: 416.980083\n",
      "Graph 406: 30 nodes\n",
      "Average loss at step 406100: 403.382053\n",
      "Average loss at step 406200: 401.105132\n",
      "Average loss at step 406300: 400.764598\n",
      "Average loss at step 406400: 399.706326\n",
      "Average loss at step 406500: 402.547134\n",
      "Average loss at step 406600: 397.248716\n",
      "Average loss at step 406700: 399.607095\n",
      "Average loss at step 406800: 398.772313\n",
      "Average loss at step 406900: 398.182431\n",
      "Average loss at step 407000: 401.242963\n",
      "Graph 407: 33 nodes\n",
      "Average loss at step 407100: 426.468570\n",
      "Average loss at step 407200: 424.586804\n",
      "Average loss at step 407300: 421.752226\n",
      "Average loss at step 407400: 424.900644\n",
      "Average loss at step 407500: 422.893510\n",
      "Average loss at step 407600: 424.040752\n",
      "Average loss at step 407700: 421.128182\n",
      "Average loss at step 407800: 421.915325\n",
      "Average loss at step 407900: 422.296943\n",
      "Average loss at step 408000: 424.373816\n",
      "Graph 408: 32 nodes\n",
      "Average loss at step 408100: 420.374679\n",
      "Average loss at step 408200: 418.455588\n",
      "Average loss at step 408300: 417.641692\n",
      "Average loss at step 408400: 419.072853\n",
      "Average loss at step 408500: 418.375690\n",
      "Average loss at step 408600: 415.618346\n",
      "Average loss at step 408700: 416.673873\n",
      "Average loss at step 408800: 418.514686\n",
      "Average loss at step 408900: 416.791501\n",
      "Average loss at step 409000: 415.781646\n",
      "Graph 409: 28 nodes\n",
      "Average loss at step 409100: 432.977872\n",
      "Average loss at step 409200: 427.783329\n",
      "Average loss at step 409300: 429.705874\n",
      "Average loss at step 409400: 430.562252\n",
      "Average loss at step 409500: 429.586745\n",
      "Average loss at step 409600: 429.140964\n",
      "Average loss at step 409700: 427.301214\n",
      "Average loss at step 409800: 426.169471\n",
      "Average loss at step 409900: 427.280112\n",
      "Average loss at step 410000: 429.374110\n",
      "Graph 410: 37 nodes\n",
      "Average loss at step 410100: 410.130053\n",
      "Average loss at step 410200: 406.999240\n",
      "Average loss at step 410300: 410.525902\n",
      "Average loss at step 410400: 407.864233\n",
      "Average loss at step 410500: 406.252145\n",
      "Average loss at step 410600: 406.149376\n",
      "Average loss at step 410700: 408.695733\n",
      "Average loss at step 410800: 406.041403\n",
      "Average loss at step 410900: 404.987931\n",
      "Average loss at step 411000: 406.777501\n",
      "Time: 51.2643401623\n",
      "Graph 411: 36 nodes\n",
      "Average loss at step 411100: 415.086058\n",
      "Average loss at step 411200: 413.811289\n",
      "Average loss at step 411300: 411.970932\n",
      "Average loss at step 411400: 411.594642\n",
      "Average loss at step 411500: 414.582150\n",
      "Average loss at step 411600: 413.848400\n",
      "Average loss at step 411700: 411.050400\n",
      "Average loss at step 411800: 412.101564\n",
      "Average loss at step 411900: 413.984105\n",
      "Average loss at step 412000: 413.701592\n",
      "Graph 412: 38 nodes\n",
      "Average loss at step 412100: 403.850569\n",
      "Average loss at step 412200: 402.629713\n",
      "Average loss at step 412300: 402.404957\n",
      "Average loss at step 412400: 399.428417\n",
      "Average loss at step 412500: 397.039716\n",
      "Average loss at step 412600: 400.269726\n",
      "Average loss at step 412700: 399.162073\n",
      "Average loss at step 412800: 397.435745\n",
      "Average loss at step 412900: 399.404748\n",
      "Average loss at step 413000: 397.750859\n",
      "Graph 413: 37 nodes\n",
      "Average loss at step 413100: 414.553574\n",
      "Average loss at step 413200: 416.513818\n",
      "Average loss at step 413300: 410.443250\n",
      "Average loss at step 413400: 412.813518\n",
      "Average loss at step 413500: 413.708488\n",
      "Average loss at step 413600: 413.384713\n",
      "Average loss at step 413700: 415.033299\n",
      "Average loss at step 413800: 414.536309\n",
      "Average loss at step 413900: 411.364437\n",
      "Average loss at step 414000: 413.238177\n",
      "Graph 414: 34 nodes\n",
      "Average loss at step 414100: 379.108506\n",
      "Average loss at step 414200: 380.109918\n",
      "Average loss at step 414300: 377.328824\n",
      "Average loss at step 414400: 377.150331\n",
      "Average loss at step 414500: 378.997946\n",
      "Average loss at step 414600: 377.852353\n",
      "Average loss at step 414700: 376.587601\n",
      "Average loss at step 414800: 378.390140\n",
      "Average loss at step 414900: 377.843271\n",
      "Average loss at step 415000: 375.813613\n",
      "Graph 415: 24 nodes\n",
      "Average loss at step 415100: 402.043398\n",
      "Average loss at step 415200: 398.732864\n",
      "Average loss at step 415300: 397.408022\n",
      "Average loss at step 415400: 392.887793\n",
      "Average loss at step 415500: 396.511842\n",
      "Average loss at step 415600: 396.644075\n",
      "Average loss at step 415700: 397.149710\n",
      "Average loss at step 415800: 395.423219\n",
      "Average loss at step 415900: 395.624736\n",
      "Average loss at step 416000: 395.198988\n",
      "Graph 416: 25 nodes\n",
      "Average loss at step 416100: 398.082138\n",
      "Average loss at step 416200: 398.318855\n",
      "Average loss at step 416300: 392.105615\n",
      "Average loss at step 416400: 394.692832\n",
      "Average loss at step 416500: 394.859961\n",
      "Average loss at step 416600: 395.246713\n",
      "Average loss at step 416700: 393.209922\n",
      "Average loss at step 416800: 391.979658\n",
      "Average loss at step 416900: 395.849817\n",
      "Average loss at step 417000: 393.786997\n",
      "Graph 417: 41 nodes\n",
      "Average loss at step 417100: 457.477566\n",
      "Average loss at step 417200: 453.762655\n",
      "Average loss at step 417300: 451.153074\n",
      "Average loss at step 417400: 451.151757\n",
      "Average loss at step 417500: 448.720895\n",
      "Average loss at step 417600: 448.273633\n",
      "Average loss at step 417700: 448.794953\n",
      "Average loss at step 417800: 450.470181\n",
      "Average loss at step 417900: 447.215454\n",
      "Average loss at step 418000: 448.358335\n",
      "Graph 418: 27 nodes\n",
      "Average loss at step 418100: 392.856041\n",
      "Average loss at step 418200: 393.640449\n",
      "Average loss at step 418300: 388.149047\n",
      "Average loss at step 418400: 388.518643\n",
      "Average loss at step 418500: 389.880039\n",
      "Average loss at step 418600: 390.370074\n",
      "Average loss at step 418700: 387.608315\n",
      "Average loss at step 418800: 388.926296\n",
      "Average loss at step 418900: 388.684577\n",
      "Average loss at step 419000: 387.487028\n",
      "Graph 419: 26 nodes\n",
      "Average loss at step 419100: 382.783063\n",
      "Average loss at step 419200: 380.586731\n",
      "Average loss at step 419300: 377.956319\n",
      "Average loss at step 419400: 382.390359\n",
      "Average loss at step 419500: 378.864023\n",
      "Average loss at step 419600: 379.054019\n",
      "Average loss at step 419700: 377.667683\n",
      "Average loss at step 419800: 378.848101\n",
      "Average loss at step 419900: 378.930798\n",
      "Average loss at step 420000: 378.228048\n",
      "Graph 420: 25 nodes\n",
      "Average loss at step 420100: 407.015956\n",
      "Average loss at step 420200: 405.383237\n",
      "Average loss at step 420300: 401.715751\n",
      "Average loss at step 420400: 403.855837\n",
      "Average loss at step 420500: 407.269033\n",
      "Average loss at step 420600: 403.120120\n",
      "Average loss at step 420700: 402.346201\n",
      "Average loss at step 420800: 406.468836\n",
      "Average loss at step 420900: 403.271725\n",
      "Average loss at step 421000: 405.214650\n",
      "Time: 51.5195391178\n",
      "Graph 421: 27 nodes\n",
      "Average loss at step 421100: 390.130535\n",
      "Average loss at step 421200: 391.344581\n",
      "Average loss at step 421300: 390.670843\n",
      "Average loss at step 421400: 389.862111\n",
      "Average loss at step 421500: 387.325129\n",
      "Average loss at step 421600: 387.654323\n",
      "Average loss at step 421700: 386.523350\n",
      "Average loss at step 421800: 387.371279\n",
      "Average loss at step 421900: 385.718787\n",
      "Average loss at step 422000: 386.735086\n",
      "Graph 422: 20 nodes\n",
      "Average loss at step 422100: 409.832404\n",
      "Average loss at step 422200: 408.835713\n",
      "Average loss at step 422300: 406.067899\n",
      "Average loss at step 422400: 408.754325\n",
      "Average loss at step 422500: 409.383494\n",
      "Average loss at step 422600: 407.854788\n",
      "Average loss at step 422700: 405.807051\n",
      "Average loss at step 422800: 408.886509\n",
      "Average loss at step 422900: 405.697173\n",
      "Average loss at step 423000: 408.927683\n",
      "Graph 423: 40 nodes\n",
      "Average loss at step 423100: 362.976177\n",
      "Average loss at step 423200: 357.177363\n",
      "Average loss at step 423300: 356.866592\n",
      "Average loss at step 423400: 351.209357\n",
      "Average loss at step 423500: 356.198816\n",
      "Average loss at step 423600: 354.497596\n",
      "Average loss at step 423700: 354.631712\n",
      "Average loss at step 423800: 355.949349\n",
      "Average loss at step 423900: 356.516574\n",
      "Average loss at step 424000: 352.191219\n",
      "Graph 424: 21 nodes\n",
      "Average loss at step 424100: 386.579178\n",
      "Average loss at step 424200: 385.497832\n",
      "Average loss at step 424300: 384.672332\n",
      "Average loss at step 424400: 384.535614\n",
      "Average loss at step 424500: 383.160358\n",
      "Average loss at step 424600: 384.442818\n",
      "Average loss at step 424700: 380.923682\n",
      "Average loss at step 424800: 381.588719\n",
      "Average loss at step 424900: 382.347986\n",
      "Average loss at step 425000: 385.254306\n",
      "Graph 425: 19 nodes\n",
      "Average loss at step 425100: 477.925114\n",
      "Average loss at step 425200: 471.559062\n",
      "Average loss at step 425300: 469.718241\n",
      "Average loss at step 425400: 469.493817\n",
      "Average loss at step 425500: 469.149995\n",
      "Average loss at step 425600: 469.233773\n",
      "Average loss at step 425700: 467.942326\n",
      "Average loss at step 425800: 467.827056\n",
      "Average loss at step 425900: 467.668350\n",
      "Average loss at step 426000: 467.749652\n",
      "Graph 426: 36 nodes\n",
      "Average loss at step 426100: 470.499932\n",
      "Average loss at step 426200: 468.083920\n",
      "Average loss at step 426300: 468.067823\n",
      "Average loss at step 426400: 467.378498\n",
      "Average loss at step 426500: 467.736976\n",
      "Average loss at step 426600: 467.948722\n",
      "Average loss at step 426700: 465.936877\n",
      "Average loss at step 426800: 467.179162\n",
      "Average loss at step 426900: 467.585892\n",
      "Average loss at step 427000: 466.858574\n",
      "Graph 427: 32 nodes\n",
      "Average loss at step 427100: 410.037480\n",
      "Average loss at step 427200: 405.016068\n",
      "Average loss at step 427300: 404.295815\n",
      "Average loss at step 427400: 402.743567\n",
      "Average loss at step 427500: 402.856509\n",
      "Average loss at step 427600: 403.636304\n",
      "Average loss at step 427700: 400.715789\n",
      "Average loss at step 427800: 404.253133\n",
      "Average loss at step 427900: 403.406445\n",
      "Average loss at step 428000: 404.698957\n",
      "Graph 428: 28 nodes\n",
      "Average loss at step 428100: 414.166944\n",
      "Average loss at step 428200: 406.956130\n",
      "Average loss at step 428300: 409.933014\n",
      "Average loss at step 428400: 408.868139\n",
      "Average loss at step 428500: 411.159159\n",
      "Average loss at step 428600: 411.967564\n",
      "Average loss at step 428700: 409.431566\n",
      "Average loss at step 428800: 411.314973\n",
      "Average loss at step 428900: 408.942522\n",
      "Average loss at step 429000: 408.747157\n",
      "Graph 429: 37 nodes\n",
      "Average loss at step 429100: 357.831373\n",
      "Average loss at step 429200: 346.791656\n",
      "Average loss at step 429300: 345.548052\n",
      "Average loss at step 429400: 346.231214\n",
      "Average loss at step 429500: 346.514764\n",
      "Average loss at step 429600: 343.098112\n",
      "Average loss at step 429700: 343.990010\n",
      "Average loss at step 429800: 341.651634\n",
      "Average loss at step 429900: 342.164951\n",
      "Average loss at step 430000: 345.192847\n",
      "Graph 430: 18 nodes\n",
      "Average loss at step 430100: 453.284234\n",
      "Average loss at step 430200: 451.347998\n",
      "Average loss at step 430300: 450.154269\n",
      "Average loss at step 430400: 449.394364\n",
      "Average loss at step 430500: 449.610394\n",
      "Average loss at step 430600: 449.854358\n",
      "Average loss at step 430700: 447.968293\n",
      "Average loss at step 430800: 450.862365\n",
      "Average loss at step 430900: 448.149966\n",
      "Average loss at step 431000: 448.039805\n",
      "Time: 51.857708931\n",
      "Graph 431: 19 nodes\n",
      "Average loss at step 431100: 448.363235\n",
      "Average loss at step 431200: 447.532707\n",
      "Average loss at step 431300: 445.565599\n",
      "Average loss at step 431400: 447.150311\n",
      "Average loss at step 431500: 445.731557\n",
      "Average loss at step 431600: 445.858367\n",
      "Average loss at step 431700: 447.140906\n",
      "Average loss at step 431800: 446.844827\n",
      "Average loss at step 431900: 446.734797\n",
      "Average loss at step 432000: 448.344609\n",
      "Graph 432: 22 nodes\n",
      "Average loss at step 432100: 439.664253\n",
      "Average loss at step 432200: 440.026036\n",
      "Average loss at step 432300: 439.419624\n",
      "Average loss at step 432400: 436.534679\n",
      "Average loss at step 432500: 435.280739\n",
      "Average loss at step 432600: 436.536169\n",
      "Average loss at step 432700: 437.774187\n",
      "Average loss at step 432800: 435.704594\n",
      "Average loss at step 432900: 439.548954\n",
      "Average loss at step 433000: 436.910233\n",
      "Graph 433: 43 nodes\n",
      "Average loss at step 433100: 413.627207\n",
      "Average loss at step 433200: 410.104504\n",
      "Average loss at step 433300: 405.592171\n",
      "Average loss at step 433400: 407.126095\n",
      "Average loss at step 433500: 407.589612\n",
      "Average loss at step 433600: 405.903477\n",
      "Average loss at step 433700: 408.153570\n",
      "Average loss at step 433800: 405.353249\n",
      "Average loss at step 433900: 407.263982\n",
      "Average loss at step 434000: 406.081515\n",
      "Graph 434: 24 nodes\n",
      "Average loss at step 434100: 399.531388\n",
      "Average loss at step 434200: 394.884571\n",
      "Average loss at step 434300: 394.290413\n",
      "Average loss at step 434400: 394.810590\n",
      "Average loss at step 434500: 394.243570\n",
      "Average loss at step 434600: 393.811428\n",
      "Average loss at step 434700: 392.321163\n",
      "Average loss at step 434800: 392.696705\n",
      "Average loss at step 434900: 393.982046\n",
      "Average loss at step 435000: 389.531258\n",
      "Graph 435: 15 nodes\n",
      "Average loss at step 435100: 432.986123\n",
      "Average loss at step 435200: 429.538221\n",
      "Average loss at step 435300: 424.080427\n",
      "Average loss at step 435400: 428.065751\n",
      "Average loss at step 435500: 425.452879\n",
      "Average loss at step 435600: 427.139519\n",
      "Average loss at step 435700: 427.087209\n",
      "Average loss at step 435800: 428.580367\n",
      "Average loss at step 435900: 427.510288\n",
      "Average loss at step 436000: 424.424394\n",
      "Graph 436: 14 nodes\n",
      "Average loss at step 436100: 427.930856\n",
      "Average loss at step 436200: 422.779230\n",
      "Average loss at step 436300: 425.008189\n",
      "Average loss at step 436400: 422.233409\n",
      "Average loss at step 436500: 422.483660\n",
      "Average loss at step 436600: 422.321786\n",
      "Average loss at step 436700: 422.603793\n",
      "Average loss at step 436800: 422.179379\n",
      "Average loss at step 436900: 421.659453\n",
      "Average loss at step 437000: 419.598285\n",
      "Graph 437: 48 nodes\n",
      "Average loss at step 437100: 371.636211\n",
      "Average loss at step 437200: 363.223561\n",
      "Average loss at step 437300: 362.978843\n",
      "Average loss at step 437400: 368.361584\n",
      "Average loss at step 437500: 363.116529\n",
      "Average loss at step 437600: 365.723654\n",
      "Average loss at step 437700: 366.426491\n",
      "Average loss at step 437800: 362.350629\n",
      "Average loss at step 437900: 359.925436\n",
      "Average loss at step 438000: 364.212242\n",
      "Graph 438: 46 nodes\n",
      "Average loss at step 438100: 359.730777\n",
      "Average loss at step 438200: 357.590710\n",
      "Average loss at step 438300: 355.541824\n",
      "Average loss at step 438400: 355.862537\n",
      "Average loss at step 438500: 356.904360\n",
      "Average loss at step 438600: 358.424387\n",
      "Average loss at step 438700: 357.734870\n",
      "Average loss at step 438800: 353.569107\n",
      "Average loss at step 438900: 357.453162\n",
      "Average loss at step 439000: 357.267580\n",
      "Graph 439: 26 nodes\n",
      "Average loss at step 439100: 392.585581\n",
      "Average loss at step 439200: 390.860682\n",
      "Average loss at step 439300: 390.458859\n",
      "Average loss at step 439400: 388.653638\n",
      "Average loss at step 439500: 390.901006\n",
      "Average loss at step 439600: 389.530268\n",
      "Average loss at step 439700: 390.443531\n",
      "Average loss at step 439800: 387.315994\n",
      "Average loss at step 439900: 388.549959\n",
      "Average loss at step 440000: 391.108693\n",
      "Graph 440: 25 nodes\n",
      "Average loss at step 440100: 412.253103\n",
      "Average loss at step 440200: 408.679681\n",
      "Average loss at step 440300: 407.816618\n",
      "Average loss at step 440400: 404.895874\n",
      "Average loss at step 440500: 408.080537\n",
      "Average loss at step 440600: 408.241383\n",
      "Average loss at step 440700: 406.993134\n",
      "Average loss at step 440800: 405.485335\n",
      "Average loss at step 440900: 406.770841\n",
      "Average loss at step 441000: 405.667613\n",
      "Time: 54.9915719032\n",
      "Graph 441: 44 nodes\n",
      "Average loss at step 441100: 367.529836\n",
      "Average loss at step 441200: 362.812592\n",
      "Average loss at step 441300: 364.611042\n",
      "Average loss at step 441400: 368.989506\n",
      "Average loss at step 441500: 362.606193\n",
      "Average loss at step 441600: 362.104373\n",
      "Average loss at step 441700: 366.333007\n",
      "Average loss at step 441800: 363.918088\n",
      "Average loss at step 441900: 364.400144\n",
      "Average loss at step 442000: 365.292068\n",
      "Graph 442: 39 nodes\n",
      "Average loss at step 442100: 374.309314\n",
      "Average loss at step 442200: 372.024260\n",
      "Average loss at step 442300: 369.562488\n",
      "Average loss at step 442400: 365.392230\n",
      "Average loss at step 442500: 370.718169\n",
      "Average loss at step 442600: 369.749223\n",
      "Average loss at step 442700: 369.972602\n",
      "Average loss at step 442800: 369.386192\n",
      "Average loss at step 442900: 366.567237\n",
      "Average loss at step 443000: 367.849628\n",
      "Graph 443: 39 nodes\n",
      "Average loss at step 443100: 379.357406\n",
      "Average loss at step 443200: 373.634058\n",
      "Average loss at step 443300: 376.568619\n",
      "Average loss at step 443400: 372.782354\n",
      "Average loss at step 443500: 372.727961\n",
      "Average loss at step 443600: 376.668182\n",
      "Average loss at step 443700: 372.259047\n",
      "Average loss at step 443800: 377.024194\n",
      "Average loss at step 443900: 372.377426\n",
      "Average loss at step 444000: 373.458395\n",
      "Graph 444: 27 nodes\n",
      "Average loss at step 444100: 394.989214\n",
      "Average loss at step 444200: 392.610050\n",
      "Average loss at step 444300: 386.536138\n",
      "Average loss at step 444400: 388.204245\n",
      "Average loss at step 444500: 389.095519\n",
      "Average loss at step 444600: 387.231116\n",
      "Average loss at step 444700: 388.077310\n",
      "Average loss at step 444800: 385.165802\n",
      "Average loss at step 444900: 385.166501\n",
      "Average loss at step 445000: 386.044516\n",
      "Graph 445: 26 nodes\n",
      "Average loss at step 445100: 398.987807\n",
      "Average loss at step 445200: 395.951914\n",
      "Average loss at step 445300: 397.739195\n",
      "Average loss at step 445400: 397.438472\n",
      "Average loss at step 445500: 395.380625\n",
      "Average loss at step 445600: 397.817477\n",
      "Average loss at step 445700: 398.698222\n",
      "Average loss at step 445800: 399.006301\n",
      "Average loss at step 445900: 395.524350\n",
      "Average loss at step 446000: 399.731457\n",
      "Graph 446: 25 nodes\n",
      "Average loss at step 446100: 401.480805\n",
      "Average loss at step 446200: 400.234720\n",
      "Average loss at step 446300: 400.696410\n",
      "Average loss at step 446400: 393.227343\n",
      "Average loss at step 446500: 401.043179\n",
      "Average loss at step 446600: 398.387763\n",
      "Average loss at step 446700: 402.427432\n",
      "Average loss at step 446800: 397.996282\n",
      "Average loss at step 446900: 400.708354\n",
      "Average loss at step 447000: 397.700911\n",
      "Graph 447: 39 nodes\n",
      "Average loss at step 447100: 368.197532\n",
      "Average loss at step 447200: 368.526928\n",
      "Average loss at step 447300: 366.526443\n",
      "Average loss at step 447400: 364.993993\n",
      "Average loss at step 447500: 366.213639\n",
      "Average loss at step 447600: 366.512723\n",
      "Average loss at step 447700: 367.208158\n",
      "Average loss at step 447800: 366.366333\n",
      "Average loss at step 447900: 368.084217\n",
      "Average loss at step 448000: 367.390300\n",
      "Graph 448: 26 nodes\n",
      "Average loss at step 448100: 401.077206\n",
      "Average loss at step 448200: 402.385408\n",
      "Average loss at step 448300: 395.256862\n",
      "Average loss at step 448400: 399.611845\n",
      "Average loss at step 448500: 397.751799\n",
      "Average loss at step 448600: 396.944407\n",
      "Average loss at step 448700: 397.485137\n",
      "Average loss at step 448800: 396.639868\n",
      "Average loss at step 448900: 397.808918\n",
      "Average loss at step 449000: 394.436002\n",
      "Graph 449: 26 nodes\n",
      "Average loss at step 449100: 401.328094\n",
      "Average loss at step 449200: 402.086021\n",
      "Average loss at step 449300: 403.272126\n",
      "Average loss at step 449400: 399.861474\n",
      "Average loss at step 449500: 400.990675\n",
      "Average loss at step 449600: 399.484550\n",
      "Average loss at step 449700: 398.451160\n",
      "Average loss at step 449800: 399.347087\n",
      "Average loss at step 449900: 399.764559\n",
      "Average loss at step 450000: 400.513924\n",
      "Graph 450: 26 nodes\n",
      "Average loss at step 450100: 410.233259\n",
      "Average loss at step 450200: 408.531521\n",
      "Average loss at step 450300: 410.468378\n",
      "Average loss at step 450400: 407.276863\n",
      "Average loss at step 450500: 410.218432\n",
      "Average loss at step 450600: 408.648022\n",
      "Average loss at step 450700: 407.557397\n",
      "Average loss at step 450800: 404.782673\n",
      "Average loss at step 450900: 413.012332\n",
      "Average loss at step 451000: 407.307982\n",
      "Time: 52.1323530674\n",
      "Graph 451: 34 nodes\n",
      "Average loss at step 451100: 401.063715\n",
      "Average loss at step 451200: 399.448728\n",
      "Average loss at step 451300: 399.101285\n",
      "Average loss at step 451400: 398.659425\n",
      "Average loss at step 451500: 398.587725\n",
      "Average loss at step 451600: 399.257362\n",
      "Average loss at step 451700: 396.459520\n",
      "Average loss at step 451800: 396.450116\n",
      "Average loss at step 451900: 397.122478\n",
      "Average loss at step 452000: 391.956970\n",
      "Graph 452: 20 nodes\n",
      "Average loss at step 452100: 405.348867\n",
      "Average loss at step 452200: 403.718193\n",
      "Average loss at step 452300: 399.091700\n",
      "Average loss at step 452400: 403.973078\n",
      "Average loss at step 452500: 400.666033\n",
      "Average loss at step 452600: 403.858116\n",
      "Average loss at step 452700: 402.257501\n",
      "Average loss at step 452800: 401.251101\n",
      "Average loss at step 452900: 400.579522\n",
      "Average loss at step 453000: 400.924879\n",
      "Graph 453: 20 nodes\n",
      "Average loss at step 453100: 407.083852\n",
      "Average loss at step 453200: 406.546911\n",
      "Average loss at step 453300: 404.992827\n",
      "Average loss at step 453400: 406.975673\n",
      "Average loss at step 453500: 407.029243\n",
      "Average loss at step 453600: 406.356256\n",
      "Average loss at step 453700: 407.233203\n",
      "Average loss at step 453800: 401.911116\n",
      "Average loss at step 453900: 407.071854\n",
      "Average loss at step 454000: 405.041313\n",
      "Graph 454: 19 nodes\n",
      "Average loss at step 454100: 405.573261\n",
      "Average loss at step 454200: 402.694660\n",
      "Average loss at step 454300: 404.707755\n",
      "Average loss at step 454400: 404.188610\n",
      "Average loss at step 454500: 404.365788\n",
      "Average loss at step 454600: 400.621354\n",
      "Average loss at step 454700: 402.246396\n",
      "Average loss at step 454800: 403.948903\n",
      "Average loss at step 454900: 401.603332\n",
      "Average loss at step 455000: 402.650991\n",
      "Graph 455: 18 nodes\n",
      "Average loss at step 455100: 408.053730\n",
      "Average loss at step 455200: 404.675497\n",
      "Average loss at step 455300: 403.032353\n",
      "Average loss at step 455400: 402.032358\n",
      "Average loss at step 455500: 407.388017\n",
      "Average loss at step 455600: 404.524803\n",
      "Average loss at step 455700: 402.067488\n",
      "Average loss at step 455800: 401.143192\n",
      "Average loss at step 455900: 405.657830\n",
      "Average loss at step 456000: 404.175457\n",
      "Graph 456: 33 nodes\n",
      "Average loss at step 456100: 360.255469\n",
      "Average loss at step 456200: 353.575430\n",
      "Average loss at step 456300: 352.767982\n",
      "Average loss at step 456400: 348.804998\n",
      "Average loss at step 456500: 348.847586\n",
      "Average loss at step 456600: 350.570318\n",
      "Average loss at step 456700: 348.754692\n",
      "Average loss at step 456800: 346.176052\n",
      "Average loss at step 456900: 346.620200\n",
      "Average loss at step 457000: 345.775280\n",
      "Graph 457: 15 nodes\n",
      "Average loss at step 457100: 427.279505\n",
      "Average loss at step 457200: 419.049660\n",
      "Average loss at step 457300: 417.655065\n",
      "Average loss at step 457400: 417.450474\n",
      "Average loss at step 457500: 418.054692\n",
      "Average loss at step 457600: 415.215633\n",
      "Average loss at step 457700: 419.769075\n",
      "Average loss at step 457800: 413.210421\n",
      "Average loss at step 457900: 417.217357\n",
      "Average loss at step 458000: 418.879446\n",
      "Graph 458: 34 nodes\n",
      "Average loss at step 458100: 379.163625\n",
      "Average loss at step 458200: 378.061211\n",
      "Average loss at step 458300: 374.610682\n",
      "Average loss at step 458400: 373.209995\n",
      "Average loss at step 458500: 374.420866\n",
      "Average loss at step 458600: 373.479496\n",
      "Average loss at step 458700: 373.587570\n",
      "Average loss at step 458800: 373.528115\n",
      "Average loss at step 458900: 370.970286\n",
      "Average loss at step 459000: 371.477020\n",
      "Graph 459: 28 nodes\n",
      "Average loss at step 459100: 377.381463\n",
      "Average loss at step 459200: 375.411584\n",
      "Average loss at step 459300: 374.092185\n",
      "Average loss at step 459400: 372.803852\n",
      "Average loss at step 459500: 374.394512\n",
      "Average loss at step 459600: 372.523422\n",
      "Average loss at step 459700: 369.753651\n",
      "Average loss at step 459800: 374.639052\n",
      "Average loss at step 459900: 370.783726\n",
      "Average loss at step 460000: 371.691985\n",
      "Graph 460: 34 nodes\n",
      "Average loss at step 460100: 411.101294\n",
      "Average loss at step 460200: 412.923350\n",
      "Average loss at step 460300: 412.631451\n",
      "Average loss at step 460400: 411.335860\n",
      "Average loss at step 460500: 411.604962\n",
      "Average loss at step 460600: 412.982282\n",
      "Average loss at step 460700: 408.645331\n",
      "Average loss at step 460800: 410.052521\n",
      "Average loss at step 460900: 410.104557\n",
      "Average loss at step 461000: 408.566574\n",
      "Time: 54.4515450001\n",
      "Graph 461: 15 nodes\n",
      "Average loss at step 461100: 430.999051\n",
      "Average loss at step 461200: 426.676316\n",
      "Average loss at step 461300: 427.953290\n",
      "Average loss at step 461400: 426.186883\n",
      "Average loss at step 461500: 427.174605\n",
      "Average loss at step 461600: 426.316217\n",
      "Average loss at step 461700: 425.326156\n",
      "Average loss at step 461800: 425.234480\n",
      "Average loss at step 461900: 426.349486\n",
      "Average loss at step 462000: 425.982775\n",
      "Graph 462: 14 nodes\n",
      "Average loss at step 462100: 442.331977\n",
      "Average loss at step 462200: 438.434554\n",
      "Average loss at step 462300: 439.272994\n",
      "Average loss at step 462400: 438.267718\n",
      "Average loss at step 462500: 439.778955\n",
      "Average loss at step 462600: 440.434082\n",
      "Average loss at step 462700: 439.702522\n",
      "Average loss at step 462800: 438.221189\n",
      "Average loss at step 462900: 438.745595\n",
      "Average loss at step 463000: 438.822229\n",
      "Graph 463: 20 nodes\n",
      "Average loss at step 463100: 401.962864\n",
      "Average loss at step 463200: 402.121446\n",
      "Average loss at step 463300: 401.532521\n",
      "Average loss at step 463400: 401.598048\n",
      "Average loss at step 463500: 403.269195\n",
      "Average loss at step 463600: 399.981861\n",
      "Average loss at step 463700: 399.955687\n",
      "Average loss at step 463800: 401.186184\n",
      "Average loss at step 463900: 401.110859\n",
      "Average loss at step 464000: 400.112198\n",
      "Graph 464: 52 nodes\n",
      "Average loss at step 464100: 383.419623\n",
      "Average loss at step 464200: 377.859211\n",
      "Average loss at step 464300: 376.209009\n",
      "Average loss at step 464400: 377.845243\n",
      "Average loss at step 464500: 380.171095\n",
      "Average loss at step 464600: 378.182533\n",
      "Average loss at step 464700: 379.382922\n",
      "Average loss at step 464800: 375.817727\n",
      "Average loss at step 464900: 379.058681\n",
      "Average loss at step 465000: 378.677981\n",
      "Graph 465: 20 nodes\n",
      "Average loss at step 465100: 407.787024\n",
      "Average loss at step 465200: 405.563393\n",
      "Average loss at step 465300: 403.445371\n",
      "Average loss at step 465400: 404.504721\n",
      "Average loss at step 465500: 405.154124\n",
      "Average loss at step 465600: 403.570742\n",
      "Average loss at step 465700: 404.507683\n",
      "Average loss at step 465800: 403.828986\n",
      "Average loss at step 465900: 405.456940\n",
      "Average loss at step 466000: 403.011267\n",
      "Graph 466: 20 nodes\n",
      "Average loss at step 466100: 414.573039\n",
      "Average loss at step 466200: 413.049034\n",
      "Average loss at step 466300: 413.226294\n",
      "Average loss at step 466400: 409.191639\n",
      "Average loss at step 466500: 406.699336\n",
      "Average loss at step 466600: 412.699670\n",
      "Average loss at step 466700: 409.082042\n",
      "Average loss at step 466800: 411.169789\n",
      "Average loss at step 466900: 411.533631\n",
      "Average loss at step 467000: 410.519525\n",
      "Graph 467: 20 nodes\n",
      "Average loss at step 467100: 400.568346\n",
      "Average loss at step 467200: 400.005351\n",
      "Average loss at step 467300: 398.006970\n",
      "Average loss at step 467400: 396.621965\n",
      "Average loss at step 467500: 399.246466\n",
      "Average loss at step 467600: 395.271326\n",
      "Average loss at step 467700: 398.597470\n",
      "Average loss at step 467800: 400.382379\n",
      "Average loss at step 467900: 399.432234\n",
      "Average loss at step 468000: 396.865272\n",
      "Graph 468: 17 nodes\n",
      "Average loss at step 468100: 468.486799\n",
      "Average loss at step 468200: 463.845937\n",
      "Average loss at step 468300: 464.725159\n",
      "Average loss at step 468400: 462.917554\n",
      "Average loss at step 468500: 464.116507\n",
      "Average loss at step 468600: 461.852860\n",
      "Average loss at step 468700: 461.347026\n",
      "Average loss at step 468800: 463.228945\n",
      "Average loss at step 468900: 461.391123\n",
      "Average loss at step 469000: 462.780861\n",
      "Graph 469: 12 nodes\n",
      "Average loss at step 469100: 460.387561\n",
      "Average loss at step 469200: 456.025093\n",
      "Average loss at step 469300: 458.792564\n",
      "Average loss at step 469400: 457.986381\n",
      "Average loss at step 469500: 456.635726\n",
      "Average loss at step 469600: 454.668188\n",
      "Average loss at step 469700: 456.965364\n",
      "Average loss at step 469800: 455.919899\n",
      "Average loss at step 469900: 456.179983\n",
      "Average loss at step 470000: 454.737781\n",
      "Graph 470: 24 nodes\n",
      "Average loss at step 470100: 376.356347\n",
      "Average loss at step 470200: 373.329293\n",
      "Average loss at step 470300: 368.232728\n",
      "Average loss at step 470400: 366.671668\n",
      "Average loss at step 470500: 368.858852\n",
      "Average loss at step 470600: 368.488410\n",
      "Average loss at step 470700: 367.360804\n",
      "Average loss at step 470800: 364.724700\n",
      "Average loss at step 470900: 366.899321\n",
      "Average loss at step 471000: 365.245258\n",
      "Time: 48.9665298462\n",
      "Graph 471: 46 nodes\n",
      "Average loss at step 471100: 455.168832\n",
      "Average loss at step 471200: 451.574175\n",
      "Average loss at step 471300: 451.015907\n",
      "Average loss at step 471400: 452.653822\n",
      "Average loss at step 471500: 453.352376\n",
      "Average loss at step 471600: 451.266882\n",
      "Average loss at step 471700: 449.656162\n",
      "Average loss at step 471800: 450.202199\n",
      "Average loss at step 471900: 451.690876\n",
      "Average loss at step 472000: 453.168292\n",
      "Graph 472: 13 nodes\n",
      "Average loss at step 472100: 459.200719\n",
      "Average loss at step 472200: 458.761228\n",
      "Average loss at step 472300: 457.915296\n",
      "Average loss at step 472400: 457.304379\n",
      "Average loss at step 472500: 455.711840\n",
      "Average loss at step 472600: 456.067299\n",
      "Average loss at step 472700: 455.417677\n",
      "Average loss at step 472800: 455.109130\n",
      "Average loss at step 472900: 455.618632\n",
      "Average loss at step 473000: 456.581207\n",
      "Graph 473: 24 nodes\n",
      "Average loss at step 473100: 420.702676\n",
      "Average loss at step 473200: 415.857708\n",
      "Average loss at step 473300: 417.383617\n",
      "Average loss at step 473400: 416.617978\n",
      "Average loss at step 473500: 413.842010\n",
      "Average loss at step 473600: 413.336031\n",
      "Average loss at step 473700: 415.932824\n",
      "Average loss at step 473800: 415.683451\n",
      "Average loss at step 473900: 413.588945\n",
      "Average loss at step 474000: 414.953189\n",
      "Graph 474: 12 nodes\n",
      "Average loss at step 474100: 457.657266\n",
      "Average loss at step 474200: 455.958204\n",
      "Average loss at step 474300: 454.457364\n",
      "Average loss at step 474400: 452.312030\n",
      "Average loss at step 474500: 451.777442\n",
      "Average loss at step 474600: 452.942192\n",
      "Average loss at step 474700: 454.106894\n",
      "Average loss at step 474800: 452.488627\n",
      "Average loss at step 474900: 450.351581\n",
      "Average loss at step 475000: 452.775642\n",
      "Graph 475: 12 nodes\n",
      "Average loss at step 475100: 469.416282\n",
      "Average loss at step 475200: 466.804182\n",
      "Average loss at step 475300: 466.857004\n",
      "Average loss at step 475400: 465.634978\n",
      "Average loss at step 475500: 465.642997\n",
      "Average loss at step 475600: 465.400371\n",
      "Average loss at step 475700: 466.226354\n",
      "Average loss at step 475800: 465.640589\n",
      "Average loss at step 475900: 465.696146\n",
      "Average loss at step 476000: 466.530647\n",
      "Graph 476: 14 nodes\n",
      "Average loss at step 476100: 462.833331\n",
      "Average loss at step 476200: 460.888534\n",
      "Average loss at step 476300: 459.361207\n",
      "Average loss at step 476400: 460.565370\n",
      "Average loss at step 476500: 460.332473\n",
      "Average loss at step 476600: 457.696268\n",
      "Average loss at step 476700: 460.366668\n",
      "Average loss at step 476800: 460.096603\n",
      "Average loss at step 476900: 459.279155\n",
      "Average loss at step 477000: 459.947048\n",
      "Graph 477: 20 nodes\n",
      "Average loss at step 477100: 402.523588\n",
      "Average loss at step 477200: 401.142161\n",
      "Average loss at step 477300: 400.915222\n",
      "Average loss at step 477400: 403.048347\n",
      "Average loss at step 477500: 396.147712\n",
      "Average loss at step 477600: 399.233824\n",
      "Average loss at step 477700: 399.284394\n",
      "Average loss at step 477800: 397.940962\n",
      "Average loss at step 477900: 400.409254\n",
      "Average loss at step 478000: 400.420693\n",
      "Graph 478: 28 nodes\n",
      "Average loss at step 478100: 393.412538\n",
      "Average loss at step 478200: 388.869840\n",
      "Average loss at step 478300: 391.122941\n",
      "Average loss at step 478400: 390.224400\n",
      "Average loss at step 478500: 392.730179\n",
      "Average loss at step 478600: 391.503963\n",
      "Average loss at step 478700: 391.107513\n",
      "Average loss at step 478800: 390.711568\n",
      "Average loss at step 478900: 390.187269\n",
      "Average loss at step 479000: 390.288958\n",
      "Graph 479: 24 nodes\n",
      "Average loss at step 479100: 378.844911\n",
      "Average loss at step 479200: 373.162062\n",
      "Average loss at step 479300: 373.908438\n",
      "Average loss at step 479400: 372.718724\n",
      "Average loss at step 479500: 372.014980\n",
      "Average loss at step 479600: 369.921303\n",
      "Average loss at step 479700: 371.406259\n",
      "Average loss at step 479800: 371.395577\n",
      "Average loss at step 479900: 369.584631\n",
      "Average loss at step 480000: 368.251724\n",
      "Graph 480: 12 nodes\n",
      "Average loss at step 480100: 462.145732\n",
      "Average loss at step 480200: 459.659800\n",
      "Average loss at step 480300: 457.703597\n",
      "Average loss at step 480400: 457.095033\n",
      "Average loss at step 480500: 457.540449\n",
      "Average loss at step 480600: 457.419802\n",
      "Average loss at step 480700: 457.843280\n",
      "Average loss at step 480800: 456.974157\n",
      "Average loss at step 480900: 455.407032\n",
      "Average loss at step 481000: 454.154736\n",
      "Time: 56.6235678196\n",
      "Graph 481: 26 nodes\n",
      "Average loss at step 481100: 406.256558\n",
      "Average loss at step 481200: 402.182253\n",
      "Average loss at step 481300: 402.261003\n",
      "Average loss at step 481400: 401.574586\n",
      "Average loss at step 481500: 401.834263\n",
      "Average loss at step 481600: 401.867615\n",
      "Average loss at step 481700: 400.475534\n",
      "Average loss at step 481800: 403.121155\n",
      "Average loss at step 481900: 401.431449\n",
      "Average loss at step 482000: 400.464118\n",
      "Graph 482: 22 nodes\n",
      "Average loss at step 482100: 414.122576\n",
      "Average loss at step 482200: 412.318365\n",
      "Average loss at step 482300: 408.439229\n",
      "Average loss at step 482400: 410.483370\n",
      "Average loss at step 482500: 411.808959\n",
      "Average loss at step 482600: 409.505924\n",
      "Average loss at step 482700: 410.911978\n",
      "Average loss at step 482800: 410.849648\n",
      "Average loss at step 482900: 405.467462\n",
      "Average loss at step 483000: 407.999537\n",
      "Graph 483: 60 nodes\n",
      "Average loss at step 483100: 367.461999\n",
      "Average loss at step 483200: 364.115976\n",
      "Average loss at step 483300: 362.722590\n",
      "Average loss at step 483400: 361.117647\n",
      "Average loss at step 483500: 362.716184\n",
      "Average loss at step 483600: 360.882658\n",
      "Average loss at step 483700: 360.293360\n",
      "Average loss at step 483800: 358.953921\n",
      "Average loss at step 483900: 359.925306\n",
      "Average loss at step 484000: 358.190952\n",
      "Graph 484: 39 nodes\n",
      "Average loss at step 484100: 398.897202\n",
      "Average loss at step 484200: 393.845957\n",
      "Average loss at step 484300: 394.089833\n",
      "Average loss at step 484400: 394.964505\n",
      "Average loss at step 484500: 391.891714\n",
      "Average loss at step 484600: 392.984515\n",
      "Average loss at step 484700: 391.755110\n",
      "Average loss at step 484800: 390.036653\n",
      "Average loss at step 484900: 393.927584\n",
      "Average loss at step 485000: 393.828000\n",
      "Graph 485: 26 nodes\n",
      "Average loss at step 485100: 403.084312\n",
      "Average loss at step 485200: 396.959873\n",
      "Average loss at step 485300: 398.967594\n",
      "Average loss at step 485400: 399.749986\n",
      "Average loss at step 485500: 399.532341\n",
      "Average loss at step 485600: 398.400242\n",
      "Average loss at step 485700: 399.292615\n",
      "Average loss at step 485800: 399.915928\n",
      "Average loss at step 485900: 399.551372\n",
      "Average loss at step 486000: 396.404351\n",
      "Graph 486: 15 nodes\n",
      "Average loss at step 486100: 473.982084\n",
      "Average loss at step 486200: 468.437800\n",
      "Average loss at step 486300: 465.627358\n",
      "Average loss at step 486400: 465.480721\n",
      "Average loss at step 486500: 465.027089\n",
      "Average loss at step 486600: 464.951729\n",
      "Average loss at step 486700: 464.933111\n",
      "Average loss at step 486800: 465.037619\n",
      "Average loss at step 486900: 464.598027\n",
      "Average loss at step 487000: 463.575895\n",
      "Graph 487: 25 nodes\n",
      "Average loss at step 487100: 461.305731\n",
      "Average loss at step 487200: 457.751170\n",
      "Average loss at step 487300: 456.378496\n",
      "Average loss at step 487400: 453.591612\n",
      "Average loss at step 487500: 456.701072\n",
      "Average loss at step 487600: 455.024312\n",
      "Average loss at step 487700: 457.419486\n",
      "Average loss at step 487800: 455.744854\n",
      "Average loss at step 487900: 453.189809\n",
      "Average loss at step 488000: 452.638114\n",
      "Graph 488: 35 nodes\n",
      "Average loss at step 488100: 451.455064\n",
      "Average loss at step 488200: 450.190395\n",
      "Average loss at step 488300: 448.742491\n",
      "Average loss at step 488400: 448.476445\n",
      "Average loss at step 488500: 448.159997\n",
      "Average loss at step 488600: 448.896197\n",
      "Average loss at step 488700: 450.251607\n",
      "Average loss at step 488800: 449.188071\n",
      "Average loss at step 488900: 450.543047\n",
      "Average loss at step 489000: 448.035127\n",
      "Graph 489: 35 nodes\n",
      "Average loss at step 489100: 445.168318\n",
      "Average loss at step 489200: 440.727294\n",
      "Average loss at step 489300: 443.331871\n",
      "Average loss at step 489400: 440.395628\n",
      "Average loss at step 489500: 442.077288\n",
      "Average loss at step 489600: 444.007971\n",
      "Average loss at step 489700: 444.012574\n",
      "Average loss at step 489800: 440.235854\n",
      "Average loss at step 489900: 443.718362\n",
      "Average loss at step 490000: 443.795589\n",
      "Graph 490: 38 nodes\n",
      "Average loss at step 490100: 461.953890\n",
      "Average loss at step 490200: 459.228397\n",
      "Average loss at step 490300: 459.138818\n",
      "Average loss at step 490400: 457.770185\n",
      "Average loss at step 490500: 458.074174\n",
      "Average loss at step 490600: 457.562672\n",
      "Average loss at step 490700: 458.233573\n",
      "Average loss at step 490800: 457.998472\n",
      "Average loss at step 490900: 457.269193\n",
      "Average loss at step 491000: 457.227134\n",
      "Time: 51.699753046\n",
      "Graph 491: 38 nodes\n",
      "Average loss at step 491100: 419.975711\n",
      "Average loss at step 491200: 419.753680\n",
      "Average loss at step 491300: 420.094588\n",
      "Average loss at step 491400: 418.488544\n",
      "Average loss at step 491500: 416.635810\n",
      "Average loss at step 491600: 419.588098\n",
      "Average loss at step 491700: 419.257797\n",
      "Average loss at step 491800: 417.647283\n",
      "Average loss at step 491900: 421.363604\n",
      "Average loss at step 492000: 418.502948\n",
      "Graph 492: 33 nodes\n",
      "Average loss at step 492100: 448.534027\n",
      "Average loss at step 492200: 445.567251\n",
      "Average loss at step 492300: 443.859766\n",
      "Average loss at step 492400: 443.741513\n",
      "Average loss at step 492500: 445.584333\n",
      "Average loss at step 492600: 444.921545\n",
      "Average loss at step 492700: 444.165379\n",
      "Average loss at step 492800: 447.302216\n",
      "Average loss at step 492900: 445.345943\n",
      "Average loss at step 493000: 444.030436\n",
      "Graph 493: 32 nodes\n",
      "Average loss at step 493100: 445.741228\n",
      "Average loss at step 493200: 444.504608\n",
      "Average loss at step 493300: 445.494211\n",
      "Average loss at step 493400: 443.278500\n",
      "Average loss at step 493500: 442.893757\n",
      "Average loss at step 493600: 444.885162\n",
      "Average loss at step 493700: 445.325045\n",
      "Average loss at step 493800: 442.852807\n",
      "Average loss at step 493900: 441.942307\n",
      "Average loss at step 494000: 444.296733\n",
      "Graph 494: 42 nodes\n",
      "Average loss at step 494100: 448.881565\n",
      "Average loss at step 494200: 447.552510\n",
      "Average loss at step 494300: 448.157218\n",
      "Average loss at step 494400: 446.961233\n",
      "Average loss at step 494500: 447.759366\n",
      "Average loss at step 494600: 448.041148\n",
      "Average loss at step 494700: 448.241480\n",
      "Average loss at step 494800: 447.010967\n",
      "Average loss at step 494900: 449.883201\n",
      "Average loss at step 495000: 449.033490\n",
      "Graph 495: 42 nodes\n",
      "Average loss at step 495100: 447.427656\n",
      "Average loss at step 495200: 447.168266\n",
      "Average loss at step 495300: 444.523561\n",
      "Average loss at step 495400: 447.049098\n",
      "Average loss at step 495500: 446.908381\n",
      "Average loss at step 495600: 446.313903\n",
      "Average loss at step 495700: 447.358740\n",
      "Average loss at step 495800: 445.920151\n",
      "Average loss at step 495900: 444.706806\n",
      "Average loss at step 496000: 446.464976\n",
      "Graph 496: 41 nodes\n",
      "Average loss at step 496100: 445.818797\n",
      "Average loss at step 496200: 446.109670\n",
      "Average loss at step 496300: 444.591998\n",
      "Average loss at step 496400: 444.014200\n",
      "Average loss at step 496500: 445.539541\n",
      "Average loss at step 496600: 444.814223\n",
      "Average loss at step 496700: 442.928598\n",
      "Average loss at step 496800: 442.103327\n",
      "Average loss at step 496900: 444.214246\n",
      "Average loss at step 497000: 443.885407\n",
      "Graph 497: 40 nodes\n",
      "Average loss at step 497100: 452.253708\n",
      "Average loss at step 497200: 448.728383\n",
      "Average loss at step 497300: 449.709878\n",
      "Average loss at step 497400: 449.297225\n",
      "Average loss at step 497500: 449.919910\n",
      "Average loss at step 497600: 449.958130\n",
      "Average loss at step 497700: 451.430052\n",
      "Average loss at step 497800: 446.713515\n",
      "Average loss at step 497900: 449.201256\n",
      "Average loss at step 498000: 448.866388\n",
      "Graph 498: 38 nodes\n",
      "Average loss at step 498100: 436.259989\n",
      "Average loss at step 498200: 434.595355\n",
      "Average loss at step 498300: 435.184363\n",
      "Average loss at step 498400: 434.644663\n",
      "Average loss at step 498500: 434.249085\n",
      "Average loss at step 498600: 436.527348\n",
      "Average loss at step 498700: 434.758989\n",
      "Average loss at step 498800: 435.372518\n",
      "Average loss at step 498900: 436.180068\n",
      "Average loss at step 499000: 434.142919\n",
      "Graph 499: 43 nodes\n",
      "Average loss at step 499100: 407.077732\n",
      "Average loss at step 499200: 403.923539\n",
      "Average loss at step 499300: 402.989666\n",
      "Average loss at step 499400: 408.031161\n",
      "Average loss at step 499500: 406.434831\n",
      "Average loss at step 499600: 403.946747\n",
      "Average loss at step 499700: 403.242973\n",
      "Average loss at step 499800: 401.691728\n",
      "Average loss at step 499900: 403.309541\n",
      "Average loss at step 500000: 402.284978\n",
      "Graph 500: 66 nodes\n",
      "Average loss at step 500100: 340.841765\n",
      "Average loss at step 500200: 335.050077\n",
      "Average loss at step 500300: 330.978390\n",
      "Average loss at step 500400: 331.605995\n",
      "Average loss at step 500500: 329.087261\n",
      "Average loss at step 500600: 329.037202\n",
      "Average loss at step 500700: 328.172552\n",
      "Average loss at step 500800: 330.737072\n",
      "Average loss at step 500900: 328.775073\n",
      "Average loss at step 501000: 328.296725\n",
      "Time: 47.5362899303\n",
      "Graph 501: 36 nodes\n",
      "Average loss at step 501100: 441.097428\n",
      "Average loss at step 501200: 439.213597\n",
      "Average loss at step 501300: 436.517802\n",
      "Average loss at step 501400: 437.782631\n",
      "Average loss at step 501500: 440.984579\n",
      "Average loss at step 501600: 435.541327\n",
      "Average loss at step 501700: 435.814831\n",
      "Average loss at step 501800: 436.806108\n",
      "Average loss at step 501900: 431.348649\n",
      "Average loss at step 502000: 433.548747\n",
      "Graph 502: 46 nodes\n",
      "Average loss at step 502100: 427.993996\n",
      "Average loss at step 502200: 427.389289\n",
      "Average loss at step 502300: 425.880597\n",
      "Average loss at step 502400: 426.070000\n",
      "Average loss at step 502500: 420.842963\n",
      "Average loss at step 502600: 421.760934\n",
      "Average loss at step 502700: 420.278668\n",
      "Average loss at step 502800: 420.775659\n",
      "Average loss at step 502900: 420.454741\n",
      "Average loss at step 503000: 422.177878\n",
      "Graph 503: 66 nodes\n",
      "Average loss at step 503100: 362.215487\n",
      "Average loss at step 503200: 354.862332\n",
      "Average loss at step 503300: 352.993338\n",
      "Average loss at step 503400: 356.419891\n",
      "Average loss at step 503500: 355.028143\n",
      "Average loss at step 503600: 354.903078\n",
      "Average loss at step 503700: 356.136111\n",
      "Average loss at step 503800: 352.048815\n",
      "Average loss at step 503900: 357.740364\n",
      "Average loss at step 504000: 355.897635\n",
      "Graph 504: 21 nodes\n",
      "Average loss at step 504100: 439.413517\n",
      "Average loss at step 504200: 434.781137\n",
      "Average loss at step 504300: 432.596468\n",
      "Average loss at step 504400: 433.482481\n",
      "Average loss at step 504500: 430.971543\n",
      "Average loss at step 504600: 429.325527\n",
      "Average loss at step 504700: 431.927277\n",
      "Average loss at step 504800: 431.365045\n",
      "Average loss at step 504900: 430.719230\n",
      "Average loss at step 505000: 430.821952\n",
      "Graph 505: 22 nodes\n",
      "Average loss at step 505100: 416.358526\n",
      "Average loss at step 505200: 413.231884\n",
      "Average loss at step 505300: 412.665742\n",
      "Average loss at step 505400: 413.655201\n",
      "Average loss at step 505500: 413.189305\n",
      "Average loss at step 505600: 412.468255\n",
      "Average loss at step 505700: 413.375685\n",
      "Average loss at step 505800: 414.939449\n",
      "Average loss at step 505900: 414.707606\n",
      "Average loss at step 506000: 413.260953\n",
      "Graph 506: 21 nodes\n",
      "Average loss at step 506100: 426.070606\n",
      "Average loss at step 506200: 424.755468\n",
      "Average loss at step 506300: 421.805344\n",
      "Average loss at step 506400: 421.528483\n",
      "Average loss at step 506500: 422.002730\n",
      "Average loss at step 506600: 425.012736\n",
      "Average loss at step 506700: 423.243134\n",
      "Average loss at step 506800: 424.212759\n",
      "Average loss at step 506900: 423.250593\n",
      "Average loss at step 507000: 424.371941\n",
      "Graph 507: 21 nodes\n",
      "Average loss at step 507100: 417.095199\n",
      "Average loss at step 507200: 415.366100\n",
      "Average loss at step 507300: 412.475258\n",
      "Average loss at step 507400: 414.087642\n",
      "Average loss at step 507500: 413.829369\n",
      "Average loss at step 507600: 414.520272\n",
      "Average loss at step 507700: 413.154928\n",
      "Average loss at step 507800: 413.159807\n",
      "Average loss at step 507900: 414.500499\n",
      "Average loss at step 508000: 414.304116\n",
      "Graph 508: 41 nodes\n",
      "Average loss at step 508100: 374.650134\n",
      "Average loss at step 508200: 367.417692\n",
      "Average loss at step 508300: 364.306648\n",
      "Average loss at step 508400: 371.698305\n",
      "Average loss at step 508500: 369.225630\n",
      "Average loss at step 508600: 370.967314\n",
      "Average loss at step 508700: 369.731311\n",
      "Average loss at step 508800: 370.939434\n",
      "Average loss at step 508900: 365.485456\n",
      "Average loss at step 509000: 370.456642\n",
      "Graph 509: 29 nodes\n",
      "Average loss at step 509100: 413.146706\n",
      "Average loss at step 509200: 413.199922\n",
      "Average loss at step 509300: 412.901626\n",
      "Average loss at step 509400: 414.343894\n",
      "Average loss at step 509500: 409.922789\n",
      "Average loss at step 509600: 413.745764\n",
      "Average loss at step 509700: 415.175119\n",
      "Average loss at step 509800: 414.208471\n",
      "Average loss at step 509900: 415.644839\n",
      "Average loss at step 510000: 414.241766\n",
      "Graph 510: 30 nodes\n",
      "Average loss at step 510100: 408.613339\n",
      "Average loss at step 510200: 407.655911\n",
      "Average loss at step 510300: 407.341403\n",
      "Average loss at step 510400: 402.694330\n",
      "Average loss at step 510500: 409.343367\n",
      "Average loss at step 510600: 407.126187\n",
      "Average loss at step 510700: 405.516853\n",
      "Average loss at step 510800: 403.973176\n",
      "Average loss at step 510900: 408.450137\n",
      "Average loss at step 511000: 408.835958\n",
      "Time: 55.5300550461\n",
      "Graph 511: 30 nodes\n",
      "Average loss at step 511100: 406.321355\n",
      "Average loss at step 511200: 406.944691\n",
      "Average loss at step 511300: 407.918478\n",
      "Average loss at step 511400: 405.501782\n",
      "Average loss at step 511500: 402.325170\n",
      "Average loss at step 511600: 406.978252\n",
      "Average loss at step 511700: 403.131104\n",
      "Average loss at step 511800: 409.905573\n",
      "Average loss at step 511900: 405.711929\n",
      "Average loss at step 512000: 403.811029\n",
      "Graph 512: 41 nodes\n",
      "Average loss at step 512100: 409.202704\n",
      "Average loss at step 512200: 406.276145\n",
      "Average loss at step 512300: 406.111884\n",
      "Average loss at step 512400: 407.080283\n",
      "Average loss at step 512500: 402.173363\n",
      "Average loss at step 512600: 406.182868\n",
      "Average loss at step 512700: 403.199620\n",
      "Average loss at step 512800: 406.274776\n",
      "Average loss at step 512900: 407.675215\n",
      "Average loss at step 513000: 403.023385\n",
      "Graph 513: 37 nodes\n",
      "Average loss at step 513100: 401.361008\n",
      "Average loss at step 513200: 402.445238\n",
      "Average loss at step 513300: 399.656158\n",
      "Average loss at step 513400: 399.202527\n",
      "Average loss at step 513500: 397.455731\n",
      "Average loss at step 513600: 400.988912\n",
      "Average loss at step 513700: 402.410739\n",
      "Average loss at step 513800: 403.693017\n",
      "Average loss at step 513900: 399.412333\n",
      "Average loss at step 514000: 401.941510\n",
      "Graph 514: 26 nodes\n",
      "Average loss at step 514100: 432.450014\n",
      "Average loss at step 514200: 429.873272\n",
      "Average loss at step 514300: 431.670445\n",
      "Average loss at step 514400: 428.325472\n",
      "Average loss at step 514500: 429.999555\n",
      "Average loss at step 514600: 429.092470\n",
      "Average loss at step 514700: 430.583202\n",
      "Average loss at step 514800: 431.308547\n",
      "Average loss at step 514900: 428.189005\n",
      "Average loss at step 515000: 431.061381\n",
      "Graph 515: 37 nodes\n",
      "Average loss at step 515100: 452.446354\n",
      "Average loss at step 515200: 451.520129\n",
      "Average loss at step 515300: 448.883950\n",
      "Average loss at step 515400: 448.881230\n",
      "Average loss at step 515500: 448.156522\n",
      "Average loss at step 515600: 449.400980\n",
      "Average loss at step 515700: 449.389942\n",
      "Average loss at step 515800: 447.393711\n",
      "Average loss at step 515900: 448.693815\n",
      "Average loss at step 516000: 448.977935\n",
      "Graph 516: 50 nodes\n",
      "Average loss at step 516100: 379.101148\n",
      "Average loss at step 516200: 376.861717\n",
      "Average loss at step 516300: 377.618703\n",
      "Average loss at step 516400: 375.062333\n",
      "Average loss at step 516500: 379.314044\n",
      "Average loss at step 516600: 376.414675\n",
      "Average loss at step 516700: 375.258592\n",
      "Average loss at step 516800: 373.898870\n",
      "Average loss at step 516900: 375.681086\n",
      "Average loss at step 517000: 375.585743\n",
      "Graph 517: 27 nodes\n",
      "Average loss at step 517100: 440.278681\n",
      "Average loss at step 517200: 438.568994\n",
      "Average loss at step 517300: 434.926383\n",
      "Average loss at step 517400: 438.500346\n",
      "Average loss at step 517500: 435.410070\n",
      "Average loss at step 517600: 435.661971\n",
      "Average loss at step 517700: 435.197986\n",
      "Average loss at step 517800: 435.804839\n",
      "Average loss at step 517900: 434.825436\n",
      "Average loss at step 518000: 434.778759\n",
      "Graph 518: 34 nodes\n",
      "Average loss at step 518100: 414.163581\n",
      "Average loss at step 518200: 411.360059\n",
      "Average loss at step 518300: 414.605906\n",
      "Average loss at step 518400: 412.830375\n",
      "Average loss at step 518500: 414.716824\n",
      "Average loss at step 518600: 413.719658\n",
      "Average loss at step 518700: 410.799829\n",
      "Average loss at step 518800: 414.509826\n",
      "Average loss at step 518900: 413.888116\n",
      "Average loss at step 519000: 413.478402\n",
      "Graph 519: 32 nodes\n",
      "Average loss at step 519100: 414.779461\n",
      "Average loss at step 519200: 415.910156\n",
      "Average loss at step 519300: 413.713825\n",
      "Average loss at step 519400: 415.978746\n",
      "Average loss at step 519500: 415.292717\n",
      "Average loss at step 519600: 414.658385\n",
      "Average loss at step 519700: 413.986697\n",
      "Average loss at step 519800: 412.553630\n",
      "Average loss at step 519900: 413.531709\n",
      "Average loss at step 520000: 409.305524\n",
      "Graph 520: 35 nodes\n",
      "Average loss at step 520100: 421.571856\n",
      "Average loss at step 520200: 417.731710\n",
      "Average loss at step 520300: 417.232666\n",
      "Average loss at step 520400: 417.412761\n",
      "Average loss at step 520500: 417.150297\n",
      "Average loss at step 520600: 418.306023\n",
      "Average loss at step 520700: 413.908519\n",
      "Average loss at step 520800: 417.963487\n",
      "Average loss at step 520900: 417.088728\n",
      "Average loss at step 521000: 418.289222\n",
      "Time: 55.8674910069\n",
      "Graph 521: 37 nodes\n",
      "Average loss at step 521100: 418.111286\n",
      "Average loss at step 521200: 418.882165\n",
      "Average loss at step 521300: 416.976364\n",
      "Average loss at step 521400: 412.851239\n",
      "Average loss at step 521500: 415.021966\n",
      "Average loss at step 521600: 415.380319\n",
      "Average loss at step 521700: 413.440090\n",
      "Average loss at step 521800: 415.981781\n",
      "Average loss at step 521900: 416.856392\n",
      "Average loss at step 522000: 416.306858\n",
      "Graph 522: 48 nodes\n",
      "Average loss at step 522100: 373.675385\n",
      "Average loss at step 522200: 370.735842\n",
      "Average loss at step 522300: 366.985847\n",
      "Average loss at step 522400: 369.476679\n",
      "Average loss at step 522500: 370.250729\n",
      "Average loss at step 522600: 368.088708\n",
      "Average loss at step 522700: 372.193969\n",
      "Average loss at step 522800: 369.335730\n",
      "Average loss at step 522900: 371.649502\n",
      "Average loss at step 523000: 372.184237\n",
      "Graph 523: 41 nodes\n",
      "Average loss at step 523100: 456.952525\n",
      "Average loss at step 523200: 452.419343\n",
      "Average loss at step 523300: 451.912777\n",
      "Average loss at step 523400: 452.448766\n",
      "Average loss at step 523500: 451.797046\n",
      "Average loss at step 523600: 453.559497\n",
      "Average loss at step 523700: 453.141540\n",
      "Average loss at step 523800: 453.195926\n",
      "Average loss at step 523900: 453.077359\n",
      "Average loss at step 524000: 452.626276\n",
      "Graph 524: 29 nodes\n",
      "Average loss at step 524100: 409.895271\n",
      "Average loss at step 524200: 406.759860\n",
      "Average loss at step 524300: 410.178626\n",
      "Average loss at step 524400: 409.679336\n",
      "Average loss at step 524500: 406.267715\n",
      "Average loss at step 524600: 412.526630\n",
      "Average loss at step 524700: 406.056291\n",
      "Average loss at step 524800: 401.517176\n",
      "Average loss at step 524900: 408.505374\n",
      "Average loss at step 525000: 407.813576\n",
      "Graph 525: 58 nodes\n",
      "Average loss at step 525100: 373.714963\n",
      "Average loss at step 525200: 371.129840\n",
      "Average loss at step 525300: 370.097614\n",
      "Average loss at step 525400: 373.304805\n",
      "Average loss at step 525500: 371.121921\n",
      "Average loss at step 525600: 367.968307\n",
      "Average loss at step 525700: 373.605239\n",
      "Average loss at step 525800: 370.035376\n",
      "Average loss at step 525900: 373.166261\n",
      "Average loss at step 526000: 367.832590\n",
      "Graph 526: 57 nodes\n",
      "Average loss at step 526100: 380.359156\n",
      "Average loss at step 526200: 380.354507\n",
      "Average loss at step 526300: 379.150030\n",
      "Average loss at step 526400: 376.477204\n",
      "Average loss at step 526500: 378.484386\n",
      "Average loss at step 526600: 376.229336\n",
      "Average loss at step 526700: 377.992767\n",
      "Average loss at step 526800: 378.959309\n",
      "Average loss at step 526900: 376.023840\n",
      "Average loss at step 527000: 377.754858\n",
      "Graph 527: 21 nodes\n",
      "Average loss at step 527100: 396.609166\n",
      "Average loss at step 527200: 395.808501\n",
      "Average loss at step 527300: 395.109206\n",
      "Average loss at step 527400: 397.699994\n",
      "Average loss at step 527500: 394.962315\n",
      "Average loss at step 527600: 397.461709\n",
      "Average loss at step 527700: 394.453041\n",
      "Average loss at step 527800: 394.453817\n",
      "Average loss at step 527900: 395.726220\n",
      "Average loss at step 528000: 399.861321\n",
      "Graph 528: 18 nodes\n",
      "Average loss at step 528100: 403.198477\n",
      "Average loss at step 528200: 401.938971\n",
      "Average loss at step 528300: 403.588949\n",
      "Average loss at step 528400: 399.877319\n",
      "Average loss at step 528500: 401.527804\n",
      "Average loss at step 528600: 397.855222\n",
      "Average loss at step 528700: 403.341173\n",
      "Average loss at step 528800: 401.878860\n",
      "Average loss at step 528900: 400.893105\n",
      "Average loss at step 529000: 402.265112\n",
      "Graph 529: 42 nodes\n",
      "Average loss at step 529100: 394.323576\n",
      "Average loss at step 529200: 392.104528\n",
      "Average loss at step 529300: 391.345668\n",
      "Average loss at step 529400: 391.430579\n",
      "Average loss at step 529500: 391.787951\n",
      "Average loss at step 529600: 391.103409\n",
      "Average loss at step 529700: 391.257641\n",
      "Average loss at step 529800: 389.739244\n",
      "Average loss at step 529900: 390.733105\n",
      "Average loss at step 530000: 389.165919\n",
      "Graph 530: 40 nodes\n",
      "Average loss at step 530100: 390.464557\n",
      "Average loss at step 530200: 383.506546\n",
      "Average loss at step 530300: 383.836932\n",
      "Average loss at step 530400: 382.474070\n",
      "Average loss at step 530500: 383.331701\n",
      "Average loss at step 530600: 382.732688\n",
      "Average loss at step 530700: 386.691371\n",
      "Average loss at step 530800: 383.936394\n",
      "Average loss at step 530900: 383.476192\n",
      "Average loss at step 531000: 385.245310\n",
      "Time: 52.5640420914\n",
      "Graph 531: 74 nodes\n",
      "Average loss at step 531100: 351.037198\n",
      "Average loss at step 531200: 345.605865\n",
      "Average loss at step 531300: 342.695485\n",
      "Average loss at step 531400: 341.828630\n",
      "Average loss at step 531500: 339.481428\n",
      "Average loss at step 531600: 342.756221\n",
      "Average loss at step 531700: 338.416313\n",
      "Average loss at step 531800: 337.456888\n",
      "Average loss at step 531900: 340.592651\n",
      "Average loss at step 532000: 345.051246\n",
      "Graph 532: 44 nodes\n",
      "Average loss at step 532100: 387.896397\n",
      "Average loss at step 532200: 385.606325\n",
      "Average loss at step 532300: 386.272080\n",
      "Average loss at step 532400: 390.825204\n",
      "Average loss at step 532500: 387.427037\n",
      "Average loss at step 532600: 382.484442\n",
      "Average loss at step 532700: 388.472049\n",
      "Average loss at step 532800: 387.079393\n",
      "Average loss at step 532900: 385.226676\n",
      "Average loss at step 533000: 386.709429\n",
      "Graph 533: 44 nodes\n",
      "Average loss at step 533100: 364.037426\n",
      "Average loss at step 533200: 356.526145\n",
      "Average loss at step 533300: 357.032207\n",
      "Average loss at step 533400: 357.827112\n",
      "Average loss at step 533500: 358.536766\n",
      "Average loss at step 533600: 357.546428\n",
      "Average loss at step 533700: 354.796027\n",
      "Average loss at step 533800: 360.997672\n",
      "Average loss at step 533900: 358.167914\n",
      "Average loss at step 534000: 360.561390\n",
      "Graph 534: 40 nodes\n",
      "Average loss at step 534100: 440.420673\n",
      "Average loss at step 534200: 438.321417\n",
      "Average loss at step 534300: 437.254408\n",
      "Average loss at step 534400: 437.401793\n",
      "Average loss at step 534500: 438.225827\n",
      "Average loss at step 534600: 437.183954\n",
      "Average loss at step 534700: 434.988641\n",
      "Average loss at step 534800: 437.394067\n",
      "Average loss at step 534900: 434.506625\n",
      "Average loss at step 535000: 435.037183\n",
      "Graph 535: 44 nodes\n",
      "Average loss at step 535100: 385.718739\n",
      "Average loss at step 535200: 381.691820\n",
      "Average loss at step 535300: 384.487378\n",
      "Average loss at step 535400: 380.874353\n",
      "Average loss at step 535500: 378.695990\n",
      "Average loss at step 535600: 382.441909\n",
      "Average loss at step 535700: 378.286443\n",
      "Average loss at step 535800: 380.364033\n",
      "Average loss at step 535900: 382.153158\n",
      "Average loss at step 536000: 381.591054\n",
      "Graph 536: 42 nodes\n",
      "Average loss at step 536100: 393.067251\n",
      "Average loss at step 536200: 391.209950\n",
      "Average loss at step 536300: 393.225106\n",
      "Average loss at step 536400: 393.777098\n",
      "Average loss at step 536500: 390.685332\n",
      "Average loss at step 536600: 391.729102\n",
      "Average loss at step 536700: 390.607259\n",
      "Average loss at step 536800: 392.654978\n",
      "Average loss at step 536900: 390.692284\n",
      "Average loss at step 537000: 393.957917\n",
      "Graph 537: 43 nodes\n",
      "Average loss at step 537100: 383.377933\n",
      "Average loss at step 537200: 385.779893\n",
      "Average loss at step 537300: 384.397892\n",
      "Average loss at step 537400: 382.592130\n",
      "Average loss at step 537500: 384.588524\n",
      "Average loss at step 537600: 385.111499\n",
      "Average loss at step 537700: 385.040251\n",
      "Average loss at step 537800: 387.630545\n",
      "Average loss at step 537900: 382.013833\n",
      "Average loss at step 538000: 380.212636\n",
      "Graph 538: 41 nodes\n",
      "Average loss at step 538100: 414.932731\n",
      "Average loss at step 538200: 414.673332\n",
      "Average loss at step 538300: 416.936720\n",
      "Average loss at step 538400: 421.366723\n",
      "Average loss at step 538500: 418.085534\n",
      "Average loss at step 538600: 418.788407\n",
      "Average loss at step 538700: 418.052070\n",
      "Average loss at step 538800: 415.424586\n",
      "Average loss at step 538900: 419.266843\n",
      "Average loss at step 539000: 419.006388\n",
      "Graph 539: 49 nodes\n",
      "Average loss at step 539100: 386.388353\n",
      "Average loss at step 539200: 388.599234\n",
      "Average loss at step 539300: 385.320024\n",
      "Average loss at step 539400: 385.180153\n",
      "Average loss at step 539500: 386.633161\n",
      "Average loss at step 539600: 386.279280\n",
      "Average loss at step 539700: 386.273822\n",
      "Average loss at step 539800: 383.243754\n",
      "Average loss at step 539900: 387.717201\n",
      "Average loss at step 540000: 382.897210\n",
      "Graph 540: 56 nodes\n",
      "Average loss at step 540100: 385.265038\n",
      "Average loss at step 540200: 379.646352\n",
      "Average loss at step 540300: 375.797284\n",
      "Average loss at step 540400: 374.052872\n",
      "Average loss at step 540500: 376.237116\n",
      "Average loss at step 540600: 374.354645\n",
      "Average loss at step 540700: 373.201333\n",
      "Average loss at step 540800: 372.429666\n",
      "Average loss at step 540900: 373.867335\n",
      "Average loss at step 541000: 375.543889\n",
      "Time: 49.2348790169\n",
      "Graph 541: 44 nodes\n",
      "Average loss at step 541100: 385.119202\n",
      "Average loss at step 541200: 378.256798\n",
      "Average loss at step 541300: 377.105086\n",
      "Average loss at step 541400: 381.669127\n",
      "Average loss at step 541500: 378.133127\n",
      "Average loss at step 541600: 381.346857\n",
      "Average loss at step 541700: 381.675272\n",
      "Average loss at step 541800: 380.329310\n",
      "Average loss at step 541900: 380.516960\n",
      "Average loss at step 542000: 379.675720\n",
      "Graph 542: 42 nodes\n",
      "Average loss at step 542100: 455.891187\n",
      "Average loss at step 542200: 455.751689\n",
      "Average loss at step 542300: 453.381202\n",
      "Average loss at step 542400: 453.203469\n",
      "Average loss at step 542500: 453.923907\n",
      "Average loss at step 542600: 453.243166\n",
      "Average loss at step 542700: 453.952590\n",
      "Average loss at step 542800: 451.645351\n",
      "Average loss at step 542900: 454.658418\n",
      "Average loss at step 543000: 453.053963\n",
      "Graph 543: 41 nodes\n",
      "Average loss at step 543100: 456.587222\n",
      "Average loss at step 543200: 453.687735\n",
      "Average loss at step 543300: 453.423727\n",
      "Average loss at step 543400: 455.019713\n",
      "Average loss at step 543500: 453.021084\n",
      "Average loss at step 543600: 453.717924\n",
      "Average loss at step 543700: 454.622572\n",
      "Average loss at step 543800: 452.195932\n",
      "Average loss at step 543900: 454.082788\n",
      "Average loss at step 544000: 452.822791\n",
      "Graph 544: 41 nodes\n",
      "Average loss at step 544100: 455.009417\n",
      "Average loss at step 544200: 453.148250\n",
      "Average loss at step 544300: 453.509080\n",
      "Average loss at step 544400: 453.095489\n",
      "Average loss at step 544500: 453.794129\n",
      "Average loss at step 544600: 454.467598\n",
      "Average loss at step 544700: 453.702155\n",
      "Average loss at step 544800: 454.476319\n",
      "Average loss at step 544900: 453.845006\n",
      "Average loss at step 545000: 453.023913\n",
      "Graph 545: 44 nodes\n",
      "Average loss at step 545100: 452.785122\n",
      "Average loss at step 545200: 452.979967\n",
      "Average loss at step 545300: 452.401890\n",
      "Average loss at step 545400: 453.497056\n",
      "Average loss at step 545500: 452.991713\n",
      "Average loss at step 545600: 452.631411\n",
      "Average loss at step 545700: 449.964276\n",
      "Average loss at step 545800: 452.353512\n",
      "Average loss at step 545900: 454.544758\n",
      "Average loss at step 546000: 450.499905\n",
      "Graph 546: 39 nodes\n",
      "Average loss at step 546100: 367.969464\n",
      "Average loss at step 546200: 362.800645\n",
      "Average loss at step 546300: 363.499427\n",
      "Average loss at step 546400: 367.018317\n",
      "Average loss at step 546500: 361.476431\n",
      "Average loss at step 546600: 358.066798\n",
      "Average loss at step 546700: 361.577227\n",
      "Average loss at step 546800: 366.533073\n",
      "Average loss at step 546900: 362.532063\n",
      "Average loss at step 547000: 368.482452\n",
      "Graph 547: 25 nodes\n",
      "Average loss at step 547100: 458.841673\n",
      "Average loss at step 547200: 457.357340\n",
      "Average loss at step 547300: 455.488622\n",
      "Average loss at step 547400: 456.773788\n",
      "Average loss at step 547500: 453.909561\n",
      "Average loss at step 547600: 457.032811\n",
      "Average loss at step 547700: 455.252879\n",
      "Average loss at step 547800: 454.329138\n",
      "Average loss at step 547900: 455.497536\n",
      "Average loss at step 548000: 455.295350\n",
      "Graph 548: 25 nodes\n",
      "Average loss at step 548100: 461.083347\n",
      "Average loss at step 548200: 458.325711\n",
      "Average loss at step 548300: 456.992352\n",
      "Average loss at step 548400: 457.836403\n",
      "Average loss at step 548500: 457.456280\n",
      "Average loss at step 548600: 455.403496\n",
      "Average loss at step 548700: 457.978257\n",
      "Average loss at step 548800: 456.979836\n",
      "Average loss at step 548900: 458.053058\n",
      "Average loss at step 549000: 459.515949\n",
      "Graph 549: 26 nodes\n",
      "Average loss at step 549100: 459.135635\n",
      "Average loss at step 549200: 455.510443\n",
      "Average loss at step 549300: 455.298388\n",
      "Average loss at step 549400: 454.744498\n",
      "Average loss at step 549500: 455.080962\n",
      "Average loss at step 549600: 455.371430\n",
      "Average loss at step 549700: 454.677444\n",
      "Average loss at step 549800: 455.083235\n",
      "Average loss at step 549900: 455.665062\n",
      "Average loss at step 550000: 455.326702\n",
      "Graph 550: 52 nodes\n",
      "Average loss at step 550100: 393.909531\n",
      "Average loss at step 550200: 391.009899\n",
      "Average loss at step 550300: 396.152081\n",
      "Average loss at step 550400: 390.864170\n",
      "Average loss at step 550500: 389.416260\n",
      "Average loss at step 550600: 388.801535\n",
      "Average loss at step 550700: 388.759862\n",
      "Average loss at step 550800: 387.082789\n",
      "Average loss at step 550900: 387.977303\n",
      "Average loss at step 551000: 387.850604\n",
      "Time: 52.7916910648\n",
      "Graph 551: 41 nodes\n",
      "Average loss at step 551100: 429.752679\n",
      "Average loss at step 551200: 432.300020\n",
      "Average loss at step 551300: 429.511196\n",
      "Average loss at step 551400: 425.995079\n",
      "Average loss at step 551500: 428.655122\n",
      "Average loss at step 551600: 427.443096\n",
      "Average loss at step 551700: 426.677379\n",
      "Average loss at step 551800: 424.347877\n",
      "Average loss at step 551900: 427.460092\n",
      "Average loss at step 552000: 425.442476\n",
      "Graph 552: 45 nodes\n",
      "Average loss at step 552100: 454.868592\n",
      "Average loss at step 552200: 453.470752\n",
      "Average loss at step 552300: 452.758624\n",
      "Average loss at step 552400: 453.507585\n",
      "Average loss at step 552500: 451.375287\n",
      "Average loss at step 552600: 452.212001\n",
      "Average loss at step 552700: 451.898425\n",
      "Average loss at step 552800: 451.727703\n",
      "Average loss at step 552900: 452.949606\n",
      "Average loss at step 553000: 449.694169\n",
      "Graph 553: 31 nodes\n",
      "Average loss at step 553100: 403.481519\n",
      "Average loss at step 553200: 400.473737\n",
      "Average loss at step 553300: 402.287361\n",
      "Average loss at step 553400: 403.549942\n",
      "Average loss at step 553500: 401.865544\n",
      "Average loss at step 553600: 401.628360\n",
      "Average loss at step 553700: 403.475402\n",
      "Average loss at step 553800: 404.004564\n",
      "Average loss at step 553900: 402.307467\n",
      "Average loss at step 554000: 401.511692\n",
      "Graph 554: 26 nodes\n",
      "Average loss at step 554100: 456.087139\n",
      "Average loss at step 554200: 455.727352\n",
      "Average loss at step 554300: 454.662110\n",
      "Average loss at step 554400: 456.419817\n",
      "Average loss at step 554500: 454.805065\n",
      "Average loss at step 554600: 455.928349\n",
      "Average loss at step 554700: 454.617382\n",
      "Average loss at step 554800: 454.710844\n",
      "Average loss at step 554900: 454.956432\n",
      "Average loss at step 555000: 455.530112\n",
      "Graph 555: 26 nodes\n",
      "Average loss at step 555100: 459.642940\n",
      "Average loss at step 555200: 459.306952\n",
      "Average loss at step 555300: 458.382865\n",
      "Average loss at step 555400: 458.187601\n",
      "Average loss at step 555500: 456.401142\n",
      "Average loss at step 555600: 457.785317\n",
      "Average loss at step 555700: 457.458095\n",
      "Average loss at step 555800: 457.361842\n",
      "Average loss at step 555900: 456.976889\n",
      "Average loss at step 556000: 457.133744\n",
      "Graph 556: 52 nodes\n",
      "Average loss at step 556100: 394.555451\n",
      "Average loss at step 556200: 393.160285\n",
      "Average loss at step 556300: 391.744898\n",
      "Average loss at step 556400: 397.024484\n",
      "Average loss at step 556500: 390.156848\n",
      "Average loss at step 556600: 397.026764\n",
      "Average loss at step 556700: 395.688954\n",
      "Average loss at step 556800: 394.658421\n",
      "Average loss at step 556900: 393.524571\n",
      "Average loss at step 557000: 394.150481\n",
      "Graph 557: 50 nodes\n",
      "Average loss at step 557100: 388.000281\n",
      "Average loss at step 557200: 384.837239\n",
      "Average loss at step 557300: 383.191281\n",
      "Average loss at step 557400: 380.820381\n",
      "Average loss at step 557500: 380.832388\n",
      "Average loss at step 557600: 384.150727\n",
      "Average loss at step 557700: 382.520821\n",
      "Average loss at step 557800: 383.753150\n",
      "Average loss at step 557900: 379.459860\n",
      "Average loss at step 558000: 383.341778\n",
      "Graph 558: 38 nodes\n",
      "Average loss at step 558100: 403.655976\n",
      "Average loss at step 558200: 401.400233\n",
      "Average loss at step 558300: 400.494215\n",
      "Average loss at step 558400: 402.229609\n",
      "Average loss at step 558500: 398.908975\n",
      "Average loss at step 558600: 402.536167\n",
      "Average loss at step 558700: 397.871803\n",
      "Average loss at step 558800: 399.131705\n",
      "Average loss at step 558900: 396.763008\n",
      "Average loss at step 559000: 395.311749\n",
      "Graph 559: 37 nodes\n",
      "Average loss at step 559100: 398.001829\n",
      "Average loss at step 559200: 393.248658\n",
      "Average loss at step 559300: 392.937301\n",
      "Average loss at step 559400: 395.622545\n",
      "Average loss at step 559500: 394.542968\n",
      "Average loss at step 559600: 396.974429\n",
      "Average loss at step 559700: 395.402951\n",
      "Average loss at step 559800: 394.411749\n",
      "Average loss at step 559900: 395.599599\n",
      "Average loss at step 560000: 394.767790\n",
      "Graph 560: 27 nodes\n",
      "Average loss at step 560100: 457.783710\n",
      "Average loss at step 560200: 454.429557\n",
      "Average loss at step 560300: 451.107755\n",
      "Average loss at step 560400: 451.771783\n",
      "Average loss at step 560500: 452.346776\n",
      "Average loss at step 560600: 453.058892\n",
      "Average loss at step 560700: 450.161087\n",
      "Average loss at step 560800: 449.822320\n",
      "Average loss at step 560900: 451.094585\n",
      "Average loss at step 561000: 453.374050\n",
      "Time: 52.0707099438\n",
      "Graph 561: 29 nodes\n",
      "Average loss at step 561100: 455.150253\n",
      "Average loss at step 561200: 455.216696\n",
      "Average loss at step 561300: 454.414987\n",
      "Average loss at step 561400: 451.709371\n",
      "Average loss at step 561500: 451.617441\n",
      "Average loss at step 561600: 451.436418\n",
      "Average loss at step 561700: 452.662044\n",
      "Average loss at step 561800: 452.747154\n",
      "Average loss at step 561900: 454.341150\n",
      "Average loss at step 562000: 452.406583\n",
      "Graph 562: 48 nodes\n",
      "Average loss at step 562100: 358.090797\n",
      "Average loss at step 562200: 353.271759\n",
      "Average loss at step 562300: 355.227554\n",
      "Average loss at step 562400: 353.199620\n",
      "Average loss at step 562500: 353.021262\n",
      "Average loss at step 562600: 352.322494\n",
      "Average loss at step 562700: 353.285204\n",
      "Average loss at step 562800: 351.671761\n",
      "Average loss at step 562900: 355.294215\n",
      "Average loss at step 563000: 354.989208\n",
      "Graph 563: 40 nodes\n",
      "Average loss at step 563100: 397.181310\n",
      "Average loss at step 563200: 393.765263\n",
      "Average loss at step 563300: 397.140268\n",
      "Average loss at step 563400: 395.868667\n",
      "Average loss at step 563500: 395.211155\n",
      "Average loss at step 563600: 395.200282\n",
      "Average loss at step 563700: 395.836370\n",
      "Average loss at step 563800: 393.134760\n",
      "Average loss at step 563900: 395.911610\n",
      "Average loss at step 564000: 394.560407\n",
      "Graph 564: 19 nodes\n",
      "Average loss at step 564100: 459.966675\n",
      "Average loss at step 564200: 457.049388\n",
      "Average loss at step 564300: 455.963818\n",
      "Average loss at step 564400: 456.991928\n",
      "Average loss at step 564500: 455.061189\n",
      "Average loss at step 564600: 455.965740\n",
      "Average loss at step 564700: 455.596021\n",
      "Average loss at step 564800: 453.968944\n",
      "Average loss at step 564900: 455.149771\n",
      "Average loss at step 565000: 455.919799\n",
      "Graph 565: 14 nodes\n",
      "Average loss at step 565100: 412.771004\n",
      "Average loss at step 565200: 410.454224\n",
      "Average loss at step 565300: 407.567682\n",
      "Average loss at step 565400: 409.260884\n",
      "Average loss at step 565500: 411.134306\n",
      "Average loss at step 565600: 408.995082\n",
      "Average loss at step 565700: 408.798394\n",
      "Average loss at step 565800: 409.018257\n",
      "Average loss at step 565900: 410.328608\n",
      "Average loss at step 566000: 411.104535\n",
      "Graph 566: 13 nodes\n",
      "Average loss at step 566100: 409.870097\n",
      "Average loss at step 566200: 411.235978\n",
      "Average loss at step 566300: 409.434069\n",
      "Average loss at step 566400: 412.386132\n",
      "Average loss at step 566500: 409.120447\n",
      "Average loss at step 566600: 409.248036\n",
      "Average loss at step 566700: 409.438500\n",
      "Average loss at step 566800: 409.646341\n",
      "Average loss at step 566900: 411.566123\n",
      "Average loss at step 567000: 407.654432\n",
      "Graph 567: 13 nodes\n",
      "Average loss at step 567100: 400.278172\n",
      "Average loss at step 567200: 400.464746\n",
      "Average loss at step 567300: 396.936302\n",
      "Average loss at step 567400: 399.045372\n",
      "Average loss at step 567500: 398.658037\n",
      "Average loss at step 567600: 402.290098\n",
      "Average loss at step 567700: 395.507404\n",
      "Average loss at step 567800: 398.713340\n",
      "Average loss at step 567900: 396.028435\n",
      "Average loss at step 568000: 397.868379\n",
      "Graph 568: 44 nodes\n",
      "Average loss at step 568100: 382.112162\n",
      "Average loss at step 568200: 379.604989\n",
      "Average loss at step 568300: 378.974840\n",
      "Average loss at step 568400: 376.083775\n",
      "Average loss at step 568500: 377.406916\n",
      "Average loss at step 568600: 379.440102\n",
      "Average loss at step 568700: 380.490155\n",
      "Average loss at step 568800: 382.212281\n",
      "Average loss at step 568900: 377.935463\n",
      "Average loss at step 569000: 377.616237\n",
      "Graph 569: 45 nodes\n",
      "Average loss at step 569100: 410.292678\n",
      "Average loss at step 569200: 409.323252\n",
      "Average loss at step 569300: 411.785581\n",
      "Average loss at step 569400: 404.097887\n",
      "Average loss at step 569500: 406.297262\n",
      "Average loss at step 569600: 408.507859\n",
      "Average loss at step 569700: 405.185023\n",
      "Average loss at step 569800: 408.087679\n",
      "Average loss at step 569900: 408.503273\n",
      "Average loss at step 570000: 407.885975\n",
      "Graph 570: 46 nodes\n",
      "Average loss at step 570100: 418.561641\n",
      "Average loss at step 570200: 417.748783\n",
      "Average loss at step 570300: 414.025609\n",
      "Average loss at step 570400: 414.476902\n",
      "Average loss at step 570500: 415.850488\n",
      "Average loss at step 570600: 414.976920\n",
      "Average loss at step 570700: 413.664673\n",
      "Average loss at step 570800: 414.430861\n",
      "Average loss at step 570900: 414.300914\n",
      "Average loss at step 571000: 415.741600\n",
      "Time: 60.4366111755\n",
      "Graph 571: 47 nodes\n",
      "Average loss at step 571100: 404.140942\n",
      "Average loss at step 571200: 403.002903\n",
      "Average loss at step 571300: 402.456001\n",
      "Average loss at step 571400: 404.053174\n",
      "Average loss at step 571500: 403.931551\n",
      "Average loss at step 571600: 405.152143\n",
      "Average loss at step 571700: 405.315899\n",
      "Average loss at step 571800: 403.809402\n",
      "Average loss at step 571900: 403.524062\n",
      "Average loss at step 572000: 402.495871\n",
      "Graph 572: 47 nodes\n",
      "Average loss at step 572100: 403.791835\n",
      "Average loss at step 572200: 403.826729\n",
      "Average loss at step 572300: 403.107772\n",
      "Average loss at step 572400: 402.372094\n",
      "Average loss at step 572500: 404.782449\n",
      "Average loss at step 572600: 406.946395\n",
      "Average loss at step 572700: 404.147907\n",
      "Average loss at step 572800: 404.021500\n",
      "Average loss at step 572900: 400.751003\n",
      "Average loss at step 573000: 400.689492\n",
      "Graph 573: 42 nodes\n",
      "Average loss at step 573100: 385.418200\n",
      "Average loss at step 573200: 381.388152\n",
      "Average loss at step 573300: 382.707179\n",
      "Average loss at step 573400: 383.163760\n",
      "Average loss at step 573500: 383.274684\n",
      "Average loss at step 573600: 381.922538\n",
      "Average loss at step 573700: 384.334494\n",
      "Average loss at step 573800: 379.927302\n",
      "Average loss at step 573900: 381.620230\n",
      "Average loss at step 574000: 381.451676\n",
      "Graph 574: 51 nodes\n",
      "Average loss at step 574100: 372.235071\n",
      "Average loss at step 574200: 370.604858\n",
      "Average loss at step 574300: 367.581063\n",
      "Average loss at step 574400: 365.825422\n",
      "Average loss at step 574500: 366.048425\n",
      "Average loss at step 574600: 371.280555\n",
      "Average loss at step 574700: 366.666626\n",
      "Average loss at step 574800: 371.814498\n",
      "Average loss at step 574900: 369.078444\n",
      "Average loss at step 575000: 368.151116\n",
      "Graph 575: 27 nodes\n",
      "Average loss at step 575100: 443.978951\n",
      "Average loss at step 575200: 442.457981\n",
      "Average loss at step 575300: 443.334000\n",
      "Average loss at step 575400: 442.430301\n",
      "Average loss at step 575500: 442.687863\n",
      "Average loss at step 575600: 442.818186\n",
      "Average loss at step 575700: 441.702292\n",
      "Average loss at step 575800: 439.509967\n",
      "Average loss at step 575900: 440.660836\n",
      "Average loss at step 576000: 440.672371\n",
      "Graph 576: 27 nodes\n",
      "Average loss at step 576100: 437.158731\n",
      "Average loss at step 576200: 436.557080\n",
      "Average loss at step 576300: 437.980193\n",
      "Average loss at step 576400: 434.905518\n",
      "Average loss at step 576500: 436.013982\n",
      "Average loss at step 576600: 436.038856\n",
      "Average loss at step 576700: 436.977344\n",
      "Average loss at step 576800: 433.890958\n",
      "Average loss at step 576900: 434.284446\n",
      "Average loss at step 577000: 434.439974\n",
      "Graph 577: 60 nodes\n",
      "Average loss at step 577100: 380.032543\n",
      "Average loss at step 577200: 376.115304\n",
      "Average loss at step 577300: 372.671744\n",
      "Average loss at step 577400: 373.139273\n",
      "Average loss at step 577500: 375.565900\n",
      "Average loss at step 577600: 375.518883\n",
      "Average loss at step 577700: 375.703835\n",
      "Average loss at step 577800: 373.910373\n",
      "Average loss at step 577900: 374.436501\n",
      "Average loss at step 578000: 371.816662\n",
      "Graph 578: 28 nodes\n",
      "Average loss at step 578100: 440.667715\n",
      "Average loss at step 578200: 438.904459\n",
      "Average loss at step 578300: 441.160314\n",
      "Average loss at step 578400: 438.626377\n",
      "Average loss at step 578500: 438.407584\n",
      "Average loss at step 578600: 440.440833\n",
      "Average loss at step 578700: 437.418519\n",
      "Average loss at step 578800: 436.841325\n",
      "Average loss at step 578900: 437.766774\n",
      "Average loss at step 579000: 438.648273\n",
      "Graph 579: 38 nodes\n",
      "Average loss at step 579100: 437.146462\n",
      "Average loss at step 579200: 436.032042\n",
      "Average loss at step 579300: 433.592593\n",
      "Average loss at step 579400: 433.100495\n",
      "Average loss at step 579500: 434.562555\n",
      "Average loss at step 579600: 435.658721\n",
      "Average loss at step 579700: 435.260445\n",
      "Average loss at step 579800: 435.922805\n",
      "Average loss at step 579900: 433.714593\n",
      "Average loss at step 580000: 435.320036\n",
      "Graph 580: 23 nodes\n",
      "Average loss at step 580100: 419.042395\n",
      "Average loss at step 580200: 415.365321\n",
      "Average loss at step 580300: 415.135669\n",
      "Average loss at step 580400: 416.743379\n",
      "Average loss at step 580500: 413.768369\n",
      "Average loss at step 580600: 413.865443\n",
      "Average loss at step 580700: 414.329370\n",
      "Average loss at step 580800: 415.000572\n",
      "Average loss at step 580900: 414.455086\n",
      "Average loss at step 581000: 418.033887\n",
      "Time: 64.4378099442\n",
      "Graph 581: 18 nodes\n",
      "Average loss at step 581100: 426.508304\n",
      "Average loss at step 581200: 423.959944\n",
      "Average loss at step 581300: 423.876346\n",
      "Average loss at step 581400: 425.284510\n",
      "Average loss at step 581500: 425.464758\n",
      "Average loss at step 581600: 425.474819\n",
      "Average loss at step 581700: 420.371392\n",
      "Average loss at step 581800: 423.254699\n",
      "Average loss at step 581900: 421.710370\n",
      "Average loss at step 582000: 423.881758\n",
      "Graph 582: 45 nodes\n",
      "Average loss at step 582100: 392.171959\n",
      "Average loss at step 582200: 389.862447\n",
      "Average loss at step 582300: 389.018857\n",
      "Average loss at step 582400: 387.557087\n",
      "Average loss at step 582500: 385.944730\n",
      "Average loss at step 582600: 384.238446\n",
      "Average loss at step 582700: 389.202930\n",
      "Average loss at step 582800: 388.819408\n",
      "Average loss at step 582900: 387.018992\n",
      "Average loss at step 583000: 385.865446\n",
      "Graph 583: 21 nodes\n",
      "Average loss at step 583100: 435.632653\n",
      "Average loss at step 583200: 434.844633\n",
      "Average loss at step 583300: 434.622704\n",
      "Average loss at step 583400: 434.898899\n",
      "Average loss at step 583500: 436.685898\n",
      "Average loss at step 583600: 435.183735\n",
      "Average loss at step 583700: 433.479659\n",
      "Average loss at step 583800: 435.786109\n",
      "Average loss at step 583900: 436.553793\n",
      "Average loss at step 584000: 434.601522\n",
      "Graph 584: 35 nodes\n",
      "Average loss at step 584100: 453.067012\n",
      "Average loss at step 584200: 449.928017\n",
      "Average loss at step 584300: 449.692739\n",
      "Average loss at step 584400: 451.511106\n",
      "Average loss at step 584500: 448.650084\n",
      "Average loss at step 584600: 449.943483\n",
      "Average loss at step 584700: 448.985006\n",
      "Average loss at step 584800: 450.531826\n",
      "Average loss at step 584900: 448.155846\n",
      "Average loss at step 585000: 448.231547\n",
      "Graph 585: 24 nodes\n",
      "Average loss at step 585100: 438.582745\n",
      "Average loss at step 585200: 436.700403\n",
      "Average loss at step 585300: 436.267669\n",
      "Average loss at step 585400: 434.792861\n",
      "Average loss at step 585500: 436.926749\n",
      "Average loss at step 585600: 432.349328\n",
      "Average loss at step 585700: 436.423650\n",
      "Average loss at step 585800: 435.596370\n",
      "Average loss at step 585900: 436.589394\n",
      "Average loss at step 586000: 439.305706\n",
      "Graph 586: 21 nodes\n",
      "Average loss at step 586100: 450.110239\n",
      "Average loss at step 586200: 447.474981\n",
      "Average loss at step 586300: 448.895278\n",
      "Average loss at step 586400: 446.179294\n",
      "Average loss at step 586500: 448.299940\n",
      "Average loss at step 586600: 443.786693\n",
      "Average loss at step 586700: 447.515462\n",
      "Average loss at step 586800: 446.395380\n",
      "Average loss at step 586900: 447.070018\n",
      "Average loss at step 587000: 446.099189\n",
      "Graph 587: 21 nodes\n",
      "Average loss at step 587100: 435.892929\n",
      "Average loss at step 587200: 434.906486\n",
      "Average loss at step 587300: 436.681697\n",
      "Average loss at step 587400: 435.068723\n",
      "Average loss at step 587500: 431.628991\n",
      "Average loss at step 587600: 429.998248\n",
      "Average loss at step 587700: 431.678821\n",
      "Average loss at step 587800: 431.772474\n",
      "Average loss at step 587900: 434.827821\n",
      "Average loss at step 588000: 433.530424\n",
      "Graph 588: 31 nodes\n",
      "Average loss at step 588100: 434.782134\n",
      "Average loss at step 588200: 433.159486\n",
      "Average loss at step 588300: 433.504238\n",
      "Average loss at step 588400: 431.741028\n",
      "Average loss at step 588500: 431.837571\n",
      "Average loss at step 588600: 430.178414\n",
      "Average loss at step 588700: 432.954124\n",
      "Average loss at step 588800: 430.622093\n",
      "Average loss at step 588900: 431.783983\n",
      "Average loss at step 589000: 430.510005\n",
      "Graph 589: 27 nodes\n",
      "Average loss at step 589100: 431.454043\n",
      "Average loss at step 589200: 429.879065\n",
      "Average loss at step 589300: 428.665164\n",
      "Average loss at step 589400: 430.945389\n",
      "Average loss at step 589500: 426.511466\n",
      "Average loss at step 589600: 427.767251\n",
      "Average loss at step 589700: 431.003448\n",
      "Average loss at step 589800: 426.469468\n",
      "Average loss at step 589900: 426.400710\n",
      "Average loss at step 590000: 427.420475\n",
      "Graph 590: 51 nodes\n",
      "Average loss at step 590100: 397.886249\n",
      "Average loss at step 590200: 397.242508\n",
      "Average loss at step 590300: 397.420927\n",
      "Average loss at step 590400: 394.958517\n",
      "Average loss at step 590500: 394.655374\n",
      "Average loss at step 590600: 394.440454\n",
      "Average loss at step 590700: 396.096779\n",
      "Average loss at step 590800: 390.991712\n",
      "Average loss at step 590900: 394.094775\n",
      "Average loss at step 591000: 389.186185\n",
      "Time: 88.7429041862\n",
      "Graph 591: 36 nodes\n",
      "Average loss at step 591100: 415.477334\n",
      "Average loss at step 591200: 415.402463\n",
      "Average loss at step 591300: 412.120851\n",
      "Average loss at step 591400: 414.068835\n",
      "Average loss at step 591500: 414.121241\n",
      "Average loss at step 591600: 414.459534\n",
      "Average loss at step 591700: 418.005430\n",
      "Average loss at step 591800: 414.949755\n",
      "Average loss at step 591900: 414.540214\n",
      "Average loss at step 592000: 414.560964\n",
      "Graph 592: 46 nodes\n",
      "Average loss at step 592100: 363.644857\n",
      "Average loss at step 592200: 357.885264\n",
      "Average loss at step 592300: 357.464114\n",
      "Average loss at step 592400: 355.072613\n",
      "Average loss at step 592500: 354.741042\n",
      "Average loss at step 592600: 355.319476\n",
      "Average loss at step 592700: 355.355375\n",
      "Average loss at step 592800: 353.278044\n",
      "Average loss at step 592900: 351.859015\n",
      "Average loss at step 593000: 354.725231\n",
      "Graph 593: 52 nodes\n",
      "Average loss at step 593100: 378.808918\n",
      "Average loss at step 593200: 379.420721\n",
      "Average loss at step 593300: 378.491299\n",
      "Average loss at step 593400: 373.132663\n",
      "Average loss at step 593500: 376.966349\n",
      "Average loss at step 593600: 373.738039\n",
      "Average loss at step 593700: 374.165435\n",
      "Average loss at step 593800: 376.356583\n",
      "Average loss at step 593900: 376.537389\n",
      "Average loss at step 594000: 375.548270\n",
      "Graph 594: 44 nodes\n",
      "Average loss at step 594100: 382.541274\n",
      "Average loss at step 594200: 378.561079\n",
      "Average loss at step 594300: 377.972810\n",
      "Average loss at step 594400: 379.004300\n",
      "Average loss at step 594500: 381.099736\n",
      "Average loss at step 594600: 378.313745\n",
      "Average loss at step 594700: 379.944907\n",
      "Average loss at step 594800: 378.725348\n",
      "Average loss at step 594900: 379.455331\n",
      "Average loss at step 595000: 379.359822\n",
      "Graph 595: 45 nodes\n",
      "Average loss at step 595100: 379.483884\n",
      "Average loss at step 595200: 378.315713\n",
      "Average loss at step 595300: 382.113516\n",
      "Average loss at step 595400: 379.320343\n",
      "Average loss at step 595500: 379.521066\n",
      "Average loss at step 595600: 379.600576\n",
      "Average loss at step 595700: 380.065521\n",
      "Average loss at step 595800: 379.339370\n",
      "Average loss at step 595900: 374.951921\n",
      "Average loss at step 596000: 380.236821\n",
      "Graph 596: 52 nodes\n",
      "Average loss at step 596100: 380.732011\n",
      "Average loss at step 596200: 374.070512\n",
      "Average loss at step 596300: 378.547441\n",
      "Average loss at step 596400: 377.266988\n",
      "Average loss at step 596500: 374.908786\n",
      "Average loss at step 596600: 375.068725\n",
      "Average loss at step 596700: 371.329838\n",
      "Average loss at step 596800: 376.055732\n",
      "Average loss at step 596900: 375.628891\n",
      "Average loss at step 597000: 377.987284\n",
      "Graph 597: 55 nodes\n",
      "Average loss at step 597100: 365.877715\n",
      "Average loss at step 597200: 364.241217\n",
      "Average loss at step 597300: 366.525320\n",
      "Average loss at step 597400: 368.872506\n",
      "Average loss at step 597500: 365.300856\n",
      "Average loss at step 597600: 367.161850\n",
      "Average loss at step 597700: 366.371734\n",
      "Average loss at step 597800: 366.239306\n",
      "Average loss at step 597900: 363.331142\n",
      "Average loss at step 598000: 364.755816\n",
      "Graph 598: 51 nodes\n",
      "Average loss at step 598100: 339.454409\n",
      "Average loss at step 598200: 337.066156\n",
      "Average loss at step 598300: 337.242224\n",
      "Average loss at step 598400: 335.253694\n",
      "Average loss at step 598500: 332.763232\n",
      "Average loss at step 598600: 333.879418\n",
      "Average loss at step 598700: 332.903114\n",
      "Average loss at step 598800: 331.449948\n",
      "Average loss at step 598900: 334.059899\n",
      "Average loss at step 599000: 334.040532\n",
      "Graph 599: 47 nodes\n",
      "Average loss at step 599100: 340.556281\n",
      "Average loss at step 599200: 338.425927\n",
      "Average loss at step 599300: 334.354511\n",
      "Average loss at step 599400: 337.080153\n",
      "Average loss at step 599500: 338.981743\n",
      "Average loss at step 599600: 332.461502\n",
      "Average loss at step 599700: 338.286693\n",
      "Average loss at step 599800: 337.368866\n",
      "Average loss at step 599900: 334.130431\n",
      "Average loss at step 600000: 331.085568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AnonymousWalkEmbeddings.AWE at 0x1a15a5f710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ''\n",
    "\n",
    "batch_size = 200\n",
    "window_size = 2\n",
    "num_samples = 256\n",
    "\n",
    "root = '../DD/'\n",
    "ext = 'graphml'\n",
    "epochs = 3\n",
    "batches_per_epoch = 200\n",
    "candidate_func = 'uniform'\n",
    "graph_labels = None\n",
    "\n",
    "model = AnonymousWalkEmbeddings.AWE(dataset = dataset, batch_size = batch_size, window_size = window_size,\n",
    "                  num_samples = num_samples, root = root,\n",
    "                  ext = ext, epochs = epochs, batches_per_epoch = batches_per_epoch,\n",
    "                  candidate_func = candidate_func, graph_labels=graph_labels)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 4127\n",
      "Number of words: 203\n",
      "Initialized\n",
      "Epoch: 0\n",
      "Graph 0: 19 nodes\n",
      "Average loss at step 100: 7307.982849\n",
      "Average loss at step 200: 1942.744523\n",
      "Average loss at step 300: 769.240798\n",
      "Average loss at step 400: 509.437876\n",
      "Average loss at step 500: 425.491517\n",
      "Average loss at step 600: 398.636527\n",
      "Average loss at step 700: 375.318670\n",
      "Average loss at step 800: 372.443504\n",
      "Average loss at step 900: 363.731722\n",
      "Average loss at step 1000: 362.946617\n",
      "Average loss at step 1100: 352.959707\n",
      "Average loss at step 1200: 352.658599\n",
      "Average loss at step 1300: 348.796355\n",
      "Average loss at step 1400: 344.631526\n",
      "Average loss at step 1500: 348.651993\n",
      "Average loss at step 1600: 343.474630\n",
      "Average loss at step 1700: 342.307970\n",
      "Average loss at step 1800: 344.178941\n",
      "Average loss at step 1900: 340.985013\n",
      "Average loss at step 2000: 339.017730\n",
      "Average loss at step 2100: 344.622488\n",
      "Average loss at step 2200: 345.871047\n",
      "Average loss at step 2300: 339.205139\n",
      "Average loss at step 2400: 341.376259\n",
      "Average loss at step 2500: 341.938422\n",
      "Average loss at step 2600: 339.861599\n",
      "Average loss at step 2700: 335.932302\n",
      "Average loss at step 2800: 341.858210\n",
      "Average loss at step 2900: 342.209166\n",
      "Average loss at step 3000: 337.529419\n",
      "Average loss at step 3100: 338.824199\n",
      "Average loss at step 3200: 334.485239\n",
      "Average loss at step 3300: 343.144279\n",
      "Average loss at step 3400: 337.662418\n",
      "Average loss at step 3500: 336.715301\n",
      "Average loss at step 3600: 334.725632\n",
      "Average loss at step 3700: 336.204521\n",
      "Average loss at step 3800: 338.512233\n",
      "Average loss at step 3900: 334.198875\n",
      "Average loss at step 4000: 339.236967\n",
      "Average loss at step 4100: 335.772093\n",
      "Average loss at step 4200: 337.517799\n",
      "Average loss at step 4300: 339.046846\n",
      "Average loss at step 4400: 334.085323\n",
      "Average loss at step 4500: 336.021610\n",
      "Average loss at step 4600: 336.697301\n",
      "Average loss at step 4700: 336.795358\n",
      "Average loss at step 4800: 335.730326\n",
      "Average loss at step 4900: 339.981425\n",
      "Average loss at step 5000: 335.188274\n",
      "Time: 23.2101740837\n",
      "Graph 1: 20 nodes\n",
      "Average loss at step 5100: 538.032118\n",
      "Average loss at step 5200: 344.373477\n",
      "Average loss at step 5300: 343.550362\n",
      "Average loss at step 5400: 339.169782\n",
      "Average loss at step 5500: 339.386278\n",
      "Average loss at step 5600: 338.263631\n",
      "Average loss at step 5700: 336.329941\n",
      "Average loss at step 5800: 336.204489\n",
      "Average loss at step 5900: 335.141625\n",
      "Average loss at step 6000: 332.294491\n",
      "Average loss at step 6100: 332.050528\n",
      "Average loss at step 6200: 335.554464\n",
      "Average loss at step 6300: 332.941667\n",
      "Average loss at step 6400: 332.764045\n",
      "Average loss at step 6500: 334.848435\n",
      "Average loss at step 6600: 335.849002\n",
      "Average loss at step 6700: 335.107070\n",
      "Average loss at step 6800: 332.990158\n",
      "Average loss at step 6900: 334.600087\n",
      "Average loss at step 7000: 334.763175\n",
      "Average loss at step 7100: 338.619581\n",
      "Average loss at step 7200: 331.738160\n",
      "Average loss at step 7300: 336.199436\n",
      "Average loss at step 7400: 332.744025\n",
      "Average loss at step 7500: 333.149564\n",
      "Average loss at step 7600: 335.190646\n",
      "Average loss at step 7700: 336.599586\n",
      "Average loss at step 7800: 332.215308\n",
      "Average loss at step 7900: 333.012042\n",
      "Average loss at step 8000: 334.093949\n",
      "Average loss at step 8100: 332.584550\n",
      "Average loss at step 8200: 333.421141\n",
      "Average loss at step 8300: 334.350362\n",
      "Average loss at step 8400: 335.657735\n",
      "Average loss at step 8500: 331.884346\n",
      "Average loss at step 8600: 330.280918\n",
      "Average loss at step 8700: 334.696715\n",
      "Average loss at step 8800: 336.588713\n",
      "Average loss at step 8900: 330.753788\n",
      "Average loss at step 9000: 332.143555\n",
      "Average loss at step 9100: 333.216781\n",
      "Average loss at step 9200: 329.988550\n",
      "Average loss at step 9300: 334.212440\n",
      "Average loss at step 9400: 331.637703\n",
      "Average loss at step 9500: 330.864721\n",
      "Average loss at step 9600: 333.735263\n",
      "Average loss at step 9700: 332.961561\n",
      "Average loss at step 9800: 331.049375\n",
      "Average loss at step 9900: 329.213285\n",
      "Average loss at step 10000: 334.745245\n",
      "Graph 2: 30 nodes\n",
      "Average loss at step 10100: 475.422197\n",
      "Average loss at step 10200: 335.767575\n",
      "Average loss at step 10300: 328.093169\n",
      "Average loss at step 10400: 330.659957\n",
      "Average loss at step 10500: 324.784259\n",
      "Average loss at step 10600: 326.263907\n",
      "Average loss at step 10700: 324.161952\n",
      "Average loss at step 10800: 328.789203\n",
      "Average loss at step 10900: 323.975354\n",
      "Average loss at step 11000: 326.973011\n",
      "Average loss at step 11100: 326.587086\n",
      "Average loss at step 11200: 323.233812\n",
      "Average loss at step 11300: 327.891678\n",
      "Average loss at step 11400: 324.746495\n",
      "Average loss at step 11500: 322.370440\n",
      "Average loss at step 11600: 326.575703\n",
      "Average loss at step 11700: 323.959818\n",
      "Average loss at step 11800: 325.225170\n",
      "Average loss at step 11900: 320.763370\n",
      "Average loss at step 12000: 326.610186\n",
      "Average loss at step 12100: 328.387059\n",
      "Average loss at step 12200: 324.797181\n",
      "Average loss at step 12300: 326.094139\n",
      "Average loss at step 12400: 323.636245\n",
      "Average loss at step 12500: 325.270253\n",
      "Average loss at step 12600: 326.208625\n",
      "Average loss at step 12700: 319.864980\n",
      "Average loss at step 12800: 319.746215\n",
      "Average loss at step 12900: 322.848743\n",
      "Average loss at step 13000: 322.460932\n",
      "Average loss at step 13100: 327.662991\n",
      "Average loss at step 13200: 323.442471\n",
      "Average loss at step 13300: 324.507165\n",
      "Average loss at step 13400: 326.307274\n",
      "Average loss at step 13500: 319.416928\n",
      "Average loss at step 13600: 326.581646\n",
      "Average loss at step 13700: 326.678964\n",
      "Average loss at step 13800: 324.655480\n",
      "Average loss at step 13900: 324.358743\n",
      "Average loss at step 14000: 324.110579\n",
      "Average loss at step 14100: 327.484031\n",
      "Average loss at step 14200: 320.606601\n",
      "Average loss at step 14300: 324.621527\n",
      "Average loss at step 14400: 321.118540\n",
      "Average loss at step 14500: 323.069663\n",
      "Average loss at step 14600: 317.472284\n",
      "Average loss at step 14700: 324.020273\n",
      "Average loss at step 14800: 322.914535\n",
      "Average loss at step 14900: 322.390331\n",
      "Average loss at step 15000: 323.269037\n",
      "Graph 3: 35 nodes\n",
      "Average loss at step 15100: 477.208621\n",
      "Average loss at step 15200: 352.214344\n",
      "Average loss at step 15300: 343.393869\n",
      "Average loss at step 15400: 345.147718\n",
      "Average loss at step 15500: 344.045621\n",
      "Average loss at step 15600: 339.526551\n",
      "Average loss at step 15700: 337.794953\n",
      "Average loss at step 15800: 338.827398\n",
      "Average loss at step 15900: 338.830466\n",
      "Average loss at step 16000: 339.372285\n",
      "Average loss at step 16100: 344.339558\n",
      "Average loss at step 16200: 337.402089\n",
      "Average loss at step 16300: 338.185251\n",
      "Average loss at step 16400: 338.531852\n",
      "Average loss at step 16500: 334.364534\n",
      "Average loss at step 16600: 339.785506\n",
      "Average loss at step 16700: 337.144615\n",
      "Average loss at step 16800: 335.843496\n",
      "Average loss at step 16900: 335.932035\n",
      "Average loss at step 17000: 334.478858\n",
      "Average loss at step 17100: 339.049565\n",
      "Average loss at step 17200: 338.265519\n",
      "Average loss at step 17300: 339.603594\n",
      "Average loss at step 17400: 343.426698\n",
      "Average loss at step 17500: 335.775143\n",
      "Average loss at step 17600: 336.048677\n",
      "Average loss at step 17700: 337.584858\n",
      "Average loss at step 17800: 334.835726\n",
      "Average loss at step 17900: 332.834600\n",
      "Average loss at step 18000: 336.849075\n",
      "Average loss at step 18100: 337.295674\n",
      "Average loss at step 18200: 338.633927\n",
      "Average loss at step 18300: 335.069216\n",
      "Average loss at step 18400: 334.027053\n",
      "Average loss at step 18500: 337.499285\n",
      "Average loss at step 18600: 337.543656\n",
      "Average loss at step 18700: 339.768524\n",
      "Average loss at step 18800: 336.422216\n",
      "Average loss at step 18900: 333.946973\n",
      "Average loss at step 19000: 335.938912\n",
      "Average loss at step 19100: 341.654549\n",
      "Average loss at step 19200: 335.839257\n",
      "Average loss at step 19300: 331.809033\n",
      "Average loss at step 19400: 337.644672\n",
      "Average loss at step 19500: 335.317259\n",
      "Average loss at step 19600: 339.348630\n",
      "Average loss at step 19700: 335.166748\n",
      "Average loss at step 19800: 337.383430\n",
      "Average loss at step 19900: 335.609636\n",
      "Average loss at step 20000: 338.877410\n",
      "Graph 4: 23 nodes\n",
      "Average loss at step 20100: 420.940170\n",
      "Average loss at step 20200: 343.226860\n",
      "Average loss at step 20300: 340.617984\n",
      "Average loss at step 20400: 337.326624\n",
      "Average loss at step 20500: 337.776699\n",
      "Average loss at step 20600: 337.147485\n",
      "Average loss at step 20700: 337.083609\n",
      "Average loss at step 20800: 336.223543\n",
      "Average loss at step 20900: 335.599165\n",
      "Average loss at step 21000: 332.958016\n",
      "Average loss at step 21100: 339.207067\n",
      "Average loss at step 21200: 331.901633\n",
      "Average loss at step 21300: 332.210235\n",
      "Average loss at step 21400: 333.676838\n",
      "Average loss at step 21500: 335.433062\n",
      "Average loss at step 21600: 334.888628\n",
      "Average loss at step 21700: 333.148789\n",
      "Average loss at step 21800: 330.145168\n",
      "Average loss at step 21900: 334.536170\n",
      "Average loss at step 22000: 333.750983\n",
      "Average loss at step 22100: 335.034066\n",
      "Average loss at step 22200: 335.199339\n",
      "Average loss at step 22300: 334.426891\n",
      "Average loss at step 22400: 333.557246\n",
      "Average loss at step 22500: 333.704736\n",
      "Average loss at step 22600: 337.379729\n",
      "Average loss at step 22700: 334.551087\n",
      "Average loss at step 22800: 337.247918\n",
      "Average loss at step 22900: 331.905953\n",
      "Average loss at step 23000: 332.179486\n",
      "Average loss at step 23100: 336.008160\n",
      "Average loss at step 23200: 332.442925\n",
      "Average loss at step 23300: 332.274219\n",
      "Average loss at step 23400: 329.138035\n",
      "Average loss at step 23500: 334.466323\n",
      "Average loss at step 23600: 334.531184\n",
      "Average loss at step 23700: 335.224896\n",
      "Average loss at step 23800: 330.628410\n",
      "Average loss at step 23900: 336.158652\n",
      "Average loss at step 24000: 333.600256\n",
      "Average loss at step 24100: 331.590291\n",
      "Average loss at step 24200: 334.521532\n",
      "Average loss at step 24300: 333.040966\n",
      "Average loss at step 24400: 334.547620\n",
      "Average loss at step 24500: 333.365198\n",
      "Average loss at step 24600: 332.606950\n",
      "Average loss at step 24700: 330.046579\n",
      "Average loss at step 24800: 333.122118\n",
      "Average loss at step 24900: 333.212650\n",
      "Average loss at step 25000: 335.589625\n",
      "Graph 5: 25 nodes\n",
      "Average loss at step 25100: 434.218262\n",
      "Average loss at step 25200: 336.841027\n",
      "Average loss at step 25300: 340.021685\n",
      "Average loss at step 25400: 331.838768\n",
      "Average loss at step 25500: 333.424903\n",
      "Average loss at step 25600: 334.991803\n",
      "Average loss at step 25700: 333.347216\n",
      "Average loss at step 25800: 334.433140\n",
      "Average loss at step 25900: 330.983965\n",
      "Average loss at step 26000: 334.080095\n",
      "Average loss at step 26100: 337.295311\n",
      "Average loss at step 26200: 330.624615\n",
      "Average loss at step 26300: 332.458136\n",
      "Average loss at step 26400: 330.881572\n",
      "Average loss at step 26500: 331.983720\n",
      "Average loss at step 26600: 329.568755\n",
      "Average loss at step 26700: 331.535818\n",
      "Average loss at step 26800: 333.187893\n",
      "Average loss at step 26900: 331.295845\n",
      "Average loss at step 27000: 334.252118\n",
      "Average loss at step 27100: 333.483693\n",
      "Average loss at step 27200: 330.701650\n",
      "Average loss at step 27300: 331.461130\n",
      "Average loss at step 27400: 331.238810\n",
      "Average loss at step 27500: 330.224116\n",
      "Average loss at step 27600: 331.842532\n",
      "Average loss at step 27700: 332.708663\n",
      "Average loss at step 27800: 328.385075\n",
      "Average loss at step 27900: 334.699429\n",
      "Average loss at step 28000: 336.254877\n",
      "Average loss at step 28100: 331.796350\n",
      "Average loss at step 28200: 330.010715\n",
      "Average loss at step 28300: 331.009886\n",
      "Average loss at step 28400: 334.146443\n",
      "Average loss at step 28500: 334.175331\n",
      "Average loss at step 28600: 327.924084\n",
      "Average loss at step 28700: 333.513098\n",
      "Average loss at step 28800: 332.171994\n",
      "Average loss at step 28900: 330.045822\n",
      "Average loss at step 29000: 332.219445\n",
      "Average loss at step 29100: 332.513171\n",
      "Average loss at step 29200: 331.243981\n",
      "Average loss at step 29300: 332.117701\n",
      "Average loss at step 29400: 331.020881\n",
      "Average loss at step 29500: 333.005600\n",
      "Average loss at step 29600: 329.653086\n",
      "Average loss at step 29700: 330.334991\n",
      "Average loss at step 29800: 327.962184\n",
      "Average loss at step 29900: 331.869188\n",
      "Average loss at step 30000: 330.429487\n",
      "Graph 6: 34 nodes\n",
      "Average loss at step 30100: 418.935776\n",
      "Average loss at step 30200: 344.772877\n",
      "Average loss at step 30300: 344.632193\n",
      "Average loss at step 30400: 345.138503\n",
      "Average loss at step 30500: 346.326387\n",
      "Average loss at step 30600: 338.320927\n",
      "Average loss at step 30700: 338.304768\n",
      "Average loss at step 30800: 338.889452\n",
      "Average loss at step 30900: 335.675194\n",
      "Average loss at step 31000: 339.235636\n",
      "Average loss at step 31100: 338.262591\n",
      "Average loss at step 31200: 337.122683\n",
      "Average loss at step 31300: 336.683008\n",
      "Average loss at step 31400: 334.232984\n",
      "Average loss at step 31500: 337.863836\n",
      "Average loss at step 31600: 336.549761\n",
      "Average loss at step 31700: 337.155084\n",
      "Average loss at step 31800: 336.056283\n",
      "Average loss at step 31900: 336.131940\n",
      "Average loss at step 32000: 335.790274\n",
      "Average loss at step 32100: 338.225849\n",
      "Average loss at step 32200: 335.832058\n",
      "Average loss at step 32300: 335.921836\n",
      "Average loss at step 32400: 336.201764\n",
      "Average loss at step 32500: 339.039318\n",
      "Average loss at step 32600: 334.153026\n",
      "Average loss at step 32700: 336.414168\n",
      "Average loss at step 32800: 338.892303\n",
      "Average loss at step 32900: 335.354667\n",
      "Average loss at step 33000: 335.711744\n",
      "Average loss at step 33100: 337.994383\n",
      "Average loss at step 33200: 336.875919\n",
      "Average loss at step 33300: 336.222686\n",
      "Average loss at step 33400: 337.506577\n",
      "Average loss at step 33500: 335.229886\n",
      "Average loss at step 33600: 333.633213\n",
      "Average loss at step 33700: 334.230968\n",
      "Average loss at step 33800: 334.927850\n",
      "Average loss at step 33900: 334.946345\n",
      "Average loss at step 34000: 338.094160\n",
      "Average loss at step 34100: 334.104234\n",
      "Average loss at step 34200: 334.959822\n",
      "Average loss at step 34300: 336.489376\n",
      "Average loss at step 34400: 334.828723\n",
      "Average loss at step 34500: 336.596730\n",
      "Average loss at step 34600: 337.225257\n",
      "Average loss at step 34700: 332.447294\n",
      "Average loss at step 34800: 332.178664\n",
      "Average loss at step 34900: 335.958866\n",
      "Average loss at step 35000: 339.938197\n",
      "Graph 7: 33 nodes\n",
      "Average loss at step 35100: 408.762365\n",
      "Average loss at step 35200: 343.032440\n",
      "Average loss at step 35300: 336.941839\n",
      "Average loss at step 35400: 337.805724\n",
      "Average loss at step 35500: 337.319763\n",
      "Average loss at step 35600: 332.736112\n",
      "Average loss at step 35700: 337.246863\n",
      "Average loss at step 35800: 333.635551\n",
      "Average loss at step 35900: 335.874052\n",
      "Average loss at step 36000: 335.090089\n",
      "Average loss at step 36100: 334.383502\n",
      "Average loss at step 36200: 335.436248\n",
      "Average loss at step 36300: 333.848196\n",
      "Average loss at step 36400: 337.051864\n",
      "Average loss at step 36500: 335.446660\n",
      "Average loss at step 36600: 334.496141\n",
      "Average loss at step 36700: 337.123371\n",
      "Average loss at step 36800: 337.056831\n",
      "Average loss at step 36900: 335.198381\n",
      "Average loss at step 37000: 337.046931\n",
      "Average loss at step 37100: 336.731286\n",
      "Average loss at step 37200: 331.927919\n",
      "Average loss at step 37300: 335.237709\n",
      "Average loss at step 37400: 334.668380\n",
      "Average loss at step 37500: 331.936622\n",
      "Average loss at step 37600: 333.757716\n",
      "Average loss at step 37700: 332.292867\n",
      "Average loss at step 37800: 332.229065\n",
      "Average loss at step 37900: 331.236364\n",
      "Average loss at step 38000: 334.299231\n",
      "Average loss at step 38100: 333.587311\n",
      "Average loss at step 38200: 337.027201\n",
      "Average loss at step 38300: 333.043960\n",
      "Average loss at step 38400: 333.070261\n",
      "Average loss at step 38500: 332.606491\n",
      "Average loss at step 38600: 333.625180\n",
      "Average loss at step 38700: 334.489943\n",
      "Average loss at step 38800: 333.511078\n",
      "Average loss at step 38900: 334.122815\n",
      "Average loss at step 39000: 333.046436\n",
      "Average loss at step 39100: 332.086893\n",
      "Average loss at step 39200: 334.906562\n",
      "Average loss at step 39300: 332.950134\n",
      "Average loss at step 39400: 335.142467\n",
      "Average loss at step 39500: 334.929352\n",
      "Average loss at step 39600: 334.806078\n",
      "Average loss at step 39700: 330.526081\n",
      "Average loss at step 39800: 333.334938\n",
      "Average loss at step 39900: 335.739172\n",
      "Average loss at step 40000: 335.619021\n",
      "Graph 8: 29 nodes\n",
      "Average loss at step 40100: 378.004927\n",
      "Average loss at step 40200: 336.976873\n",
      "Average loss at step 40300: 338.040063\n",
      "Average loss at step 40400: 333.763423\n",
      "Average loss at step 40500: 330.388982\n",
      "Average loss at step 40600: 335.273197\n",
      "Average loss at step 40700: 331.315201\n",
      "Average loss at step 40800: 335.090607\n",
      "Average loss at step 40900: 333.890991\n",
      "Average loss at step 41000: 334.225488\n",
      "Average loss at step 41100: 330.975740\n",
      "Average loss at step 41200: 332.744536\n",
      "Average loss at step 41300: 331.994056\n",
      "Average loss at step 41400: 331.138689\n",
      "Average loss at step 41500: 332.530677\n",
      "Average loss at step 41600: 332.315925\n",
      "Average loss at step 41700: 326.171145\n",
      "Average loss at step 41800: 330.543680\n",
      "Average loss at step 41900: 331.426924\n",
      "Average loss at step 42000: 330.877261\n",
      "Average loss at step 42100: 331.647128\n",
      "Average loss at step 42200: 330.295628\n",
      "Average loss at step 42300: 334.031888\n",
      "Average loss at step 42400: 330.721813\n",
      "Average loss at step 42500: 332.886082\n",
      "Average loss at step 42600: 329.109909\n",
      "Average loss at step 42700: 331.400732\n",
      "Average loss at step 42800: 330.393202\n",
      "Average loss at step 42900: 333.103767\n",
      "Average loss at step 43000: 334.425637\n",
      "Average loss at step 43100: 329.810910\n",
      "Average loss at step 43200: 332.550627\n",
      "Average loss at step 43300: 334.603030\n",
      "Average loss at step 43400: 330.769191\n",
      "Average loss at step 43500: 331.758660\n",
      "Average loss at step 43600: 331.890490\n",
      "Average loss at step 43700: 335.650534\n",
      "Average loss at step 43800: 330.008371\n",
      "Average loss at step 43900: 330.887067\n",
      "Average loss at step 44000: 331.637646\n",
      "Average loss at step 44100: 330.249405\n",
      "Average loss at step 44200: 329.221473\n",
      "Average loss at step 44300: 334.146463\n",
      "Average loss at step 44400: 332.924083\n",
      "Average loss at step 44500: 330.996278\n",
      "Average loss at step 44600: 330.242282\n",
      "Average loss at step 44700: 334.004110\n",
      "Average loss at step 44800: 330.561634\n",
      "Average loss at step 44900: 330.006879\n",
      "Average loss at step 45000: 333.146273\n",
      "Graph 9: 22 nodes\n",
      "Average loss at step 45100: 379.191867\n",
      "Average loss at step 45200: 330.394661\n",
      "Average loss at step 45300: 332.352212\n",
      "Average loss at step 45400: 330.577943\n",
      "Average loss at step 45500: 331.893358\n",
      "Average loss at step 45600: 330.899170\n",
      "Average loss at step 45700: 328.070542\n",
      "Average loss at step 45800: 330.777393\n",
      "Average loss at step 45900: 330.274643\n",
      "Average loss at step 46000: 330.182923\n",
      "Average loss at step 46100: 330.918264\n",
      "Average loss at step 46200: 329.552881\n",
      "Average loss at step 46300: 326.902371\n",
      "Average loss at step 46400: 328.198908\n",
      "Average loss at step 46500: 328.601389\n",
      "Average loss at step 46600: 329.257707\n",
      "Average loss at step 46700: 325.875650\n",
      "Average loss at step 46800: 331.501726\n",
      "Average loss at step 46900: 326.432556\n",
      "Average loss at step 47000: 331.317752\n",
      "Average loss at step 47100: 328.466234\n",
      "Average loss at step 47200: 324.538856\n",
      "Average loss at step 47300: 328.118839\n",
      "Average loss at step 47400: 325.484748\n",
      "Average loss at step 47500: 325.012562\n",
      "Average loss at step 47600: 327.852502\n",
      "Average loss at step 47700: 328.291898\n",
      "Average loss at step 47800: 326.263256\n",
      "Average loss at step 47900: 326.763049\n",
      "Average loss at step 48000: 329.521006\n",
      "Average loss at step 48100: 329.249237\n",
      "Average loss at step 48200: 329.456381\n",
      "Average loss at step 48300: 330.210987\n",
      "Average loss at step 48400: 328.529585\n",
      "Average loss at step 48500: 328.574073\n",
      "Average loss at step 48600: 326.678141\n",
      "Average loss at step 48700: 324.780925\n",
      "Average loss at step 48800: 329.132931\n",
      "Average loss at step 48900: 329.435012\n",
      "Average loss at step 49000: 329.674086\n",
      "Average loss at step 49100: 327.944269\n",
      "Average loss at step 49200: 330.108277\n",
      "Average loss at step 49300: 327.690665\n",
      "Average loss at step 49400: 329.259981\n",
      "Average loss at step 49500: 327.732439\n",
      "Average loss at step 49600: 325.383924\n",
      "Average loss at step 49700: 325.907394\n",
      "Average loss at step 49800: 327.384138\n",
      "Average loss at step 49900: 326.959166\n",
      "Average loss at step 50000: 326.705499\n",
      "Graph 10: 29 nodes\n",
      "Average loss at step 50100: 392.493624\n",
      "Average loss at step 50200: 337.430699\n",
      "Average loss at step 50300: 334.535822\n",
      "Average loss at step 50400: 335.258864\n",
      "Average loss at step 50500: 333.563913\n",
      "Average loss at step 50600: 333.668365\n",
      "Average loss at step 50700: 332.594459\n",
      "Average loss at step 50800: 331.476346\n",
      "Average loss at step 50900: 334.767926\n",
      "Average loss at step 51000: 332.742403\n",
      "Average loss at step 51100: 333.309549\n",
      "Average loss at step 51200: 327.234380\n",
      "Average loss at step 51300: 334.835839\n",
      "Average loss at step 51400: 335.343707\n",
      "Average loss at step 51500: 336.284788\n",
      "Average loss at step 51600: 334.126577\n",
      "Average loss at step 51700: 333.933749\n",
      "Average loss at step 51800: 334.833140\n",
      "Average loss at step 51900: 330.608867\n",
      "Average loss at step 52000: 334.286967\n",
      "Average loss at step 52100: 331.261991\n",
      "Average loss at step 52200: 328.924419\n",
      "Average loss at step 52300: 333.348365\n",
      "Average loss at step 52400: 332.574833\n",
      "Average loss at step 52500: 331.410119\n",
      "Average loss at step 52600: 331.460595\n",
      "Average loss at step 52700: 328.666750\n",
      "Average loss at step 52800: 331.366554\n",
      "Average loss at step 52900: 330.881465\n",
      "Average loss at step 53000: 333.719655\n",
      "Average loss at step 53100: 333.402910\n",
      "Average loss at step 53200: 331.400456\n",
      "Average loss at step 53300: 324.227714\n",
      "Average loss at step 53400: 329.995286\n",
      "Average loss at step 53500: 329.551451\n",
      "Average loss at step 53600: 331.903315\n",
      "Average loss at step 53700: 331.895998\n",
      "Average loss at step 53800: 327.364678\n",
      "Average loss at step 53900: 330.267341\n",
      "Average loss at step 54000: 329.671669\n",
      "Average loss at step 54100: 330.115896\n",
      "Average loss at step 54200: 330.400103\n",
      "Average loss at step 54300: 328.809700\n",
      "Average loss at step 54400: 326.886883\n",
      "Average loss at step 54500: 333.858316\n",
      "Average loss at step 54600: 329.531632\n",
      "Average loss at step 54700: 333.295873\n",
      "Average loss at step 54800: 332.090565\n",
      "Average loss at step 54900: 332.480435\n",
      "Average loss at step 55000: 324.599116\n",
      "Time: 32.7220220566\n",
      "Graph 11: 15 nodes\n",
      "Average loss at step 55100: 1084.531472\n",
      "Average loss at step 55200: 975.442123\n",
      "Average loss at step 55300: 869.930304\n",
      "Average loss at step 55400: 709.929791\n",
      "Average loss at step 55500: 604.502561\n",
      "Average loss at step 55600: 566.393461\n",
      "Average loss at step 55700: 523.663009\n",
      "Average loss at step 55800: 522.506901\n",
      "Average loss at step 55900: 501.397807\n",
      "Average loss at step 56000: 462.513315\n",
      "Average loss at step 56100: 431.055899\n",
      "Average loss at step 56200: 426.645453\n",
      "Average loss at step 56300: 439.716431\n",
      "Average loss at step 56400: 420.225346\n",
      "Average loss at step 56500: 423.098065\n",
      "Average loss at step 56600: 409.293208\n",
      "Average loss at step 56700: 399.699828\n",
      "Average loss at step 56800: 404.402290\n",
      "Average loss at step 56900: 404.799179\n",
      "Average loss at step 57000: 402.137436\n",
      "Average loss at step 57100: 398.159927\n",
      "Average loss at step 57200: 398.391431\n",
      "Average loss at step 57300: 398.857132\n",
      "Average loss at step 57400: 384.645514\n",
      "Average loss at step 57500: 387.298330\n",
      "Average loss at step 57600: 386.885060\n",
      "Average loss at step 57700: 389.093873\n",
      "Average loss at step 57800: 390.878757\n",
      "Average loss at step 57900: 378.697764\n",
      "Average loss at step 58000: 380.138764\n",
      "Average loss at step 58100: 382.478549\n",
      "Average loss at step 58200: 389.566456\n",
      "Average loss at step 58300: 376.870293\n",
      "Average loss at step 58400: 384.462467\n",
      "Average loss at step 58500: 372.970524\n",
      "Average loss at step 58600: 377.369360\n",
      "Average loss at step 58700: 387.713573\n",
      "Average loss at step 58800: 379.146897\n",
      "Average loss at step 58900: 373.096615\n",
      "Average loss at step 59000: 372.224325\n",
      "Average loss at step 59100: 376.256819\n",
      "Average loss at step 59200: 374.570125\n",
      "Average loss at step 59300: 367.928462\n",
      "Average loss at step 59400: 371.336623\n",
      "Average loss at step 59500: 381.291322\n",
      "Average loss at step 59600: 369.468572\n",
      "Average loss at step 59700: 377.846089\n",
      "Average loss at step 59800: 381.062917\n",
      "Average loss at step 59900: 367.785123\n",
      "Average loss at step 60000: 374.054338\n",
      "Graph 12: 20 nodes\n",
      "Average loss at step 60100: 367.508019\n",
      "Average loss at step 60200: 336.638454\n",
      "Average loss at step 60300: 328.989492\n",
      "Average loss at step 60400: 335.069680\n",
      "Average loss at step 60500: 333.711358\n",
      "Average loss at step 60600: 332.105837\n",
      "Average loss at step 60700: 330.289420\n",
      "Average loss at step 60800: 331.413046\n",
      "Average loss at step 60900: 329.268802\n",
      "Average loss at step 61000: 331.534913\n",
      "Average loss at step 61100: 328.993052\n",
      "Average loss at step 61200: 327.144650\n",
      "Average loss at step 61300: 326.752555\n",
      "Average loss at step 61400: 333.513282\n",
      "Average loss at step 61500: 328.715789\n",
      "Average loss at step 61600: 326.597342\n",
      "Average loss at step 61700: 330.977205\n",
      "Average loss at step 61800: 326.548188\n",
      "Average loss at step 61900: 325.668227\n",
      "Average loss at step 62000: 328.595559\n",
      "Average loss at step 62100: 326.846180\n",
      "Average loss at step 62200: 334.755461\n",
      "Average loss at step 62300: 332.069232\n",
      "Average loss at step 62400: 328.667619\n",
      "Average loss at step 62500: 329.675213\n",
      "Average loss at step 62600: 325.589090\n",
      "Average loss at step 62700: 330.478559\n",
      "Average loss at step 62800: 326.590057\n",
      "Average loss at step 62900: 326.907805\n",
      "Average loss at step 63000: 331.983330\n",
      "Average loss at step 63100: 329.274482\n",
      "Average loss at step 63200: 325.038415\n",
      "Average loss at step 63300: 329.334510\n",
      "Average loss at step 63400: 327.588745\n",
      "Average loss at step 63500: 329.881873\n",
      "Average loss at step 63600: 324.820060\n",
      "Average loss at step 63700: 329.761381\n",
      "Average loss at step 63800: 328.930521\n",
      "Average loss at step 63900: 327.749797\n",
      "Average loss at step 64000: 326.763217\n",
      "Average loss at step 64100: 330.350913\n",
      "Average loss at step 64200: 327.485569\n",
      "Average loss at step 64300: 328.715186\n",
      "Average loss at step 64400: 325.351474\n",
      "Average loss at step 64500: 326.191597\n",
      "Average loss at step 64600: 327.304865\n",
      "Average loss at step 64700: 326.896845\n",
      "Average loss at step 64800: 330.462891\n",
      "Average loss at step 64900: 327.889220\n",
      "Average loss at step 65000: 327.574622\n",
      "Graph 13: 15 nodes\n",
      "Average loss at step 65100: 433.300670\n",
      "Average loss at step 65200: 343.352059\n",
      "Average loss at step 65300: 349.203859\n",
      "Average loss at step 65400: 346.181294\n",
      "Average loss at step 65500: 345.883045\n",
      "Average loss at step 65600: 339.749342\n",
      "Average loss at step 65700: 338.677455\n",
      "Average loss at step 65800: 344.360167\n",
      "Average loss at step 65900: 347.886437\n",
      "Average loss at step 66000: 340.850554\n",
      "Average loss at step 66100: 342.101478\n",
      "Average loss at step 66200: 341.229106\n",
      "Average loss at step 66300: 336.599290\n",
      "Average loss at step 66400: 338.002043\n",
      "Average loss at step 66500: 341.818250\n",
      "Average loss at step 66600: 341.092098\n",
      "Average loss at step 66700: 342.008726\n",
      "Average loss at step 66800: 337.215266\n",
      "Average loss at step 66900: 335.847464\n",
      "Average loss at step 67000: 341.163584\n",
      "Average loss at step 67100: 339.195825\n",
      "Average loss at step 67200: 340.446708\n",
      "Average loss at step 67300: 338.856484\n",
      "Average loss at step 67400: 336.686504\n",
      "Average loss at step 67500: 337.189145\n",
      "Average loss at step 67600: 338.050905\n",
      "Average loss at step 67700: 336.805098\n",
      "Average loss at step 67800: 335.766497\n",
      "Average loss at step 67900: 336.128139\n",
      "Average loss at step 68000: 343.160266\n",
      "Average loss at step 68100: 336.838474\n",
      "Average loss at step 68200: 336.641046\n",
      "Average loss at step 68300: 339.459996\n",
      "Average loss at step 68400: 339.694776\n",
      "Average loss at step 68500: 338.738467\n",
      "Average loss at step 68600: 337.842808\n",
      "Average loss at step 68700: 337.076527\n",
      "Average loss at step 68800: 337.044357\n",
      "Average loss at step 68900: 336.602496\n",
      "Average loss at step 69000: 334.018824\n",
      "Average loss at step 69100: 332.357286\n",
      "Average loss at step 69200: 335.559556\n",
      "Average loss at step 69300: 337.767101\n",
      "Average loss at step 69400: 334.442820\n",
      "Average loss at step 69500: 334.601009\n",
      "Average loss at step 69600: 334.280573\n",
      "Average loss at step 69700: 339.906769\n",
      "Average loss at step 69800: 336.681368\n",
      "Average loss at step 69900: 338.840219\n",
      "Average loss at step 70000: 335.570981\n",
      "Graph 14: 27 nodes\n",
      "Average loss at step 70100: 422.285521\n",
      "Average loss at step 70200: 339.932559\n",
      "Average loss at step 70300: 338.554839\n",
      "Average loss at step 70400: 339.229929\n",
      "Average loss at step 70500: 335.244278\n",
      "Average loss at step 70600: 337.803228\n",
      "Average loss at step 70700: 333.880094\n",
      "Average loss at step 70800: 333.115943\n",
      "Average loss at step 70900: 330.940543\n",
      "Average loss at step 71000: 336.907430\n",
      "Average loss at step 71100: 334.541958\n",
      "Average loss at step 71200: 335.433114\n",
      "Average loss at step 71300: 336.327129\n",
      "Average loss at step 71400: 334.493565\n",
      "Average loss at step 71500: 334.815533\n",
      "Average loss at step 71600: 330.937701\n",
      "Average loss at step 71700: 336.460352\n",
      "Average loss at step 71800: 332.793192\n",
      "Average loss at step 71900: 334.885745\n",
      "Average loss at step 72000: 330.355363\n",
      "Average loss at step 72100: 330.548321\n",
      "Average loss at step 72200: 333.688656\n",
      "Average loss at step 72300: 334.945506\n",
      "Average loss at step 72400: 337.093207\n",
      "Average loss at step 72500: 335.787845\n",
      "Average loss at step 72600: 334.822123\n",
      "Average loss at step 72700: 336.979422\n",
      "Average loss at step 72800: 335.512759\n",
      "Average loss at step 72900: 331.952780\n",
      "Average loss at step 73000: 330.062516\n",
      "Average loss at step 73100: 328.831759\n",
      "Average loss at step 73200: 335.383792\n",
      "Average loss at step 73300: 335.733084\n",
      "Average loss at step 73400: 332.061087\n",
      "Average loss at step 73500: 333.910983\n",
      "Average loss at step 73600: 330.243345\n",
      "Average loss at step 73700: 334.469869\n",
      "Average loss at step 73800: 332.251347\n",
      "Average loss at step 73900: 333.700085\n",
      "Average loss at step 74000: 333.873028\n",
      "Average loss at step 74100: 334.518572\n",
      "Average loss at step 74200: 330.677944\n",
      "Average loss at step 74300: 334.395725\n",
      "Average loss at step 74400: 330.725241\n",
      "Average loss at step 74500: 331.848642\n",
      "Average loss at step 74600: 336.856447\n",
      "Average loss at step 74700: 335.549602\n",
      "Average loss at step 74800: 332.679498\n",
      "Average loss at step 74900: 331.520043\n",
      "Average loss at step 75000: 332.924196\n",
      "Graph 15: 25 nodes\n",
      "Average loss at step 75100: 351.397012\n",
      "Average loss at step 75200: 338.333063\n",
      "Average loss at step 75300: 333.240906\n",
      "Average loss at step 75400: 338.086118\n",
      "Average loss at step 75500: 332.797010\n",
      "Average loss at step 75600: 331.549520\n",
      "Average loss at step 75700: 335.119440\n",
      "Average loss at step 75800: 329.228858\n",
      "Average loss at step 75900: 334.908739\n",
      "Average loss at step 76000: 331.870355\n",
      "Average loss at step 76100: 332.123167\n",
      "Average loss at step 76200: 329.825173\n",
      "Average loss at step 76300: 334.896272\n",
      "Average loss at step 76400: 332.431197\n",
      "Average loss at step 76500: 331.671017\n",
      "Average loss at step 76600: 330.249911\n",
      "Average loss at step 76700: 327.356924\n",
      "Average loss at step 76800: 332.234325\n",
      "Average loss at step 76900: 331.067610\n",
      "Average loss at step 77000: 329.657636\n",
      "Average loss at step 77100: 332.453918\n",
      "Average loss at step 77200: 333.564914\n",
      "Average loss at step 77300: 332.586360\n",
      "Average loss at step 77400: 334.102224\n",
      "Average loss at step 77500: 331.170818\n",
      "Average loss at step 77600: 334.453864\n",
      "Average loss at step 77700: 328.756097\n",
      "Average loss at step 77800: 335.027338\n",
      "Average loss at step 77900: 329.339389\n",
      "Average loss at step 78000: 331.660290\n",
      "Average loss at step 78100: 328.980795\n",
      "Average loss at step 78200: 331.746992\n",
      "Average loss at step 78300: 334.362548\n",
      "Average loss at step 78400: 331.344972\n",
      "Average loss at step 78500: 328.066973\n",
      "Average loss at step 78600: 333.544703\n",
      "Average loss at step 78700: 333.702319\n",
      "Average loss at step 78800: 328.728608\n",
      "Average loss at step 78900: 333.050239\n",
      "Average loss at step 79000: 331.544330\n",
      "Average loss at step 79100: 334.283746\n",
      "Average loss at step 79200: 331.687126\n",
      "Average loss at step 79300: 329.483202\n",
      "Average loss at step 79400: 332.514798\n",
      "Average loss at step 79500: 334.147540\n",
      "Average loss at step 79600: 332.265457\n",
      "Average loss at step 79700: 329.306301\n",
      "Average loss at step 79800: 333.791926\n",
      "Average loss at step 79900: 332.564816\n",
      "Average loss at step 80000: 331.303762\n",
      "Graph 16: 25 nodes\n",
      "Average loss at step 80100: 450.748719\n",
      "Average loss at step 80200: 368.834503\n",
      "Average loss at step 80300: 365.888148\n",
      "Average loss at step 80400: 368.517199\n",
      "Average loss at step 80500: 362.407500\n",
      "Average loss at step 80600: 363.264602\n",
      "Average loss at step 80700: 362.314368\n",
      "Average loss at step 80800: 365.305011\n",
      "Average loss at step 80900: 358.634097\n",
      "Average loss at step 81000: 359.375069\n",
      "Average loss at step 81100: 359.918848\n",
      "Average loss at step 81200: 357.253705\n",
      "Average loss at step 81300: 366.804959\n",
      "Average loss at step 81400: 361.832468\n",
      "Average loss at step 81500: 366.933197\n",
      "Average loss at step 81600: 363.038349\n",
      "Average loss at step 81700: 356.073275\n",
      "Average loss at step 81800: 364.298191\n",
      "Average loss at step 81900: 352.370116\n",
      "Average loss at step 82000: 361.412254\n",
      "Average loss at step 82100: 365.881678\n",
      "Average loss at step 82200: 355.799836\n",
      "Average loss at step 82300: 361.398194\n",
      "Average loss at step 82400: 360.201528\n",
      "Average loss at step 82500: 354.836756\n",
      "Average loss at step 82600: 353.312891\n",
      "Average loss at step 82700: 354.392787\n",
      "Average loss at step 82800: 359.975559\n",
      "Average loss at step 82900: 357.421570\n",
      "Average loss at step 83000: 365.000806\n",
      "Average loss at step 83100: 356.302814\n",
      "Average loss at step 83200: 352.955587\n",
      "Average loss at step 83300: 354.971534\n",
      "Average loss at step 83400: 358.517637\n",
      "Average loss at step 83500: 356.408112\n",
      "Average loss at step 83600: 359.139815\n",
      "Average loss at step 83700: 354.109773\n",
      "Average loss at step 83800: 348.750047\n",
      "Average loss at step 83900: 356.177722\n",
      "Average loss at step 84000: 360.472563\n",
      "Average loss at step 84100: 356.286321\n",
      "Average loss at step 84200: 361.027606\n",
      "Average loss at step 84300: 354.240192\n",
      "Average loss at step 84400: 354.466268\n",
      "Average loss at step 84500: 363.288430\n",
      "Average loss at step 84600: 356.540923\n",
      "Average loss at step 84700: 350.900466\n",
      "Average loss at step 84800: 353.055004\n",
      "Average loss at step 84900: 361.625203\n",
      "Average loss at step 85000: 355.975709\n",
      "Graph 17: 17 nodes\n",
      "Average loss at step 85100: 485.593609\n",
      "Average loss at step 85200: 383.220370\n",
      "Average loss at step 85300: 401.487427\n",
      "Average loss at step 85400: 374.829886\n",
      "Average loss at step 85500: 385.209541\n",
      "Average loss at step 85600: 364.729519\n",
      "Average loss at step 85700: 381.328075\n",
      "Average loss at step 85800: 376.719392\n",
      "Average loss at step 85900: 372.816800\n",
      "Average loss at step 86000: 370.187303\n",
      "Average loss at step 86100: 367.521981\n",
      "Average loss at step 86200: 369.812471\n",
      "Average loss at step 86300: 362.920471\n",
      "Average loss at step 86400: 360.822330\n",
      "Average loss at step 86500: 364.155523\n",
      "Average loss at step 86600: 354.307826\n",
      "Average loss at step 86700: 356.120389\n",
      "Average loss at step 86800: 343.329912\n",
      "Average loss at step 86900: 359.528476\n",
      "Average loss at step 87000: 357.700391\n",
      "Average loss at step 87100: 342.695877\n",
      "Average loss at step 87200: 352.649540\n",
      "Average loss at step 87300: 352.123469\n",
      "Average loss at step 87400: 352.183464\n",
      "Average loss at step 87500: 344.606842\n",
      "Average loss at step 87600: 348.118586\n",
      "Average loss at step 87700: 346.671376\n",
      "Average loss at step 87800: 345.503968\n",
      "Average loss at step 87900: 348.641263\n",
      "Average loss at step 88000: 358.776682\n",
      "Average loss at step 88100: 343.638964\n",
      "Average loss at step 88200: 349.068114\n",
      "Average loss at step 88300: 347.043250\n",
      "Average loss at step 88400: 342.614519\n",
      "Average loss at step 88500: 345.086263\n",
      "Average loss at step 88600: 345.204856\n",
      "Average loss at step 88700: 338.761032\n",
      "Average loss at step 88800: 345.601266\n",
      "Average loss at step 88900: 345.949707\n",
      "Average loss at step 89000: 340.276615\n",
      "Average loss at step 89100: 343.592500\n",
      "Average loss at step 89200: 342.475601\n",
      "Average loss at step 89300: 342.332124\n",
      "Average loss at step 89400: 339.519020\n",
      "Average loss at step 89500: 345.603529\n",
      "Average loss at step 89600: 346.387157\n",
      "Average loss at step 89700: 340.972282\n",
      "Average loss at step 89800: 345.012009\n",
      "Average loss at step 89900: 349.469893\n",
      "Average loss at step 90000: 343.805314\n",
      "Graph 18: 18 nodes\n",
      "Average loss at step 90100: 358.865203\n",
      "Average loss at step 90200: 332.657617\n",
      "Average loss at step 90300: 330.554037\n",
      "Average loss at step 90400: 328.586869\n",
      "Average loss at step 90500: 328.872519\n",
      "Average loss at step 90600: 329.496624\n",
      "Average loss at step 90700: 329.003330\n",
      "Average loss at step 90800: 330.313148\n",
      "Average loss at step 90900: 326.896278\n",
      "Average loss at step 91000: 327.468609\n",
      "Average loss at step 91100: 329.262241\n",
      "Average loss at step 91200: 324.532965\n",
      "Average loss at step 91300: 327.695384\n",
      "Average loss at step 91400: 326.990106\n",
      "Average loss at step 91500: 322.705089\n",
      "Average loss at step 91600: 321.944690\n",
      "Average loss at step 91700: 324.732646\n",
      "Average loss at step 91800: 327.554672\n",
      "Average loss at step 91900: 326.723231\n",
      "Average loss at step 92000: 326.814182\n",
      "Average loss at step 92100: 323.173742\n",
      "Average loss at step 92200: 322.527515\n",
      "Average loss at step 92300: 325.968289\n",
      "Average loss at step 92400: 325.716066\n",
      "Average loss at step 92500: 326.881885\n",
      "Average loss at step 92600: 329.590887\n",
      "Average loss at step 92700: 322.531885\n",
      "Average loss at step 92800: 325.877844\n",
      "Average loss at step 92900: 324.134722\n",
      "Average loss at step 93000: 325.750366\n",
      "Average loss at step 93100: 323.368497\n",
      "Average loss at step 93200: 325.961243\n",
      "Average loss at step 93300: 324.679852\n",
      "Average loss at step 93400: 327.029677\n",
      "Average loss at step 93500: 324.861305\n",
      "Average loss at step 93600: 325.903774\n",
      "Average loss at step 93700: 321.667632\n",
      "Average loss at step 93800: 324.169118\n",
      "Average loss at step 93900: 321.038086\n",
      "Average loss at step 94000: 322.190695\n",
      "Average loss at step 94100: 324.463974\n",
      "Average loss at step 94200: 326.453498\n",
      "Average loss at step 94300: 324.118565\n",
      "Average loss at step 94400: 327.483782\n",
      "Average loss at step 94500: 322.045998\n",
      "Average loss at step 94600: 322.986901\n",
      "Average loss at step 94700: 325.181548\n",
      "Average loss at step 94800: 325.157669\n",
      "Average loss at step 94900: 326.950952\n",
      "Average loss at step 95000: 320.616597\n",
      "Graph 19: 16 nodes\n",
      "Average loss at step 95100: 363.154574\n",
      "Average loss at step 95200: 337.163991\n",
      "Average loss at step 95300: 333.267161\n",
      "Average loss at step 95400: 329.924250\n",
      "Average loss at step 95500: 330.238199\n",
      "Average loss at step 95600: 334.033365\n",
      "Average loss at step 95700: 332.184448\n",
      "Average loss at step 95800: 331.091971\n",
      "Average loss at step 95900: 330.487559\n",
      "Average loss at step 96000: 329.264890\n",
      "Average loss at step 96100: 329.323297\n",
      "Average loss at step 96200: 328.365381\n",
      "Average loss at step 96300: 329.127374\n",
      "Average loss at step 96400: 332.345247\n",
      "Average loss at step 96500: 330.647115\n",
      "Average loss at step 96600: 328.753502\n",
      "Average loss at step 96700: 327.322348\n",
      "Average loss at step 96800: 331.271759\n",
      "Average loss at step 96900: 329.235043\n",
      "Average loss at step 97000: 329.996213\n",
      "Average loss at step 97100: 328.437774\n",
      "Average loss at step 97200: 329.826645\n",
      "Average loss at step 97300: 329.690320\n",
      "Average loss at step 97400: 330.235249\n",
      "Average loss at step 97500: 329.057301\n",
      "Average loss at step 97600: 328.126208\n",
      "Average loss at step 97700: 332.312080\n",
      "Average loss at step 97800: 331.101781\n",
      "Average loss at step 97900: 327.766743\n",
      "Average loss at step 98000: 330.241305\n",
      "Average loss at step 98100: 329.198504\n",
      "Average loss at step 98200: 329.530330\n",
      "Average loss at step 98300: 329.737408\n",
      "Average loss at step 98400: 331.709913\n",
      "Average loss at step 98500: 328.802535\n",
      "Average loss at step 98600: 330.221532\n",
      "Average loss at step 98700: 327.946140\n",
      "Average loss at step 98800: 325.318083\n",
      "Average loss at step 98900: 328.027240\n",
      "Average loss at step 99000: 329.025942\n",
      "Average loss at step 99100: 333.305248\n",
      "Average loss at step 99200: 328.234153\n",
      "Average loss at step 99300: 329.265182\n",
      "Average loss at step 99400: 327.736723\n",
      "Average loss at step 99500: 322.661977\n",
      "Average loss at step 99600: 334.097760\n",
      "Average loss at step 99700: 326.988710\n",
      "Average loss at step 99800: 332.607284\n",
      "Average loss at step 99900: 327.808602\n",
      "Average loss at step 100000: 332.395616\n",
      "Graph 20: 19 nodes\n",
      "Average loss at step 100100: 354.927582\n",
      "Average loss at step 100200: 347.531729\n",
      "Average loss at step 100300: 340.803018\n",
      "Average loss at step 100400: 335.074217\n",
      "Average loss at step 100500: 339.354634\n",
      "Average loss at step 100600: 335.380338\n",
      "Average loss at step 100700: 338.136179\n",
      "Average loss at step 100800: 334.497535\n",
      "Average loss at step 100900: 338.690043\n",
      "Average loss at step 101000: 336.304840\n",
      "Average loss at step 101100: 345.591007\n",
      "Average loss at step 101200: 337.871851\n",
      "Average loss at step 101300: 339.149232\n",
      "Average loss at step 101400: 333.559711\n",
      "Average loss at step 101500: 339.751658\n",
      "Average loss at step 101600: 335.074342\n",
      "Average loss at step 101700: 339.274012\n",
      "Average loss at step 101800: 335.616724\n",
      "Average loss at step 101900: 337.193680\n",
      "Average loss at step 102000: 339.712563\n",
      "Average loss at step 102100: 333.876459\n",
      "Average loss at step 102200: 336.725606\n",
      "Average loss at step 102300: 335.372934\n",
      "Average loss at step 102400: 336.325464\n",
      "Average loss at step 102500: 338.432983\n",
      "Average loss at step 102600: 337.136317\n",
      "Average loss at step 102700: 334.062919\n",
      "Average loss at step 102800: 336.740946\n",
      "Average loss at step 102900: 335.004394\n",
      "Average loss at step 103000: 336.518681\n",
      "Average loss at step 103100: 338.340806\n",
      "Average loss at step 103200: 335.388114\n",
      "Average loss at step 103300: 336.184254\n",
      "Average loss at step 103400: 337.503532\n",
      "Average loss at step 103500: 333.096999\n",
      "Average loss at step 103600: 337.642464\n",
      "Average loss at step 103700: 338.697122\n",
      "Average loss at step 103800: 338.395581\n",
      "Average loss at step 103900: 337.411728\n",
      "Average loss at step 104000: 334.280341\n",
      "Average loss at step 104100: 339.954465\n",
      "Average loss at step 104200: 338.331450\n",
      "Average loss at step 104300: 334.992058\n",
      "Average loss at step 104400: 337.320634\n",
      "Average loss at step 104500: 335.221994\n",
      "Average loss at step 104600: 334.923320\n",
      "Average loss at step 104700: 334.636734\n",
      "Average loss at step 104800: 335.542585\n",
      "Average loss at step 104900: 334.335369\n",
      "Average loss at step 105000: 334.049169\n",
      "Time: 21.2425019741\n",
      "Graph 21: 20 nodes\n",
      "Average loss at step 105100: 372.586793\n",
      "Average loss at step 105200: 340.755371\n",
      "Average loss at step 105300: 344.717212\n",
      "Average loss at step 105400: 341.947926\n",
      "Average loss at step 105500: 338.714922\n",
      "Average loss at step 105600: 341.448501\n",
      "Average loss at step 105700: 340.227745\n",
      "Average loss at step 105800: 337.249905\n",
      "Average loss at step 105900: 334.354877\n",
      "Average loss at step 106000: 340.132403\n",
      "Average loss at step 106100: 338.801802\n",
      "Average loss at step 106200: 342.495684\n",
      "Average loss at step 106300: 338.618237\n",
      "Average loss at step 106400: 344.115151\n",
      "Average loss at step 106500: 336.694543\n",
      "Average loss at step 106600: 338.374221\n",
      "Average loss at step 106700: 335.323335\n",
      "Average loss at step 106800: 339.536663\n",
      "Average loss at step 106900: 334.996876\n",
      "Average loss at step 107000: 336.709128\n",
      "Average loss at step 107100: 342.448318\n",
      "Average loss at step 107200: 340.353210\n",
      "Average loss at step 107300: 340.222436\n",
      "Average loss at step 107400: 337.869495\n",
      "Average loss at step 107500: 340.497301\n",
      "Average loss at step 107600: 337.041790\n",
      "Average loss at step 107700: 340.618478\n",
      "Average loss at step 107800: 343.148041\n",
      "Average loss at step 107900: 336.488345\n",
      "Average loss at step 108000: 340.658029\n",
      "Average loss at step 108100: 339.326495\n",
      "Average loss at step 108200: 334.628348\n",
      "Average loss at step 108300: 337.552655\n",
      "Average loss at step 108400: 332.590079\n",
      "Average loss at step 108500: 333.844753\n",
      "Average loss at step 108600: 335.741567\n",
      "Average loss at step 108700: 335.951492\n",
      "Average loss at step 108800: 341.839721\n",
      "Average loss at step 108900: 336.682445\n",
      "Average loss at step 109000: 336.983111\n",
      "Average loss at step 109100: 338.303926\n",
      "Average loss at step 109200: 335.033778\n",
      "Average loss at step 109300: 336.546146\n",
      "Average loss at step 109400: 337.513449\n",
      "Average loss at step 109500: 333.984084\n",
      "Average loss at step 109600: 337.193254\n",
      "Average loss at step 109700: 335.080644\n",
      "Average loss at step 109800: 335.860140\n",
      "Average loss at step 109900: 338.092165\n",
      "Average loss at step 110000: 336.621391\n",
      "Graph 22: 23 nodes\n",
      "Average loss at step 110100: 360.230525\n",
      "Average loss at step 110200: 335.850291\n",
      "Average loss at step 110300: 334.117511\n",
      "Average loss at step 110400: 332.247127\n",
      "Average loss at step 110500: 330.272576\n",
      "Average loss at step 110600: 328.613381\n",
      "Average loss at step 110700: 328.190856\n",
      "Average loss at step 110800: 333.495460\n",
      "Average loss at step 110900: 329.270587\n",
      "Average loss at step 111000: 324.457392\n",
      "Average loss at step 111100: 329.384155\n",
      "Average loss at step 111200: 323.492750\n",
      "Average loss at step 111300: 332.015684\n",
      "Average loss at step 111400: 330.179072\n",
      "Average loss at step 111500: 328.795445\n",
      "Average loss at step 111600: 327.498890\n",
      "Average loss at step 111700: 330.389909\n",
      "Average loss at step 111800: 328.028606\n",
      "Average loss at step 111900: 327.590244\n",
      "Average loss at step 112000: 326.525623\n",
      "Average loss at step 112100: 323.406204\n",
      "Average loss at step 112200: 325.444103\n",
      "Average loss at step 112300: 329.221256\n",
      "Average loss at step 112400: 327.898146\n",
      "Average loss at step 112500: 331.035623\n",
      "Average loss at step 112600: 326.507320\n",
      "Average loss at step 112700: 327.252148\n",
      "Average loss at step 112800: 328.127197\n",
      "Average loss at step 112900: 328.238376\n",
      "Average loss at step 113000: 326.518594\n",
      "Average loss at step 113100: 329.229769\n",
      "Average loss at step 113200: 323.772232\n",
      "Average loss at step 113300: 328.420883\n",
      "Average loss at step 113400: 329.894290\n",
      "Average loss at step 113500: 326.448456\n",
      "Average loss at step 113600: 328.551300\n",
      "Average loss at step 113700: 327.378621\n",
      "Average loss at step 113800: 331.215161\n",
      "Average loss at step 113900: 328.380249\n",
      "Average loss at step 114000: 326.792410\n",
      "Average loss at step 114100: 319.662749\n",
      "Average loss at step 114200: 325.154446\n",
      "Average loss at step 114300: 326.093645\n",
      "Average loss at step 114400: 329.117504\n",
      "Average loss at step 114500: 328.073310\n",
      "Average loss at step 114600: 322.770710\n",
      "Average loss at step 114700: 329.848656\n",
      "Average loss at step 114800: 325.492449\n",
      "Average loss at step 114900: 325.736408\n",
      "Average loss at step 115000: 326.863548\n",
      "Graph 23: 12 nodes\n",
      "Average loss at step 115100: 467.612337\n",
      "Average loss at step 115200: 357.520089\n",
      "Average loss at step 115300: 356.414903\n",
      "Average loss at step 115400: 341.649453\n",
      "Average loss at step 115500: 343.763112\n",
      "Average loss at step 115600: 340.350175\n",
      "Average loss at step 115700: 342.589366\n",
      "Average loss at step 115800: 337.687400\n",
      "Average loss at step 115900: 329.459780\n",
      "Average loss at step 116000: 339.466928\n",
      "Average loss at step 116100: 341.398669\n",
      "Average loss at step 116200: 345.176476\n",
      "Average loss at step 116300: 345.675265\n",
      "Average loss at step 116400: 332.135048\n",
      "Average loss at step 116500: 334.201197\n",
      "Average loss at step 116600: 338.624635\n",
      "Average loss at step 116700: 339.059011\n",
      "Average loss at step 116800: 339.133367\n",
      "Average loss at step 116900: 328.911089\n",
      "Average loss at step 117000: 331.468797\n",
      "Average loss at step 117100: 330.779761\n",
      "Average loss at step 117200: 326.693697\n",
      "Average loss at step 117300: 335.188999\n",
      "Average loss at step 117400: 332.332263\n",
      "Average loss at step 117500: 327.887331\n",
      "Average loss at step 117600: 328.889498\n",
      "Average loss at step 117700: 324.060563\n",
      "Average loss at step 117800: 328.871586\n",
      "Average loss at step 117900: 331.921409\n",
      "Average loss at step 118000: 334.087453\n",
      "Average loss at step 118100: 335.106009\n",
      "Average loss at step 118200: 329.688583\n",
      "Average loss at step 118300: 329.302078\n",
      "Average loss at step 118400: 324.787510\n",
      "Average loss at step 118500: 334.165520\n",
      "Average loss at step 118600: 325.017368\n",
      "Average loss at step 118700: 330.320637\n",
      "Average loss at step 118800: 330.000798\n",
      "Average loss at step 118900: 324.064906\n",
      "Average loss at step 119000: 335.865227\n",
      "Average loss at step 119100: 331.942688\n",
      "Average loss at step 119200: 321.082657\n",
      "Average loss at step 119300: 330.110466\n",
      "Average loss at step 119400: 330.908943\n",
      "Average loss at step 119500: 338.601185\n",
      "Average loss at step 119600: 323.894182\n",
      "Average loss at step 119700: 329.276745\n",
      "Average loss at step 119800: 325.996677\n",
      "Average loss at step 119900: 328.603501\n",
      "Average loss at step 120000: 329.927012\n",
      "Graph 24: 25 nodes\n",
      "Average loss at step 120100: 352.408744\n",
      "Average loss at step 120200: 337.508814\n",
      "Average loss at step 120300: 335.136564\n",
      "Average loss at step 120400: 323.152437\n",
      "Average loss at step 120500: 328.032698\n",
      "Average loss at step 120600: 325.648709\n",
      "Average loss at step 120700: 326.709829\n",
      "Average loss at step 120800: 327.400728\n",
      "Average loss at step 120900: 324.116900\n",
      "Average loss at step 121000: 322.386573\n",
      "Average loss at step 121100: 323.236168\n",
      "Average loss at step 121200: 330.072718\n",
      "Average loss at step 121300: 323.929262\n",
      "Average loss at step 121400: 326.104304\n",
      "Average loss at step 121500: 328.048002\n",
      "Average loss at step 121600: 325.111647\n",
      "Average loss at step 121700: 321.851841\n",
      "Average loss at step 121800: 323.230998\n",
      "Average loss at step 121900: 329.160132\n",
      "Average loss at step 122000: 321.296644\n",
      "Average loss at step 122100: 322.035496\n",
      "Average loss at step 122200: 324.802478\n",
      "Average loss at step 122300: 325.044977\n",
      "Average loss at step 122400: 322.143763\n",
      "Average loss at step 122500: 324.820542\n",
      "Average loss at step 122600: 324.187193\n",
      "Average loss at step 122700: 321.834668\n",
      "Average loss at step 122800: 325.840357\n",
      "Average loss at step 122900: 323.485852\n",
      "Average loss at step 123000: 325.620717\n",
      "Average loss at step 123100: 322.128740\n",
      "Average loss at step 123200: 328.339399\n",
      "Average loss at step 123300: 321.862880\n",
      "Average loss at step 123400: 325.599275\n",
      "Average loss at step 123500: 324.549385\n",
      "Average loss at step 123600: 322.931257\n",
      "Average loss at step 123700: 325.674748\n",
      "Average loss at step 123800: 324.776014\n",
      "Average loss at step 123900: 320.950745\n",
      "Average loss at step 124000: 325.563948\n",
      "Average loss at step 124100: 319.852913\n",
      "Average loss at step 124200: 315.972884\n",
      "Average loss at step 124300: 325.015716\n",
      "Average loss at step 124400: 320.677658\n",
      "Average loss at step 124500: 323.380782\n",
      "Average loss at step 124600: 324.426934\n",
      "Average loss at step 124700: 322.026359\n",
      "Average loss at step 124800: 323.831884\n",
      "Average loss at step 124900: 321.021822\n",
      "Average loss at step 125000: 320.947717\n",
      "Graph 25: 16 nodes\n",
      "Average loss at step 125100: 363.900928\n",
      "Average loss at step 125200: 347.092534\n",
      "Average loss at step 125300: 347.957872\n",
      "Average loss at step 125400: 340.311594\n",
      "Average loss at step 125500: 339.805059\n",
      "Average loss at step 125600: 344.333552\n",
      "Average loss at step 125700: 340.724034\n",
      "Average loss at step 125800: 343.354480\n",
      "Average loss at step 125900: 344.102114\n",
      "Average loss at step 126000: 339.795983\n",
      "Average loss at step 126100: 339.836274\n",
      "Average loss at step 126200: 339.154584\n",
      "Average loss at step 126300: 337.984394\n",
      "Average loss at step 126400: 342.755098\n",
      "Average loss at step 126500: 342.134879\n",
      "Average loss at step 126600: 339.422075\n",
      "Average loss at step 126700: 336.788999\n",
      "Average loss at step 126800: 339.438128\n",
      "Average loss at step 126900: 338.021632\n",
      "Average loss at step 127000: 338.951185\n",
      "Average loss at step 127100: 336.981374\n",
      "Average loss at step 127200: 340.164287\n",
      "Average loss at step 127300: 338.238309\n",
      "Average loss at step 127400: 337.021510\n",
      "Average loss at step 127500: 338.255900\n",
      "Average loss at step 127600: 337.907590\n",
      "Average loss at step 127700: 337.999313\n",
      "Average loss at step 127800: 333.806876\n",
      "Average loss at step 127900: 341.318393\n",
      "Average loss at step 128000: 337.511405\n",
      "Average loss at step 128100: 334.795187\n",
      "Average loss at step 128200: 339.721319\n",
      "Average loss at step 128300: 335.416191\n",
      "Average loss at step 128400: 337.915916\n",
      "Average loss at step 128500: 335.306436\n",
      "Average loss at step 128600: 339.745145\n",
      "Average loss at step 128700: 336.770165\n",
      "Average loss at step 128800: 336.664699\n",
      "Average loss at step 128900: 338.175233\n",
      "Average loss at step 129000: 341.579218\n",
      "Average loss at step 129100: 338.381565\n",
      "Average loss at step 129200: 339.471843\n",
      "Average loss at step 129300: 337.070356\n",
      "Average loss at step 129400: 337.457169\n",
      "Average loss at step 129500: 337.173307\n",
      "Average loss at step 129600: 335.344493\n",
      "Average loss at step 129700: 336.346805\n",
      "Average loss at step 129800: 338.997432\n",
      "Average loss at step 129900: 338.083077\n",
      "Average loss at step 130000: 340.281047\n",
      "Graph 26: 59 nodes\n",
      "Average loss at step 130100: 347.881434\n",
      "Average loss at step 130200: 333.762368\n",
      "Average loss at step 130300: 330.877975\n",
      "Average loss at step 130400: 331.507121\n",
      "Average loss at step 130500: 329.447486\n",
      "Average loss at step 130600: 326.122894\n",
      "Average loss at step 130700: 327.642699\n",
      "Average loss at step 130800: 330.470505\n",
      "Average loss at step 130900: 328.777357\n",
      "Average loss at step 131000: 331.052953\n",
      "Average loss at step 131100: 327.996542\n",
      "Average loss at step 131200: 328.781113\n",
      "Average loss at step 131300: 328.924970\n",
      "Average loss at step 131400: 329.999545\n",
      "Average loss at step 131500: 329.240173\n",
      "Average loss at step 131600: 325.994227\n",
      "Average loss at step 131700: 329.169848\n",
      "Average loss at step 131800: 325.184111\n",
      "Average loss at step 131900: 329.947083\n",
      "Average loss at step 132000: 325.467689\n",
      "Average loss at step 132100: 326.782549\n",
      "Average loss at step 132200: 326.775999\n",
      "Average loss at step 132300: 328.791412\n",
      "Average loss at step 132400: 327.325142\n",
      "Average loss at step 132500: 328.088601\n",
      "Average loss at step 132600: 326.918434\n",
      "Average loss at step 132700: 327.348313\n",
      "Average loss at step 132800: 328.320503\n",
      "Average loss at step 132900: 330.972952\n",
      "Average loss at step 133000: 329.662039\n",
      "Average loss at step 133100: 331.327559\n",
      "Average loss at step 133200: 327.050893\n",
      "Average loss at step 133300: 325.800627\n",
      "Average loss at step 133400: 327.714121\n",
      "Average loss at step 133500: 329.712365\n",
      "Average loss at step 133600: 326.340660\n",
      "Average loss at step 133700: 327.786054\n",
      "Average loss at step 133800: 327.750813\n",
      "Average loss at step 133900: 326.917390\n",
      "Average loss at step 134000: 328.491481\n",
      "Average loss at step 134100: 330.870906\n",
      "Average loss at step 134200: 331.986918\n",
      "Average loss at step 134300: 328.671968\n",
      "Average loss at step 134400: 326.490949\n",
      "Average loss at step 134500: 330.161270\n",
      "Average loss at step 134600: 329.059067\n",
      "Average loss at step 134700: 326.988884\n",
      "Average loss at step 134800: 328.295663\n",
      "Average loss at step 134900: 329.316877\n",
      "Average loss at step 135000: 329.756063\n",
      "Graph 27: 34 nodes\n",
      "Average loss at step 135100: 338.798388\n",
      "Average loss at step 135200: 339.262121\n",
      "Average loss at step 135300: 331.686216\n",
      "Average loss at step 135400: 332.962013\n",
      "Average loss at step 135500: 333.528891\n",
      "Average loss at step 135600: 333.314758\n",
      "Average loss at step 135700: 329.547332\n",
      "Average loss at step 135800: 326.499104\n",
      "Average loss at step 135900: 330.088068\n",
      "Average loss at step 136000: 325.535856\n",
      "Average loss at step 136100: 329.451592\n",
      "Average loss at step 136200: 329.394199\n",
      "Average loss at step 136300: 324.853887\n",
      "Average loss at step 136400: 327.379063\n",
      "Average loss at step 136500: 331.380480\n",
      "Average loss at step 136600: 330.156190\n",
      "Average loss at step 136700: 332.385464\n",
      "Average loss at step 136800: 329.109628\n",
      "Average loss at step 136900: 326.684469\n",
      "Average loss at step 137000: 327.321239\n",
      "Average loss at step 137100: 327.162181\n",
      "Average loss at step 137200: 326.981659\n",
      "Average loss at step 137300: 329.123577\n",
      "Average loss at step 137400: 325.926711\n",
      "Average loss at step 137500: 326.717503\n",
      "Average loss at step 137600: 325.954782\n",
      "Average loss at step 137700: 331.242938\n",
      "Average loss at step 137800: 329.014727\n",
      "Average loss at step 137900: 326.822721\n",
      "Average loss at step 138000: 324.490362\n",
      "Average loss at step 138100: 328.296237\n",
      "Average loss at step 138200: 329.669836\n",
      "Average loss at step 138300: 326.565170\n",
      "Average loss at step 138400: 326.687296\n",
      "Average loss at step 138500: 328.802071\n",
      "Average loss at step 138600: 323.881426\n",
      "Average loss at step 138700: 327.226562\n",
      "Average loss at step 138800: 329.711344\n",
      "Average loss at step 138900: 325.899986\n",
      "Average loss at step 139000: 327.399596\n",
      "Average loss at step 139100: 327.052593\n",
      "Average loss at step 139200: 325.789801\n",
      "Average loss at step 139300: 328.719057\n",
      "Average loss at step 139400: 327.967417\n",
      "Average loss at step 139500: 324.455210\n",
      "Average loss at step 139600: 327.748161\n",
      "Average loss at step 139700: 327.728765\n",
      "Average loss at step 139800: 328.487298\n",
      "Average loss at step 139900: 331.172456\n",
      "Average loss at step 140000: 328.329268\n",
      "Graph 28: 18 nodes\n",
      "Average loss at step 140100: 364.331148\n",
      "Average loss at step 140200: 337.959873\n",
      "Average loss at step 140300: 336.110061\n",
      "Average loss at step 140400: 332.330088\n",
      "Average loss at step 140500: 334.356016\n",
      "Average loss at step 140600: 327.395247\n",
      "Average loss at step 140700: 331.128908\n",
      "Average loss at step 140800: 332.396407\n",
      "Average loss at step 140900: 331.719102\n",
      "Average loss at step 141000: 328.139519\n",
      "Average loss at step 141100: 330.595365\n",
      "Average loss at step 141200: 328.772549\n",
      "Average loss at step 141300: 332.639466\n",
      "Average loss at step 141400: 329.094664\n",
      "Average loss at step 141500: 332.308347\n",
      "Average loss at step 141600: 328.150207\n",
      "Average loss at step 141700: 328.624009\n",
      "Average loss at step 141800: 329.030294\n",
      "Average loss at step 141900: 329.659949\n",
      "Average loss at step 142000: 336.303716\n",
      "Average loss at step 142100: 328.774298\n",
      "Average loss at step 142200: 326.683102\n",
      "Average loss at step 142300: 328.816064\n",
      "Average loss at step 142400: 329.435549\n",
      "Average loss at step 142500: 330.511755\n",
      "Average loss at step 142600: 327.876477\n",
      "Average loss at step 142700: 330.611746\n",
      "Average loss at step 142800: 329.980314\n",
      "Average loss at step 142900: 326.669399\n",
      "Average loss at step 143000: 327.814361\n",
      "Average loss at step 143100: 331.611581\n",
      "Average loss at step 143200: 322.882179\n",
      "Average loss at step 143300: 330.352119\n",
      "Average loss at step 143400: 327.207431\n",
      "Average loss at step 143500: 329.076183\n",
      "Average loss at step 143600: 324.815924\n",
      "Average loss at step 143700: 328.074516\n",
      "Average loss at step 143800: 325.533445\n",
      "Average loss at step 143900: 330.501187\n",
      "Average loss at step 144000: 326.902582\n",
      "Average loss at step 144100: 326.631202\n",
      "Average loss at step 144200: 327.064412\n",
      "Average loss at step 144300: 330.945938\n",
      "Average loss at step 144400: 324.105952\n",
      "Average loss at step 144500: 328.886575\n",
      "Average loss at step 144600: 328.556303\n",
      "Average loss at step 144700: 327.035114\n",
      "Average loss at step 144800: 327.601634\n",
      "Average loss at step 144900: 324.960236\n",
      "Average loss at step 145000: 325.243505\n",
      "Graph 29: 55 nodes\n",
      "Average loss at step 145100: 347.873178\n",
      "Average loss at step 145200: 329.840083\n",
      "Average loss at step 145300: 331.054220\n",
      "Average loss at step 145400: 329.278464\n",
      "Average loss at step 145500: 331.036283\n",
      "Average loss at step 145600: 327.048903\n",
      "Average loss at step 145700: 333.873925\n",
      "Average loss at step 145800: 327.877759\n",
      "Average loss at step 145900: 329.479331\n",
      "Average loss at step 146000: 331.854373\n",
      "Average loss at step 146100: 326.030233\n",
      "Average loss at step 146200: 329.496782\n",
      "Average loss at step 146300: 330.751968\n",
      "Average loss at step 146400: 328.860995\n",
      "Average loss at step 146500: 330.475525\n",
      "Average loss at step 146600: 326.660220\n",
      "Average loss at step 146700: 329.517335\n",
      "Average loss at step 146800: 329.783669\n",
      "Average loss at step 146900: 328.782614\n",
      "Average loss at step 147000: 325.543544\n",
      "Average loss at step 147100: 330.148380\n",
      "Average loss at step 147200: 329.353689\n",
      "Average loss at step 147300: 329.095168\n",
      "Average loss at step 147400: 326.546410\n",
      "Average loss at step 147500: 325.663141\n",
      "Average loss at step 147600: 326.712351\n",
      "Average loss at step 147700: 328.212171\n",
      "Average loss at step 147800: 319.326887\n",
      "Average loss at step 147900: 329.312812\n",
      "Average loss at step 148000: 325.922705\n",
      "Average loss at step 148100: 322.252896\n",
      "Average loss at step 148200: 329.104423\n",
      "Average loss at step 148300: 328.031066\n",
      "Average loss at step 148400: 330.507070\n",
      "Average loss at step 148500: 326.782259\n",
      "Average loss at step 148600: 327.966791\n",
      "Average loss at step 148700: 327.512269\n",
      "Average loss at step 148800: 328.253493\n",
      "Average loss at step 148900: 332.457673\n",
      "Average loss at step 149000: 328.458857\n",
      "Average loss at step 149100: 328.637419\n",
      "Average loss at step 149200: 327.520217\n",
      "Average loss at step 149300: 329.684493\n",
      "Average loss at step 149400: 325.223885\n",
      "Average loss at step 149500: 322.938818\n",
      "Average loss at step 149600: 323.211722\n",
      "Average loss at step 149700: 324.698931\n",
      "Average loss at step 149800: 329.082478\n",
      "Average loss at step 149900: 326.147898\n",
      "Average loss at step 150000: 324.160706\n",
      "Graph 30: 31 nodes\n",
      "Average loss at step 150100: 336.349054\n",
      "Average loss at step 150200: 331.780490\n",
      "Average loss at step 150300: 329.250383\n",
      "Average loss at step 150400: 327.633169\n",
      "Average loss at step 150500: 329.203806\n",
      "Average loss at step 150600: 331.197974\n",
      "Average loss at step 150700: 330.741730\n",
      "Average loss at step 150800: 326.529766\n",
      "Average loss at step 150900: 327.777871\n",
      "Average loss at step 151000: 327.314869\n",
      "Average loss at step 151100: 326.614566\n",
      "Average loss at step 151200: 327.180001\n",
      "Average loss at step 151300: 325.644624\n",
      "Average loss at step 151400: 326.397645\n",
      "Average loss at step 151500: 323.413131\n",
      "Average loss at step 151600: 327.684295\n",
      "Average loss at step 151700: 326.484133\n",
      "Average loss at step 151800: 323.856447\n",
      "Average loss at step 151900: 323.703072\n",
      "Average loss at step 152000: 319.567598\n",
      "Average loss at step 152100: 323.036397\n",
      "Average loss at step 152200: 328.107814\n",
      "Average loss at step 152300: 325.270101\n",
      "Average loss at step 152400: 326.662804\n",
      "Average loss at step 152500: 323.296242\n",
      "Average loss at step 152600: 329.008227\n",
      "Average loss at step 152700: 323.845905\n",
      "Average loss at step 152800: 321.763240\n",
      "Average loss at step 152900: 322.960920\n",
      "Average loss at step 153000: 326.991621\n",
      "Average loss at step 153100: 327.370125\n",
      "Average loss at step 153200: 324.228101\n",
      "Average loss at step 153300: 326.400447\n",
      "Average loss at step 153400: 323.659492\n",
      "Average loss at step 153500: 325.550592\n",
      "Average loss at step 153600: 328.671079\n",
      "Average loss at step 153700: 327.598315\n",
      "Average loss at step 153800: 327.435111\n",
      "Average loss at step 153900: 322.205814\n",
      "Average loss at step 154000: 321.377213\n",
      "Average loss at step 154100: 326.250221\n",
      "Average loss at step 154200: 325.505497\n",
      "Average loss at step 154300: 323.362372\n",
      "Average loss at step 154400: 322.893060\n",
      "Average loss at step 154500: 324.396851\n",
      "Average loss at step 154600: 320.923689\n",
      "Average loss at step 154700: 321.291833\n",
      "Average loss at step 154800: 321.865526\n",
      "Average loss at step 154900: 321.977033\n",
      "Average loss at step 155000: 321.175078\n",
      "Time: 19.2369129658\n",
      "Graph 31: 32 nodes\n",
      "Average loss at step 155100: 342.091875\n",
      "Average loss at step 155200: 334.719475\n",
      "Average loss at step 155300: 330.462900\n",
      "Average loss at step 155400: 331.243492\n",
      "Average loss at step 155500: 329.459254\n",
      "Average loss at step 155600: 331.712499\n",
      "Average loss at step 155700: 330.821468\n",
      "Average loss at step 155800: 328.094127\n",
      "Average loss at step 155900: 325.768863\n",
      "Average loss at step 156000: 328.812032\n",
      "Average loss at step 156100: 331.140718\n",
      "Average loss at step 156200: 323.128565\n",
      "Average loss at step 156300: 324.214593\n",
      "Average loss at step 156400: 330.661181\n",
      "Average loss at step 156500: 327.144996\n",
      "Average loss at step 156600: 329.742817\n",
      "Average loss at step 156700: 328.225756\n",
      "Average loss at step 156800: 326.055653\n",
      "Average loss at step 156900: 323.586131\n",
      "Average loss at step 157000: 327.316057\n",
      "Average loss at step 157100: 326.804228\n",
      "Average loss at step 157200: 330.550452\n",
      "Average loss at step 157300: 325.679394\n",
      "Average loss at step 157400: 329.557904\n",
      "Average loss at step 157500: 326.670002\n",
      "Average loss at step 157600: 324.902819\n",
      "Average loss at step 157700: 325.761716\n",
      "Average loss at step 157800: 324.786423\n",
      "Average loss at step 157900: 325.832916\n",
      "Average loss at step 158000: 330.390339\n",
      "Average loss at step 158100: 327.945092\n",
      "Average loss at step 158200: 329.568702\n",
      "Average loss at step 158300: 325.533114\n",
      "Average loss at step 158400: 327.003787\n",
      "Average loss at step 158500: 328.139519\n",
      "Average loss at step 158600: 329.751693\n",
      "Average loss at step 158700: 329.179673\n",
      "Average loss at step 158800: 329.656295\n",
      "Average loss at step 158900: 326.881278\n",
      "Average loss at step 159000: 324.755607\n",
      "Average loss at step 159100: 324.974388\n",
      "Average loss at step 159200: 325.335208\n",
      "Average loss at step 159300: 331.163264\n",
      "Average loss at step 159400: 324.854777\n",
      "Average loss at step 159500: 328.949288\n",
      "Average loss at step 159600: 324.939002\n",
      "Average loss at step 159700: 326.816962\n",
      "Average loss at step 159800: 323.883825\n",
      "Average loss at step 159900: 326.308746\n",
      "Average loss at step 160000: 326.274437\n",
      "Graph 32: 41 nodes\n",
      "Average loss at step 160100: 336.469289\n",
      "Average loss at step 160200: 329.472031\n",
      "Average loss at step 160300: 333.834031\n",
      "Average loss at step 160400: 333.676015\n",
      "Average loss at step 160500: 330.538133\n",
      "Average loss at step 160600: 331.734934\n",
      "Average loss at step 160700: 327.506068\n",
      "Average loss at step 160800: 333.591453\n",
      "Average loss at step 160900: 328.648315\n",
      "Average loss at step 161000: 329.888294\n",
      "Average loss at step 161100: 332.633264\n",
      "Average loss at step 161200: 328.147136\n",
      "Average loss at step 161300: 330.308026\n",
      "Average loss at step 161400: 329.959267\n",
      "Average loss at step 161500: 329.292565\n",
      "Average loss at step 161600: 329.993478\n",
      "Average loss at step 161700: 328.021887\n",
      "Average loss at step 161800: 328.558773\n",
      "Average loss at step 161900: 329.260508\n",
      "Average loss at step 162000: 332.425147\n",
      "Average loss at step 162100: 328.230519\n",
      "Average loss at step 162200: 329.552645\n",
      "Average loss at step 162300: 333.262563\n",
      "Average loss at step 162400: 330.647815\n",
      "Average loss at step 162500: 328.172131\n",
      "Average loss at step 162600: 327.942639\n",
      "Average loss at step 162700: 326.524529\n",
      "Average loss at step 162800: 327.022853\n",
      "Average loss at step 162900: 331.936923\n",
      "Average loss at step 163000: 328.963088\n",
      "Average loss at step 163100: 333.876246\n",
      "Average loss at step 163200: 323.709200\n",
      "Average loss at step 163300: 329.733431\n",
      "Average loss at step 163400: 325.244397\n",
      "Average loss at step 163500: 330.265012\n",
      "Average loss at step 163600: 330.269091\n",
      "Average loss at step 163700: 327.515651\n",
      "Average loss at step 163800: 331.271549\n",
      "Average loss at step 163900: 328.094051\n",
      "Average loss at step 164000: 329.727394\n",
      "Average loss at step 164100: 331.109709\n",
      "Average loss at step 164200: 328.829344\n",
      "Average loss at step 164300: 328.667713\n",
      "Average loss at step 164400: 329.408910\n",
      "Average loss at step 164500: 331.021741\n",
      "Average loss at step 164600: 332.468225\n",
      "Average loss at step 164700: 330.238258\n",
      "Average loss at step 164800: 328.718683\n",
      "Average loss at step 164900: 328.770303\n",
      "Average loss at step 165000: 335.530957\n",
      "Graph 33: 40 nodes\n",
      "Average loss at step 165100: 353.713499\n",
      "Average loss at step 165200: 333.141268\n",
      "Average loss at step 165300: 331.159383\n",
      "Average loss at step 165400: 333.442737\n",
      "Average loss at step 165500: 334.158271\n",
      "Average loss at step 165600: 328.493102\n",
      "Average loss at step 165700: 332.427098\n",
      "Average loss at step 165800: 331.270211\n",
      "Average loss at step 165900: 332.610400\n",
      "Average loss at step 166000: 332.951332\n",
      "Average loss at step 166100: 331.833716\n",
      "Average loss at step 166200: 327.183897\n",
      "Average loss at step 166300: 331.230148\n",
      "Average loss at step 166400: 332.053217\n",
      "Average loss at step 166500: 329.300254\n",
      "Average loss at step 166600: 327.530352\n",
      "Average loss at step 166700: 329.980438\n",
      "Average loss at step 166800: 328.582887\n",
      "Average loss at step 166900: 330.491385\n",
      "Average loss at step 167000: 329.217247\n",
      "Average loss at step 167100: 330.871071\n",
      "Average loss at step 167200: 329.353766\n",
      "Average loss at step 167300: 331.293712\n",
      "Average loss at step 167400: 327.430534\n",
      "Average loss at step 167500: 331.014602\n",
      "Average loss at step 167600: 329.195723\n",
      "Average loss at step 167700: 328.877553\n",
      "Average loss at step 167800: 333.366122\n",
      "Average loss at step 167900: 331.870583\n",
      "Average loss at step 168000: 333.504141\n",
      "Average loss at step 168100: 333.715479\n",
      "Average loss at step 168200: 330.054843\n",
      "Average loss at step 168300: 327.128915\n",
      "Average loss at step 168400: 327.364582\n",
      "Average loss at step 168500: 328.012201\n",
      "Average loss at step 168600: 332.940065\n",
      "Average loss at step 168700: 331.251578\n",
      "Average loss at step 168800: 331.080851\n",
      "Average loss at step 168900: 332.351699\n",
      "Average loss at step 169000: 325.928940\n",
      "Average loss at step 169100: 329.008537\n",
      "Average loss at step 169200: 329.871039\n",
      "Average loss at step 169300: 329.692251\n",
      "Average loss at step 169400: 329.849958\n",
      "Average loss at step 169500: 332.299839\n",
      "Average loss at step 169600: 329.577058\n",
      "Average loss at step 169700: 329.202726\n",
      "Average loss at step 169800: 331.074674\n",
      "Average loss at step 169900: 329.994043\n",
      "Average loss at step 170000: 327.820786\n",
      "Graph 34: 29 nodes\n",
      "Average loss at step 170100: 344.211718\n",
      "Average loss at step 170200: 335.499121\n",
      "Average loss at step 170300: 332.660676\n",
      "Average loss at step 170400: 328.744609\n",
      "Average loss at step 170500: 332.473102\n",
      "Average loss at step 170600: 331.619878\n",
      "Average loss at step 170700: 334.367426\n",
      "Average loss at step 170800: 334.104172\n",
      "Average loss at step 170900: 329.302148\n",
      "Average loss at step 171000: 330.530219\n",
      "Average loss at step 171100: 330.174974\n",
      "Average loss at step 171200: 333.436711\n",
      "Average loss at step 171300: 333.378870\n",
      "Average loss at step 171400: 331.990432\n",
      "Average loss at step 171500: 332.582048\n",
      "Average loss at step 171600: 333.052324\n",
      "Average loss at step 171700: 329.751649\n",
      "Average loss at step 171800: 329.592422\n",
      "Average loss at step 171900: 328.090752\n",
      "Average loss at step 172000: 326.539416\n",
      "Average loss at step 172100: 329.726859\n",
      "Average loss at step 172200: 331.147568\n",
      "Average loss at step 172300: 328.376786\n",
      "Average loss at step 172400: 332.244625\n",
      "Average loss at step 172500: 331.786907\n",
      "Average loss at step 172600: 331.107935\n",
      "Average loss at step 172700: 330.585965\n",
      "Average loss at step 172800: 331.756649\n",
      "Average loss at step 172900: 327.149833\n",
      "Average loss at step 173000: 331.980267\n",
      "Average loss at step 173100: 329.593969\n",
      "Average loss at step 173200: 328.502206\n",
      "Average loss at step 173300: 329.716317\n",
      "Average loss at step 173400: 323.332074\n",
      "Average loss at step 173500: 328.034081\n",
      "Average loss at step 173600: 328.804085\n",
      "Average loss at step 173700: 329.458414\n",
      "Average loss at step 173800: 332.678834\n",
      "Average loss at step 173900: 333.802306\n",
      "Average loss at step 174000: 331.680961\n",
      "Average loss at step 174100: 329.440462\n",
      "Average loss at step 174200: 332.685157\n",
      "Average loss at step 174300: 329.396572\n",
      "Average loss at step 174400: 328.394312\n",
      "Average loss at step 174500: 328.521180\n",
      "Average loss at step 174600: 332.807438\n",
      "Average loss at step 174700: 333.922211\n",
      "Average loss at step 174800: 331.167152\n",
      "Average loss at step 174900: 329.793836\n",
      "Average loss at step 175000: 329.000095\n",
      "Graph 35: 22 nodes\n",
      "Average loss at step 175100: 359.172431\n",
      "Average loss at step 175200: 339.749787\n",
      "Average loss at step 175300: 337.652452\n",
      "Average loss at step 175400: 337.887933\n",
      "Average loss at step 175500: 338.099350\n",
      "Average loss at step 175600: 338.190249\n",
      "Average loss at step 175700: 337.148438\n",
      "Average loss at step 175800: 337.024235\n",
      "Average loss at step 175900: 338.614355\n",
      "Average loss at step 176000: 336.905627\n",
      "Average loss at step 176100: 335.932115\n",
      "Average loss at step 176200: 335.602218\n",
      "Average loss at step 176300: 338.617718\n",
      "Average loss at step 176400: 336.768792\n",
      "Average loss at step 176500: 335.708655\n",
      "Average loss at step 176600: 333.021382\n",
      "Average loss at step 176700: 337.988382\n",
      "Average loss at step 176800: 336.107755\n",
      "Average loss at step 176900: 332.126484\n",
      "Average loss at step 177000: 335.849365\n",
      "Average loss at step 177100: 340.451754\n",
      "Average loss at step 177200: 336.754639\n",
      "Average loss at step 177300: 338.398646\n",
      "Average loss at step 177400: 330.845952\n",
      "Average loss at step 177500: 335.975260\n",
      "Average loss at step 177600: 333.857270\n",
      "Average loss at step 177700: 335.068715\n",
      "Average loss at step 177800: 335.305420\n",
      "Average loss at step 177900: 339.073586\n",
      "Average loss at step 178000: 334.760655\n",
      "Average loss at step 178100: 334.808258\n",
      "Average loss at step 178200: 339.581902\n",
      "Average loss at step 178300: 335.628584\n",
      "Average loss at step 178400: 333.873528\n",
      "Average loss at step 178500: 334.316394\n",
      "Average loss at step 178600: 336.083886\n",
      "Average loss at step 178700: 335.787890\n",
      "Average loss at step 178800: 334.796988\n",
      "Average loss at step 178900: 334.028851\n",
      "Average loss at step 179000: 336.102532\n",
      "Average loss at step 179100: 334.753012\n",
      "Average loss at step 179200: 335.153292\n",
      "Average loss at step 179300: 333.902955\n",
      "Average loss at step 179400: 335.355515\n",
      "Average loss at step 179500: 334.527186\n",
      "Average loss at step 179600: 333.877680\n",
      "Average loss at step 179700: 336.406076\n",
      "Average loss at step 179800: 339.659012\n",
      "Average loss at step 179900: 340.588341\n",
      "Average loss at step 180000: 336.813752\n",
      "Graph 36: 26 nodes\n",
      "Average loss at step 180100: 340.953271\n",
      "Average loss at step 180200: 328.901969\n",
      "Average loss at step 180300: 327.096112\n",
      "Average loss at step 180400: 323.018070\n",
      "Average loss at step 180500: 324.084544\n",
      "Average loss at step 180600: 328.945240\n",
      "Average loss at step 180700: 322.379086\n",
      "Average loss at step 180800: 325.124951\n",
      "Average loss at step 180900: 326.125414\n",
      "Average loss at step 181000: 325.637977\n",
      "Average loss at step 181100: 323.094045\n",
      "Average loss at step 181200: 319.832152\n",
      "Average loss at step 181300: 326.009042\n",
      "Average loss at step 181400: 328.576529\n",
      "Average loss at step 181500: 324.736896\n",
      "Average loss at step 181600: 320.377441\n",
      "Average loss at step 181700: 323.342719\n",
      "Average loss at step 181800: 325.990006\n",
      "Average loss at step 181900: 323.778162\n",
      "Average loss at step 182000: 322.941841\n",
      "Average loss at step 182100: 323.843934\n",
      "Average loss at step 182200: 325.587935\n",
      "Average loss at step 182300: 322.265973\n",
      "Average loss at step 182400: 326.332925\n",
      "Average loss at step 182500: 324.331356\n",
      "Average loss at step 182600: 323.150612\n",
      "Average loss at step 182700: 322.416962\n",
      "Average loss at step 182800: 323.241740\n",
      "Average loss at step 182900: 323.964567\n",
      "Average loss at step 183000: 325.786476\n",
      "Average loss at step 183100: 326.225889\n",
      "Average loss at step 183200: 325.287591\n",
      "Average loss at step 183300: 322.574793\n",
      "Average loss at step 183400: 321.089833\n",
      "Average loss at step 183500: 325.106621\n",
      "Average loss at step 183600: 325.195826\n",
      "Average loss at step 183700: 321.489748\n",
      "Average loss at step 183800: 325.909879\n",
      "Average loss at step 183900: 323.952580\n",
      "Average loss at step 184000: 322.112973\n",
      "Average loss at step 184100: 325.877057\n",
      "Average loss at step 184200: 320.877091\n",
      "Average loss at step 184300: 327.659201\n",
      "Average loss at step 184400: 325.598456\n",
      "Average loss at step 184500: 322.762901\n",
      "Average loss at step 184600: 327.005679\n",
      "Average loss at step 184700: 325.292669\n",
      "Average loss at step 184800: 324.253033\n",
      "Average loss at step 184900: 318.784276\n",
      "Average loss at step 185000: 325.847142\n",
      "Graph 37: 29 nodes\n",
      "Average loss at step 185100: 367.788651\n",
      "Average loss at step 185200: 341.923377\n",
      "Average loss at step 185300: 344.122204\n",
      "Average loss at step 185400: 341.287576\n",
      "Average loss at step 185500: 336.151047\n",
      "Average loss at step 185600: 336.007310\n",
      "Average loss at step 185700: 340.602949\n",
      "Average loss at step 185800: 339.057309\n",
      "Average loss at step 185900: 339.830000\n",
      "Average loss at step 186000: 334.746890\n",
      "Average loss at step 186100: 338.695671\n",
      "Average loss at step 186200: 337.653599\n",
      "Average loss at step 186300: 338.134380\n",
      "Average loss at step 186400: 335.373526\n",
      "Average loss at step 186500: 332.080766\n",
      "Average loss at step 186600: 333.445831\n",
      "Average loss at step 186700: 338.601463\n",
      "Average loss at step 186800: 335.069077\n",
      "Average loss at step 186900: 331.641103\n",
      "Average loss at step 187000: 336.972078\n",
      "Average loss at step 187100: 336.028327\n",
      "Average loss at step 187200: 337.369395\n",
      "Average loss at step 187300: 334.964005\n",
      "Average loss at step 187400: 340.876417\n",
      "Average loss at step 187500: 339.199554\n",
      "Average loss at step 187600: 337.173750\n",
      "Average loss at step 187700: 334.808320\n",
      "Average loss at step 187800: 334.086015\n",
      "Average loss at step 187900: 334.852543\n",
      "Average loss at step 188000: 342.505304\n",
      "Average loss at step 188100: 331.077427\n",
      "Average loss at step 188200: 332.764713\n",
      "Average loss at step 188300: 339.016563\n",
      "Average loss at step 188400: 335.401744\n",
      "Average loss at step 188500: 337.566677\n",
      "Average loss at step 188600: 339.455848\n",
      "Average loss at step 188700: 339.391881\n",
      "Average loss at step 188800: 334.155889\n",
      "Average loss at step 188900: 336.687036\n",
      "Average loss at step 189000: 334.717042\n",
      "Average loss at step 189100: 336.741066\n",
      "Average loss at step 189200: 329.836992\n",
      "Average loss at step 189300: 334.981367\n",
      "Average loss at step 189400: 336.383584\n",
      "Average loss at step 189500: 338.845599\n",
      "Average loss at step 189600: 335.182985\n",
      "Average loss at step 189700: 336.311827\n",
      "Average loss at step 189800: 334.071349\n",
      "Average loss at step 189900: 339.457011\n",
      "Average loss at step 190000: 332.523438\n",
      "Graph 38: 34 nodes\n",
      "Average loss at step 190100: 358.112423\n",
      "Average loss at step 190200: 319.132469\n",
      "Average loss at step 190300: 322.702919\n",
      "Average loss at step 190400: 325.064384\n",
      "Average loss at step 190500: 320.826054\n",
      "Average loss at step 190600: 319.710392\n",
      "Average loss at step 190700: 322.117388\n",
      "Average loss at step 190800: 322.056906\n",
      "Average loss at step 190900: 318.680361\n",
      "Average loss at step 191000: 321.714590\n",
      "Average loss at step 191100: 325.237784\n",
      "Average loss at step 191200: 322.113630\n",
      "Average loss at step 191300: 322.135909\n",
      "Average loss at step 191400: 318.059861\n",
      "Average loss at step 191500: 321.743294\n",
      "Average loss at step 191600: 319.937263\n",
      "Average loss at step 191700: 319.060382\n",
      "Average loss at step 191800: 319.729254\n",
      "Average loss at step 191900: 321.616135\n",
      "Average loss at step 192000: 320.404105\n",
      "Average loss at step 192100: 319.475597\n",
      "Average loss at step 192200: 320.776127\n",
      "Average loss at step 192300: 319.910137\n",
      "Average loss at step 192400: 318.922168\n",
      "Average loss at step 192500: 318.020862\n",
      "Average loss at step 192600: 319.764034\n",
      "Average loss at step 192700: 319.740640\n",
      "Average loss at step 192800: 319.387973\n",
      "Average loss at step 192900: 320.074459\n",
      "Average loss at step 193000: 320.278196\n",
      "Average loss at step 193100: 318.281484\n",
      "Average loss at step 193200: 321.385808\n",
      "Average loss at step 193300: 319.851557\n",
      "Average loss at step 193400: 321.508275\n",
      "Average loss at step 193500: 324.636561\n",
      "Average loss at step 193600: 320.167764\n",
      "Average loss at step 193700: 320.877184\n",
      "Average loss at step 193800: 318.284710\n",
      "Average loss at step 193900: 323.036964\n",
      "Average loss at step 194000: 317.832165\n",
      "Average loss at step 194100: 316.130041\n",
      "Average loss at step 194200: 318.585638\n",
      "Average loss at step 194300: 321.069539\n",
      "Average loss at step 194400: 319.107343\n",
      "Average loss at step 194500: 321.217297\n",
      "Average loss at step 194600: 319.962941\n",
      "Average loss at step 194700: 319.111669\n",
      "Average loss at step 194800: 320.829038\n",
      "Average loss at step 194900: 318.485152\n",
      "Average loss at step 195000: 318.579078\n",
      "Graph 39: 23 nodes\n",
      "Average loss at step 195100: 358.395719\n",
      "Average loss at step 195200: 329.794904\n",
      "Average loss at step 195300: 329.228965\n",
      "Average loss at step 195400: 328.376126\n",
      "Average loss at step 195500: 331.189250\n",
      "Average loss at step 195600: 329.067647\n",
      "Average loss at step 195700: 329.220430\n",
      "Average loss at step 195800: 332.921754\n",
      "Average loss at step 195900: 324.752390\n",
      "Average loss at step 196000: 330.083220\n",
      "Average loss at step 196100: 326.968744\n",
      "Average loss at step 196200: 327.738499\n",
      "Average loss at step 196300: 330.107703\n",
      "Average loss at step 196400: 327.679377\n",
      "Average loss at step 196500: 326.179413\n",
      "Average loss at step 196600: 323.445029\n",
      "Average loss at step 196700: 329.552078\n",
      "Average loss at step 196800: 331.030068\n",
      "Average loss at step 196900: 327.294512\n",
      "Average loss at step 197000: 328.641728\n",
      "Average loss at step 197100: 330.788105\n",
      "Average loss at step 197200: 330.841163\n",
      "Average loss at step 197300: 328.511923\n",
      "Average loss at step 197400: 329.226324\n",
      "Average loss at step 197500: 329.927836\n",
      "Average loss at step 197600: 328.721084\n",
      "Average loss at step 197700: 329.245523\n",
      "Average loss at step 197800: 324.045633\n",
      "Average loss at step 197900: 328.160352\n",
      "Average loss at step 198000: 329.046460\n",
      "Average loss at step 198100: 329.003384\n",
      "Average loss at step 198200: 328.293158\n",
      "Average loss at step 198300: 327.939487\n",
      "Average loss at step 198400: 329.626359\n",
      "Average loss at step 198500: 327.826069\n",
      "Average loss at step 198600: 328.196969\n",
      "Average loss at step 198700: 325.854342\n",
      "Average loss at step 198800: 327.753313\n",
      "Average loss at step 198900: 325.152185\n",
      "Average loss at step 199000: 327.122703\n",
      "Average loss at step 199100: 329.755574\n",
      "Average loss at step 199200: 326.555227\n",
      "Average loss at step 199300: 327.464875\n",
      "Average loss at step 199400: 328.697314\n",
      "Average loss at step 199500: 329.113640\n",
      "Average loss at step 199600: 330.537375\n",
      "Average loss at step 199700: 326.372661\n",
      "Average loss at step 199800: 326.904922\n",
      "Average loss at step 199900: 327.393669\n",
      "Average loss at step 200000: 324.213388\n",
      "Graph 40: 28 nodes\n",
      "Average loss at step 200100: 342.883567\n",
      "Average loss at step 200200: 330.752994\n",
      "Average loss at step 200300: 328.376652\n",
      "Average loss at step 200400: 329.923722\n",
      "Average loss at step 200500: 331.472342\n",
      "Average loss at step 200600: 329.914021\n",
      "Average loss at step 200700: 327.830316\n",
      "Average loss at step 200800: 323.577600\n",
      "Average loss at step 200900: 324.436635\n",
      "Average loss at step 201000: 327.541925\n",
      "Average loss at step 201100: 325.583195\n",
      "Average loss at step 201200: 328.122355\n",
      "Average loss at step 201300: 327.440994\n",
      "Average loss at step 201400: 320.842722\n",
      "Average loss at step 201500: 329.571333\n",
      "Average loss at step 201600: 324.699337\n",
      "Average loss at step 201700: 326.995965\n",
      "Average loss at step 201800: 323.500879\n",
      "Average loss at step 201900: 326.691544\n",
      "Average loss at step 202000: 324.560143\n",
      "Average loss at step 202100: 326.598028\n",
      "Average loss at step 202200: 325.899445\n",
      "Average loss at step 202300: 328.300053\n",
      "Average loss at step 202400: 325.102499\n",
      "Average loss at step 202500: 327.081625\n",
      "Average loss at step 202600: 326.269051\n",
      "Average loss at step 202700: 317.469875\n",
      "Average loss at step 202800: 325.728122\n",
      "Average loss at step 202900: 325.632336\n",
      "Average loss at step 203000: 323.984525\n",
      "Average loss at step 203100: 322.306435\n",
      "Average loss at step 203200: 324.194180\n",
      "Average loss at step 203300: 325.318214\n",
      "Average loss at step 203400: 324.561680\n",
      "Average loss at step 203500: 324.005928\n",
      "Average loss at step 203600: 326.039830\n",
      "Average loss at step 203700: 325.472695\n",
      "Average loss at step 203800: 324.797604\n",
      "Average loss at step 203900: 325.365424\n",
      "Average loss at step 204000: 323.658969\n",
      "Average loss at step 204100: 325.717768\n",
      "Average loss at step 204200: 326.231770\n",
      "Average loss at step 204300: 324.928825\n",
      "Average loss at step 204400: 324.765534\n",
      "Average loss at step 204500: 328.522739\n",
      "Average loss at step 204600: 324.272785\n",
      "Average loss at step 204700: 326.991530\n",
      "Average loss at step 204800: 321.011522\n",
      "Average loss at step 204900: 330.600845\n",
      "Average loss at step 205000: 326.185094\n",
      "Time: 19.2744011879\n",
      "Graph 41: 24 nodes\n",
      "Average loss at step 205100: 343.327695\n",
      "Average loss at step 205200: 333.732938\n",
      "Average loss at step 205300: 337.621435\n",
      "Average loss at step 205400: 331.325063\n",
      "Average loss at step 205500: 336.523510\n",
      "Average loss at step 205600: 336.052771\n",
      "Average loss at step 205700: 332.418839\n",
      "Average loss at step 205800: 332.119574\n",
      "Average loss at step 205900: 330.570644\n",
      "Average loss at step 206000: 332.724489\n",
      "Average loss at step 206100: 331.103092\n",
      "Average loss at step 206200: 331.271743\n",
      "Average loss at step 206300: 332.303206\n",
      "Average loss at step 206400: 331.326396\n",
      "Average loss at step 206500: 332.694980\n",
      "Average loss at step 206600: 331.925206\n",
      "Average loss at step 206700: 329.381382\n",
      "Average loss at step 206800: 330.585961\n",
      "Average loss at step 206900: 330.652786\n",
      "Average loss at step 207000: 330.026062\n",
      "Average loss at step 207100: 327.216381\n",
      "Average loss at step 207200: 333.509087\n",
      "Average loss at step 207300: 327.773774\n",
      "Average loss at step 207400: 331.448654\n",
      "Average loss at step 207500: 334.018922\n",
      "Average loss at step 207600: 330.096772\n",
      "Average loss at step 207700: 334.150836\n",
      "Average loss at step 207800: 329.062388\n",
      "Average loss at step 207900: 333.292433\n",
      "Average loss at step 208000: 330.816724\n",
      "Average loss at step 208100: 331.574654\n",
      "Average loss at step 208200: 332.075811\n",
      "Average loss at step 208300: 329.103736\n",
      "Average loss at step 208400: 328.479498\n",
      "Average loss at step 208500: 332.535714\n",
      "Average loss at step 208600: 329.934518\n",
      "Average loss at step 208700: 332.277505\n",
      "Average loss at step 208800: 331.548094\n",
      "Average loss at step 208900: 335.544127\n",
      "Average loss at step 209000: 330.152543\n",
      "Average loss at step 209100: 328.822949\n",
      "Average loss at step 209200: 330.629369\n",
      "Average loss at step 209300: 330.393687\n",
      "Average loss at step 209400: 331.467432\n",
      "Average loss at step 209500: 333.272188\n",
      "Average loss at step 209600: 332.351385\n",
      "Average loss at step 209700: 330.377531\n",
      "Average loss at step 209800: 330.438583\n",
      "Average loss at step 209900: 330.871951\n",
      "Average loss at step 210000: 331.029732\n",
      "Graph 42: 26 nodes\n",
      "Average loss at step 210100: 337.751222\n",
      "Average loss at step 210200: 336.030076\n",
      "Average loss at step 210300: 331.565821\n",
      "Average loss at step 210400: 329.363131\n",
      "Average loss at step 210500: 331.044314\n",
      "Average loss at step 210600: 327.939448\n",
      "Average loss at step 210700: 325.326961\n",
      "Average loss at step 210800: 327.388856\n",
      "Average loss at step 210900: 331.364859\n",
      "Average loss at step 211000: 331.693140\n",
      "Average loss at step 211100: 328.821839\n",
      "Average loss at step 211200: 330.417976\n",
      "Average loss at step 211300: 329.013250\n",
      "Average loss at step 211400: 326.934481\n",
      "Average loss at step 211500: 330.530788\n",
      "Average loss at step 211600: 329.139718\n",
      "Average loss at step 211700: 332.015772\n",
      "Average loss at step 211800: 330.218968\n",
      "Average loss at step 211900: 329.133503\n",
      "Average loss at step 212000: 330.030827\n",
      "Average loss at step 212100: 330.274912\n",
      "Average loss at step 212200: 328.301973\n",
      "Average loss at step 212300: 329.877289\n",
      "Average loss at step 212400: 327.919137\n",
      "Average loss at step 212500: 330.577909\n",
      "Average loss at step 212600: 328.997214\n",
      "Average loss at step 212700: 331.723087\n",
      "Average loss at step 212800: 327.618622\n",
      "Average loss at step 212900: 328.095504\n",
      "Average loss at step 213000: 330.479057\n",
      "Average loss at step 213100: 330.698695\n",
      "Average loss at step 213200: 329.605163\n",
      "Average loss at step 213300: 333.007238\n",
      "Average loss at step 213400: 329.518151\n",
      "Average loss at step 213500: 330.035296\n",
      "Average loss at step 213600: 328.437051\n",
      "Average loss at step 213700: 330.060374\n",
      "Average loss at step 213800: 330.560750\n",
      "Average loss at step 213900: 330.090621\n",
      "Average loss at step 214000: 330.793579\n",
      "Average loss at step 214100: 331.707795\n",
      "Average loss at step 214200: 330.817209\n",
      "Average loss at step 214300: 333.421889\n",
      "Average loss at step 214400: 327.999397\n",
      "Average loss at step 214500: 328.215687\n",
      "Average loss at step 214600: 329.605978\n",
      "Average loss at step 214700: 329.073364\n",
      "Average loss at step 214800: 326.049751\n",
      "Average loss at step 214900: 325.170078\n",
      "Average loss at step 215000: 326.573198\n",
      "Graph 43: 18 nodes\n",
      "Average loss at step 215100: 341.888312\n",
      "Average loss at step 215200: 337.390726\n",
      "Average loss at step 215300: 338.580184\n",
      "Average loss at step 215400: 341.598671\n",
      "Average loss at step 215500: 331.513459\n",
      "Average loss at step 215600: 333.945721\n",
      "Average loss at step 215700: 333.143620\n",
      "Average loss at step 215800: 336.258968\n",
      "Average loss at step 215900: 336.488725\n",
      "Average loss at step 216000: 334.102885\n",
      "Average loss at step 216100: 331.821374\n",
      "Average loss at step 216200: 335.425296\n",
      "Average loss at step 216300: 337.560567\n",
      "Average loss at step 216400: 335.431814\n",
      "Average loss at step 216500: 338.238192\n",
      "Average loss at step 216600: 334.929734\n",
      "Average loss at step 216700: 333.585707\n",
      "Average loss at step 216800: 333.286600\n",
      "Average loss at step 216900: 334.883825\n",
      "Average loss at step 217000: 333.968563\n",
      "Average loss at step 217100: 333.898611\n",
      "Average loss at step 217200: 334.353391\n",
      "Average loss at step 217300: 334.430302\n",
      "Average loss at step 217400: 331.954334\n",
      "Average loss at step 217500: 336.063204\n",
      "Average loss at step 217600: 331.302514\n",
      "Average loss at step 217700: 334.187027\n",
      "Average loss at step 217800: 334.839382\n",
      "Average loss at step 217900: 334.806238\n",
      "Average loss at step 218000: 332.909219\n",
      "Average loss at step 218100: 334.669835\n",
      "Average loss at step 218200: 334.145817\n",
      "Average loss at step 218300: 331.972278\n",
      "Average loss at step 218400: 329.708920\n",
      "Average loss at step 218500: 331.871193\n",
      "Average loss at step 218600: 334.705760\n",
      "Average loss at step 218700: 333.219388\n",
      "Average loss at step 218800: 331.899393\n",
      "Average loss at step 218900: 334.194737\n",
      "Average loss at step 219000: 334.078299\n",
      "Average loss at step 219100: 334.414626\n",
      "Average loss at step 219200: 336.130055\n",
      "Average loss at step 219300: 335.306672\n",
      "Average loss at step 219400: 335.158657\n",
      "Average loss at step 219500: 334.458572\n",
      "Average loss at step 219600: 330.889169\n",
      "Average loss at step 219700: 333.908680\n",
      "Average loss at step 219800: 334.088721\n",
      "Average loss at step 219900: 336.026702\n",
      "Average loss at step 220000: 336.818127\n",
      "Graph 44: 21 nodes\n",
      "Average loss at step 220100: 344.198797\n",
      "Average loss at step 220200: 337.005276\n",
      "Average loss at step 220300: 333.466990\n",
      "Average loss at step 220400: 329.335664\n",
      "Average loss at step 220500: 335.350001\n",
      "Average loss at step 220600: 333.636389\n",
      "Average loss at step 220700: 333.040276\n",
      "Average loss at step 220800: 335.481869\n",
      "Average loss at step 220900: 330.983001\n",
      "Average loss at step 221000: 330.329963\n",
      "Average loss at step 221100: 330.030229\n",
      "Average loss at step 221200: 329.814183\n",
      "Average loss at step 221300: 331.175453\n",
      "Average loss at step 221400: 327.854836\n",
      "Average loss at step 221500: 330.211909\n",
      "Average loss at step 221600: 328.978985\n",
      "Average loss at step 221700: 332.543372\n",
      "Average loss at step 221800: 330.474359\n",
      "Average loss at step 221900: 325.950596\n",
      "Average loss at step 222000: 327.560599\n",
      "Average loss at step 222100: 334.488291\n",
      "Average loss at step 222200: 330.861536\n",
      "Average loss at step 222300: 331.198535\n",
      "Average loss at step 222400: 330.617173\n",
      "Average loss at step 222500: 325.408218\n",
      "Average loss at step 222600: 333.820396\n",
      "Average loss at step 222700: 332.271610\n",
      "Average loss at step 222800: 329.542348\n",
      "Average loss at step 222900: 331.395114\n",
      "Average loss at step 223000: 331.509093\n",
      "Average loss at step 223100: 330.248307\n",
      "Average loss at step 223200: 329.823613\n",
      "Average loss at step 223300: 333.307767\n",
      "Average loss at step 223400: 329.404030\n",
      "Average loss at step 223500: 328.566692\n",
      "Average loss at step 223600: 331.791217\n",
      "Average loss at step 223700: 330.737509\n",
      "Average loss at step 223800: 332.377063\n",
      "Average loss at step 223900: 331.422889\n",
      "Average loss at step 224000: 330.277759\n",
      "Average loss at step 224100: 327.951799\n",
      "Average loss at step 224200: 331.438586\n",
      "Average loss at step 224300: 329.359357\n",
      "Average loss at step 224400: 333.741880\n",
      "Average loss at step 224500: 329.109446\n",
      "Average loss at step 224600: 328.430929\n",
      "Average loss at step 224700: 330.296606\n",
      "Average loss at step 224800: 331.374299\n",
      "Average loss at step 224900: 330.955961\n",
      "Average loss at step 225000: 329.052643\n",
      "Graph 45: 28 nodes\n",
      "Average loss at step 225100: 342.140310\n",
      "Average loss at step 225200: 337.499037\n",
      "Average loss at step 225300: 331.895426\n",
      "Average loss at step 225400: 333.978080\n",
      "Average loss at step 225500: 334.664871\n",
      "Average loss at step 225600: 329.010341\n",
      "Average loss at step 225700: 334.213794\n",
      "Average loss at step 225800: 331.234646\n",
      "Average loss at step 225900: 335.519181\n",
      "Average loss at step 226000: 331.670328\n",
      "Average loss at step 226100: 336.492727\n",
      "Average loss at step 226200: 330.508617\n",
      "Average loss at step 226300: 330.980392\n",
      "Average loss at step 226400: 330.488935\n",
      "Average loss at step 226500: 333.992575\n",
      "Average loss at step 226600: 330.301351\n",
      "Average loss at step 226700: 334.257173\n",
      "Average loss at step 226800: 330.610305\n",
      "Average loss at step 226900: 329.367269\n",
      "Average loss at step 227000: 331.410338\n",
      "Average loss at step 227100: 331.657251\n",
      "Average loss at step 227200: 330.041106\n",
      "Average loss at step 227300: 329.053130\n",
      "Average loss at step 227400: 333.507327\n",
      "Average loss at step 227500: 334.037174\n",
      "Average loss at step 227600: 330.478447\n",
      "Average loss at step 227700: 330.982776\n",
      "Average loss at step 227800: 331.306990\n",
      "Average loss at step 227900: 333.784218\n",
      "Average loss at step 228000: 335.125020\n",
      "Average loss at step 228100: 332.388785\n",
      "Average loss at step 228200: 330.833645\n",
      "Average loss at step 228300: 333.100280\n",
      "Average loss at step 228400: 330.175335\n",
      "Average loss at step 228500: 330.901063\n",
      "Average loss at step 228600: 331.477262\n",
      "Average loss at step 228700: 332.265212\n",
      "Average loss at step 228800: 334.185794\n",
      "Average loss at step 228900: 335.143477\n",
      "Average loss at step 229000: 332.592664\n",
      "Average loss at step 229100: 331.927276\n",
      "Average loss at step 229200: 328.510503\n",
      "Average loss at step 229300: 325.633040\n",
      "Average loss at step 229400: 330.924351\n",
      "Average loss at step 229500: 335.761375\n",
      "Average loss at step 229600: 329.844465\n",
      "Average loss at step 229700: 331.794906\n",
      "Average loss at step 229800: 331.446209\n",
      "Average loss at step 229900: 333.422921\n",
      "Average loss at step 230000: 329.405892\n",
      "Graph 46: 35 nodes\n",
      "Average loss at step 230100: 341.290897\n",
      "Average loss at step 230200: 337.013676\n",
      "Average loss at step 230300: 334.436162\n",
      "Average loss at step 230400: 337.540900\n",
      "Average loss at step 230500: 334.267164\n",
      "Average loss at step 230600: 332.077188\n",
      "Average loss at step 230700: 330.900910\n",
      "Average loss at step 230800: 333.127704\n",
      "Average loss at step 230900: 330.334943\n",
      "Average loss at step 231000: 337.134947\n",
      "Average loss at step 231100: 335.010802\n",
      "Average loss at step 231200: 331.170301\n",
      "Average loss at step 231300: 334.189340\n",
      "Average loss at step 231400: 329.471255\n",
      "Average loss at step 231500: 333.597536\n",
      "Average loss at step 231600: 333.426711\n",
      "Average loss at step 231700: 331.835588\n",
      "Average loss at step 231800: 333.143088\n",
      "Average loss at step 231900: 332.720012\n",
      "Average loss at step 232000: 332.098059\n",
      "Average loss at step 232100: 334.573927\n",
      "Average loss at step 232200: 332.078291\n",
      "Average loss at step 232300: 331.560406\n",
      "Average loss at step 232400: 332.076983\n",
      "Average loss at step 232500: 330.433826\n",
      "Average loss at step 232600: 333.242127\n",
      "Average loss at step 232700: 333.969262\n",
      "Average loss at step 232800: 331.467216\n",
      "Average loss at step 232900: 333.587103\n",
      "Average loss at step 233000: 326.181076\n",
      "Average loss at step 233100: 330.461124\n",
      "Average loss at step 233200: 332.500058\n",
      "Average loss at step 233300: 330.569827\n",
      "Average loss at step 233400: 332.666982\n",
      "Average loss at step 233500: 334.009024\n",
      "Average loss at step 233600: 333.336670\n",
      "Average loss at step 233700: 330.697738\n",
      "Average loss at step 233800: 330.431685\n",
      "Average loss at step 233900: 331.611371\n",
      "Average loss at step 234000: 332.144836\n",
      "Average loss at step 234100: 332.962607\n",
      "Average loss at step 234200: 334.086719\n",
      "Average loss at step 234300: 329.242579\n",
      "Average loss at step 234400: 331.191401\n",
      "Average loss at step 234500: 330.522517\n",
      "Average loss at step 234600: 331.136356\n",
      "Average loss at step 234700: 330.948295\n",
      "Average loss at step 234800: 331.229800\n",
      "Average loss at step 234900: 329.994747\n",
      "Average loss at step 235000: 331.573225\n",
      "Graph 47: 24 nodes\n",
      "Average loss at step 235100: 364.794503\n",
      "Average loss at step 235200: 338.613845\n",
      "Average loss at step 235300: 339.377106\n",
      "Average loss at step 235400: 343.047598\n",
      "Average loss at step 235500: 337.322943\n",
      "Average loss at step 235600: 339.819618\n",
      "Average loss at step 235700: 336.324792\n",
      "Average loss at step 235800: 337.394933\n",
      "Average loss at step 235900: 339.755118\n",
      "Average loss at step 236000: 336.667238\n",
      "Average loss at step 236100: 344.489564\n",
      "Average loss at step 236200: 337.111500\n",
      "Average loss at step 236300: 332.775007\n",
      "Average loss at step 236400: 340.440780\n",
      "Average loss at step 236500: 335.477221\n",
      "Average loss at step 236600: 333.877819\n",
      "Average loss at step 236700: 335.571337\n",
      "Average loss at step 236800: 336.516071\n",
      "Average loss at step 236900: 338.686307\n",
      "Average loss at step 237000: 337.737282\n",
      "Average loss at step 237100: 336.152461\n",
      "Average loss at step 237200: 336.382721\n",
      "Average loss at step 237300: 329.737833\n",
      "Average loss at step 237400: 337.045420\n",
      "Average loss at step 237500: 336.312973\n",
      "Average loss at step 237600: 335.486070\n",
      "Average loss at step 237700: 338.885597\n",
      "Average loss at step 237800: 338.069207\n",
      "Average loss at step 237900: 337.112951\n",
      "Average loss at step 238000: 334.527085\n",
      "Average loss at step 238100: 336.273364\n",
      "Average loss at step 238200: 333.346492\n",
      "Average loss at step 238300: 334.946927\n",
      "Average loss at step 238400: 334.312209\n",
      "Average loss at step 238500: 338.796844\n",
      "Average loss at step 238600: 339.182750\n",
      "Average loss at step 238700: 337.028286\n",
      "Average loss at step 238800: 332.938236\n",
      "Average loss at step 238900: 338.067917\n",
      "Average loss at step 239000: 335.915006\n",
      "Average loss at step 239100: 336.234047\n",
      "Average loss at step 239200: 337.155377\n",
      "Average loss at step 239300: 337.521992\n",
      "Average loss at step 239400: 335.227383\n",
      "Average loss at step 239500: 334.885709\n",
      "Average loss at step 239600: 334.771102\n",
      "Average loss at step 239700: 339.112641\n",
      "Average loss at step 239800: 334.811395\n",
      "Average loss at step 239900: 335.327141\n",
      "Average loss at step 240000: 335.420170\n",
      "Graph 48: 22 nodes\n",
      "Average loss at step 240100: 343.633899\n",
      "Average loss at step 240200: 334.051632\n",
      "Average loss at step 240300: 332.532459\n",
      "Average loss at step 240400: 333.602311\n",
      "Average loss at step 240500: 333.668832\n",
      "Average loss at step 240600: 330.751499\n",
      "Average loss at step 240700: 334.841529\n",
      "Average loss at step 240800: 336.129162\n",
      "Average loss at step 240900: 330.694342\n",
      "Average loss at step 241000: 335.184172\n",
      "Average loss at step 241100: 329.515113\n",
      "Average loss at step 241200: 331.008906\n",
      "Average loss at step 241300: 332.514147\n",
      "Average loss at step 241400: 329.499409\n",
      "Average loss at step 241500: 332.721816\n",
      "Average loss at step 241600: 332.049815\n",
      "Average loss at step 241700: 330.136696\n",
      "Average loss at step 241800: 331.751449\n",
      "Average loss at step 241900: 330.719243\n",
      "Average loss at step 242000: 331.473180\n",
      "Average loss at step 242100: 330.755350\n",
      "Average loss at step 242200: 333.362246\n",
      "Average loss at step 242300: 332.931152\n",
      "Average loss at step 242400: 332.072517\n",
      "Average loss at step 242500: 329.029948\n",
      "Average loss at step 242600: 332.221293\n",
      "Average loss at step 242700: 329.426468\n",
      "Average loss at step 242800: 332.873394\n",
      "Average loss at step 242900: 330.324953\n",
      "Average loss at step 243000: 328.665068\n",
      "Average loss at step 243100: 330.076997\n",
      "Average loss at step 243200: 330.252109\n",
      "Average loss at step 243300: 329.587301\n",
      "Average loss at step 243400: 331.600291\n",
      "Average loss at step 243500: 327.858021\n",
      "Average loss at step 243600: 329.976326\n",
      "Average loss at step 243700: 327.458565\n",
      "Average loss at step 243800: 330.093739\n",
      "Average loss at step 243900: 332.420086\n",
      "Average loss at step 244000: 328.260137\n",
      "Average loss at step 244100: 330.663393\n",
      "Average loss at step 244200: 329.442394\n",
      "Average loss at step 244300: 330.686989\n",
      "Average loss at step 244400: 330.083059\n",
      "Average loss at step 244500: 332.915994\n",
      "Average loss at step 244600: 330.157218\n",
      "Average loss at step 244700: 330.272118\n",
      "Average loss at step 244800: 329.342820\n",
      "Average loss at step 244900: 328.116828\n",
      "Average loss at step 245000: 328.326608\n",
      "Graph 49: 32 nodes\n",
      "Average loss at step 245100: 335.719506\n",
      "Average loss at step 245200: 331.072250\n",
      "Average loss at step 245300: 328.006320\n",
      "Average loss at step 245400: 334.114551\n",
      "Average loss at step 245500: 330.155378\n",
      "Average loss at step 245600: 332.937052\n",
      "Average loss at step 245700: 328.824317\n",
      "Average loss at step 245800: 333.293399\n",
      "Average loss at step 245900: 326.261469\n",
      "Average loss at step 246000: 330.488750\n",
      "Average loss at step 246100: 332.492241\n",
      "Average loss at step 246200: 330.652757\n",
      "Average loss at step 246300: 332.773682\n",
      "Average loss at step 246400: 328.160922\n",
      "Average loss at step 246500: 328.610804\n",
      "Average loss at step 246600: 330.575053\n",
      "Average loss at step 246700: 324.807731\n",
      "Average loss at step 246800: 327.694928\n",
      "Average loss at step 246900: 331.504551\n",
      "Average loss at step 247000: 329.737416\n",
      "Average loss at step 247100: 330.381811\n",
      "Average loss at step 247200: 328.398999\n",
      "Average loss at step 247300: 329.191036\n",
      "Average loss at step 247400: 328.367130\n",
      "Average loss at step 247500: 328.523894\n",
      "Average loss at step 247600: 327.071055\n",
      "Average loss at step 247700: 327.269347\n",
      "Average loss at step 247800: 329.574099\n",
      "Average loss at step 247900: 329.866276\n",
      "Average loss at step 248000: 326.622087\n",
      "Average loss at step 248100: 331.473332\n",
      "Average loss at step 248200: 332.855197\n",
      "Average loss at step 248300: 328.767525\n",
      "Average loss at step 248400: 329.107452\n",
      "Average loss at step 248500: 329.222115\n",
      "Average loss at step 248600: 323.556410\n",
      "Average loss at step 248700: 333.464923\n",
      "Average loss at step 248800: 328.881505\n",
      "Average loss at step 248900: 324.470083\n",
      "Average loss at step 249000: 326.961208\n",
      "Average loss at step 249100: 328.791150\n",
      "Average loss at step 249200: 329.923867\n",
      "Average loss at step 249300: 326.929254\n",
      "Average loss at step 249400: 331.014084\n",
      "Average loss at step 249500: 325.757779\n",
      "Average loss at step 249600: 329.332508\n",
      "Average loss at step 249700: 328.763482\n",
      "Average loss at step 249800: 328.187934\n",
      "Average loss at step 249900: 326.921125\n",
      "Average loss at step 250000: 324.487517\n",
      "Graph 50: 22 nodes\n",
      "Average loss at step 250100: 344.318460\n",
      "Average loss at step 250200: 336.871445\n",
      "Average loss at step 250300: 333.878967\n",
      "Average loss at step 250400: 334.081643\n",
      "Average loss at step 250500: 329.167485\n",
      "Average loss at step 250600: 330.969235\n",
      "Average loss at step 250700: 332.251199\n",
      "Average loss at step 250800: 331.942481\n",
      "Average loss at step 250900: 329.199764\n",
      "Average loss at step 251000: 329.124840\n",
      "Average loss at step 251100: 327.762598\n",
      "Average loss at step 251200: 328.271557\n",
      "Average loss at step 251300: 330.313331\n",
      "Average loss at step 251400: 331.587892\n",
      "Average loss at step 251500: 327.731299\n",
      "Average loss at step 251600: 326.782429\n",
      "Average loss at step 251700: 330.487855\n",
      "Average loss at step 251800: 326.474028\n",
      "Average loss at step 251900: 331.852661\n",
      "Average loss at step 252000: 326.721186\n",
      "Average loss at step 252100: 327.476641\n",
      "Average loss at step 252200: 327.480456\n",
      "Average loss at step 252300: 328.099576\n",
      "Average loss at step 252400: 328.764522\n",
      "Average loss at step 252500: 329.622426\n",
      "Average loss at step 252600: 328.915038\n",
      "Average loss at step 252700: 328.141585\n",
      "Average loss at step 252800: 326.917795\n",
      "Average loss at step 252900: 327.960694\n",
      "Average loss at step 253000: 329.942230\n",
      "Average loss at step 253100: 331.167075\n",
      "Average loss at step 253200: 323.299014\n",
      "Average loss at step 253300: 328.276851\n",
      "Average loss at step 253400: 324.075089\n",
      "Average loss at step 253500: 328.482389\n",
      "Average loss at step 253600: 330.167520\n",
      "Average loss at step 253700: 325.211848\n",
      "Average loss at step 253800: 328.376936\n",
      "Average loss at step 253900: 327.864441\n",
      "Average loss at step 254000: 331.133532\n",
      "Average loss at step 254100: 326.750692\n",
      "Average loss at step 254200: 328.094729\n",
      "Average loss at step 254300: 331.379487\n",
      "Average loss at step 254400: 326.129585\n",
      "Average loss at step 254500: 324.119271\n",
      "Average loss at step 254600: 327.390432\n",
      "Average loss at step 254700: 327.121560\n",
      "Average loss at step 254800: 327.763984\n",
      "Average loss at step 254900: 328.471274\n",
      "Average loss at step 255000: 326.196849\n",
      "Time: 19.4241991043\n",
      "Graph 51: 32 nodes\n",
      "Average loss at step 255100: 333.162367\n",
      "Average loss at step 255200: 332.936431\n",
      "Average loss at step 255300: 325.425576\n",
      "Average loss at step 255400: 329.708638\n",
      "Average loss at step 255500: 330.085563\n",
      "Average loss at step 255600: 324.253342\n",
      "Average loss at step 255700: 328.268301\n",
      "Average loss at step 255800: 325.106698\n",
      "Average loss at step 255900: 324.713634\n",
      "Average loss at step 256000: 330.664373\n",
      "Average loss at step 256100: 327.080315\n",
      "Average loss at step 256200: 322.737882\n",
      "Average loss at step 256300: 326.903206\n",
      "Average loss at step 256400: 325.292106\n",
      "Average loss at step 256500: 326.851074\n",
      "Average loss at step 256600: 327.477841\n",
      "Average loss at step 256700: 326.827589\n",
      "Average loss at step 256800: 325.978280\n",
      "Average loss at step 256900: 320.295669\n",
      "Average loss at step 257000: 326.904605\n",
      "Average loss at step 257100: 326.288583\n",
      "Average loss at step 257200: 326.443235\n",
      "Average loss at step 257300: 324.696935\n",
      "Average loss at step 257400: 327.722282\n",
      "Average loss at step 257500: 325.756851\n",
      "Average loss at step 257600: 326.812469\n",
      "Average loss at step 257700: 323.088279\n",
      "Average loss at step 257800: 329.276742\n",
      "Average loss at step 257900: 328.356345\n",
      "Average loss at step 258000: 328.453738\n",
      "Average loss at step 258100: 327.610122\n",
      "Average loss at step 258200: 326.722519\n",
      "Average loss at step 258300: 323.543485\n",
      "Average loss at step 258400: 324.347404\n",
      "Average loss at step 258500: 324.155538\n",
      "Average loss at step 258600: 327.613790\n",
      "Average loss at step 258700: 321.437584\n",
      "Average loss at step 258800: 325.018697\n",
      "Average loss at step 258900: 327.696456\n",
      "Average loss at step 259000: 323.776375\n",
      "Average loss at step 259100: 324.712923\n",
      "Average loss at step 259200: 324.383470\n",
      "Average loss at step 259300: 324.318355\n",
      "Average loss at step 259400: 325.916320\n",
      "Average loss at step 259500: 326.615355\n",
      "Average loss at step 259600: 325.344150\n",
      "Average loss at step 259700: 326.453017\n",
      "Average loss at step 259800: 323.446282\n",
      "Average loss at step 259900: 328.193004\n",
      "Average loss at step 260000: 324.184051\n",
      "Graph 52: 19 nodes\n",
      "Average loss at step 260100: 330.438521\n",
      "Average loss at step 260200: 322.838285\n",
      "Average loss at step 260300: 323.712935\n",
      "Average loss at step 260400: 319.962640\n",
      "Average loss at step 260500: 326.667150\n",
      "Average loss at step 260600: 321.990503\n",
      "Average loss at step 260700: 325.604439\n",
      "Average loss at step 260800: 322.356760\n",
      "Average loss at step 260900: 319.245661\n",
      "Average loss at step 261000: 322.491253\n",
      "Average loss at step 261100: 321.713280\n",
      "Average loss at step 261200: 320.976119\n",
      "Average loss at step 261300: 319.685566\n",
      "Average loss at step 261400: 322.219946\n",
      "Average loss at step 261500: 320.445179\n",
      "Average loss at step 261600: 321.029049\n",
      "Average loss at step 261700: 318.303290\n",
      "Average loss at step 261800: 318.733912\n",
      "Average loss at step 261900: 322.286349\n",
      "Average loss at step 262000: 321.832850\n",
      "Average loss at step 262100: 321.265543\n",
      "Average loss at step 262200: 316.445461\n",
      "Average loss at step 262300: 318.296794\n",
      "Average loss at step 262400: 318.923444\n",
      "Average loss at step 262500: 318.510660\n",
      "Average loss at step 262600: 318.027962\n",
      "Average loss at step 262700: 319.816335\n",
      "Average loss at step 262800: 322.108181\n",
      "Average loss at step 262900: 320.015862\n",
      "Average loss at step 263000: 319.944544\n",
      "Average loss at step 263100: 320.076368\n",
      "Average loss at step 263200: 316.767705\n",
      "Average loss at step 263300: 319.423013\n",
      "Average loss at step 263400: 319.841941\n",
      "Average loss at step 263500: 322.319173\n",
      "Average loss at step 263600: 317.343634\n",
      "Average loss at step 263700: 319.225600\n",
      "Average loss at step 263800: 323.180710\n",
      "Average loss at step 263900: 322.486886\n",
      "Average loss at step 264000: 320.764649\n",
      "Average loss at step 264100: 321.093324\n",
      "Average loss at step 264200: 320.370898\n",
      "Average loss at step 264300: 321.704024\n",
      "Average loss at step 264400: 321.998770\n",
      "Average loss at step 264500: 320.162740\n",
      "Average loss at step 264600: 321.898354\n",
      "Average loss at step 264700: 318.091549\n",
      "Average loss at step 264800: 319.573270\n",
      "Average loss at step 264900: 319.843885\n",
      "Average loss at step 265000: 319.026318\n",
      "Graph 53: 20 nodes\n",
      "Average loss at step 265100: 344.519403\n",
      "Average loss at step 265200: 335.303977\n",
      "Average loss at step 265300: 333.948957\n",
      "Average loss at step 265400: 333.596207\n",
      "Average loss at step 265500: 330.684224\n",
      "Average loss at step 265600: 331.173922\n",
      "Average loss at step 265700: 332.109908\n",
      "Average loss at step 265800: 335.666319\n",
      "Average loss at step 265900: 329.596373\n",
      "Average loss at step 266000: 332.273801\n",
      "Average loss at step 266100: 333.585421\n",
      "Average loss at step 266200: 330.456727\n",
      "Average loss at step 266300: 329.786398\n",
      "Average loss at step 266400: 333.131447\n",
      "Average loss at step 266500: 331.583123\n",
      "Average loss at step 266600: 331.593807\n",
      "Average loss at step 266700: 330.533355\n",
      "Average loss at step 266800: 332.336837\n",
      "Average loss at step 266900: 329.028183\n",
      "Average loss at step 267000: 332.182213\n",
      "Average loss at step 267100: 329.890785\n",
      "Average loss at step 267200: 332.885131\n",
      "Average loss at step 267300: 329.103096\n",
      "Average loss at step 267400: 328.239389\n",
      "Average loss at step 267500: 333.020184\n",
      "Average loss at step 267600: 327.192405\n",
      "Average loss at step 267700: 330.077820\n",
      "Average loss at step 267800: 332.491592\n",
      "Average loss at step 267900: 331.937965\n",
      "Average loss at step 268000: 332.354473\n",
      "Average loss at step 268100: 330.077230\n",
      "Average loss at step 268200: 330.922773\n",
      "Average loss at step 268300: 330.109505\n",
      "Average loss at step 268400: 328.012083\n",
      "Average loss at step 268500: 330.947311\n",
      "Average loss at step 268600: 332.724351\n",
      "Average loss at step 268700: 331.953875\n",
      "Average loss at step 268800: 328.572855\n",
      "Average loss at step 268900: 334.782050\n",
      "Average loss at step 269000: 330.818513\n",
      "Average loss at step 269100: 330.795972\n",
      "Average loss at step 269200: 334.111540\n",
      "Average loss at step 269300: 333.388073\n",
      "Average loss at step 269400: 333.199391\n",
      "Average loss at step 269500: 329.058685\n",
      "Average loss at step 269600: 329.127515\n",
      "Average loss at step 269700: 328.492735\n",
      "Average loss at step 269800: 330.954065\n",
      "Average loss at step 269900: 329.692857\n",
      "Average loss at step 270000: 332.551939\n",
      "Graph 54: 16 nodes\n",
      "Average loss at step 270100: 338.712931\n",
      "Average loss at step 270200: 331.984113\n",
      "Average loss at step 270300: 332.241153\n",
      "Average loss at step 270400: 328.305312\n",
      "Average loss at step 270500: 331.413465\n",
      "Average loss at step 270600: 332.477649\n",
      "Average loss at step 270700: 326.809201\n",
      "Average loss at step 270800: 331.101888\n",
      "Average loss at step 270900: 326.662510\n",
      "Average loss at step 271000: 325.134470\n",
      "Average loss at step 271100: 326.528065\n",
      "Average loss at step 271200: 330.770780\n",
      "Average loss at step 271300: 331.541049\n",
      "Average loss at step 271400: 326.685340\n",
      "Average loss at step 271500: 330.200799\n",
      "Average loss at step 271600: 329.677423\n",
      "Average loss at step 271700: 328.256390\n",
      "Average loss at step 271800: 328.502872\n",
      "Average loss at step 271900: 322.001535\n",
      "Average loss at step 272000: 326.189463\n",
      "Average loss at step 272100: 330.449998\n",
      "Average loss at step 272200: 326.045280\n",
      "Average loss at step 272300: 327.214031\n",
      "Average loss at step 272400: 324.700087\n",
      "Average loss at step 272500: 328.285338\n",
      "Average loss at step 272600: 330.551990\n",
      "Average loss at step 272700: 325.974729\n",
      "Average loss at step 272800: 328.686847\n",
      "Average loss at step 272900: 325.598133\n",
      "Average loss at step 273000: 326.629184\n",
      "Average loss at step 273100: 325.066519\n",
      "Average loss at step 273200: 327.877878\n",
      "Average loss at step 273300: 328.162116\n",
      "Average loss at step 273400: 323.247494\n",
      "Average loss at step 273500: 325.744573\n",
      "Average loss at step 273600: 325.330859\n",
      "Average loss at step 273700: 328.988326\n",
      "Average loss at step 273800: 329.992848\n",
      "Average loss at step 273900: 328.206436\n",
      "Average loss at step 274000: 325.008669\n",
      "Average loss at step 274100: 326.913501\n",
      "Average loss at step 274200: 326.187432\n",
      "Average loss at step 274300: 326.804083\n",
      "Average loss at step 274400: 323.523978\n",
      "Average loss at step 274500: 326.142572\n",
      "Average loss at step 274600: 328.910074\n",
      "Average loss at step 274700: 328.222294\n",
      "Average loss at step 274800: 329.058463\n",
      "Average loss at step 274900: 327.975091\n",
      "Average loss at step 275000: 326.129663\n",
      "Graph 55: 23 nodes\n",
      "Average loss at step 275100: 339.804991\n",
      "Average loss at step 275200: 331.250749\n",
      "Average loss at step 275300: 328.979270\n",
      "Average loss at step 275400: 329.437130\n",
      "Average loss at step 275500: 331.348904\n",
      "Average loss at step 275600: 330.221318\n",
      "Average loss at step 275700: 328.690275\n",
      "Average loss at step 275800: 331.263537\n",
      "Average loss at step 275900: 329.413510\n",
      "Average loss at step 276000: 324.980903\n",
      "Average loss at step 276100: 329.605643\n",
      "Average loss at step 276200: 330.584029\n",
      "Average loss at step 276300: 328.809469\n",
      "Average loss at step 276400: 328.971598\n",
      "Average loss at step 276500: 327.162037\n",
      "Average loss at step 276600: 328.016842\n",
      "Average loss at step 276700: 328.830053\n",
      "Average loss at step 276800: 330.498698\n",
      "Average loss at step 276900: 330.108689\n",
      "Average loss at step 277000: 330.323363\n",
      "Average loss at step 277100: 329.533323\n",
      "Average loss at step 277200: 326.382508\n",
      "Average loss at step 277300: 327.711445\n",
      "Average loss at step 277400: 326.421458\n",
      "Average loss at step 277500: 328.494327\n",
      "Average loss at step 277600: 327.532722\n",
      "Average loss at step 277700: 332.463967\n",
      "Average loss at step 277800: 326.935959\n",
      "Average loss at step 277900: 328.535219\n",
      "Average loss at step 278000: 325.659323\n",
      "Average loss at step 278100: 325.876937\n",
      "Average loss at step 278200: 328.897349\n",
      "Average loss at step 278300: 329.710896\n",
      "Average loss at step 278400: 326.265911\n",
      "Average loss at step 278500: 331.897221\n",
      "Average loss at step 278600: 331.445266\n",
      "Average loss at step 278700: 326.927281\n",
      "Average loss at step 278800: 324.688301\n",
      "Average loss at step 278900: 329.808316\n",
      "Average loss at step 279000: 328.956578\n",
      "Average loss at step 279100: 326.959974\n",
      "Average loss at step 279200: 328.692316\n",
      "Average loss at step 279300: 329.890429\n",
      "Average loss at step 279400: 329.754250\n",
      "Average loss at step 279500: 329.388139\n",
      "Average loss at step 279600: 323.423672\n",
      "Average loss at step 279700: 325.861224\n",
      "Average loss at step 279800: 329.099761\n",
      "Average loss at step 279900: 325.800296\n",
      "Average loss at step 280000: 329.171094\n",
      "Graph 56: 12 nodes\n",
      "Average loss at step 280100: 335.224817\n",
      "Average loss at step 280200: 321.439043\n",
      "Average loss at step 280300: 319.985376\n",
      "Average loss at step 280400: 311.352234\n",
      "Average loss at step 280500: 315.701049\n",
      "Average loss at step 280600: 312.155013\n",
      "Average loss at step 280700: 310.740388\n",
      "Average loss at step 280800: 308.528211\n",
      "Average loss at step 280900: 312.867572\n",
      "Average loss at step 281000: 309.620149\n",
      "Average loss at step 281100: 310.509351\n",
      "Average loss at step 281200: 311.224937\n",
      "Average loss at step 281300: 307.893264\n",
      "Average loss at step 281400: 306.166915\n",
      "Average loss at step 281500: 310.816357\n",
      "Average loss at step 281600: 306.679266\n",
      "Average loss at step 281700: 311.797463\n",
      "Average loss at step 281800: 307.778025\n",
      "Average loss at step 281900: 313.954817\n",
      "Average loss at step 282000: 312.833382\n",
      "Average loss at step 282100: 311.709826\n",
      "Average loss at step 282200: 307.991202\n",
      "Average loss at step 282300: 310.589059\n",
      "Average loss at step 282400: 311.013506\n",
      "Average loss at step 282500: 308.008606\n",
      "Average loss at step 282600: 307.941493\n",
      "Average loss at step 282700: 307.709680\n",
      "Average loss at step 282800: 310.763829\n",
      "Average loss at step 282900: 310.315064\n",
      "Average loss at step 283000: 313.616862\n",
      "Average loss at step 283100: 307.489280\n",
      "Average loss at step 283200: 316.401631\n",
      "Average loss at step 283300: 311.156257\n",
      "Average loss at step 283400: 308.922494\n",
      "Average loss at step 283500: 311.395589\n",
      "Average loss at step 283600: 305.993712\n",
      "Average loss at step 283700: 306.851990\n",
      "Average loss at step 283800: 305.837580\n",
      "Average loss at step 283900: 301.272181\n",
      "Average loss at step 284000: 310.905113\n",
      "Average loss at step 284100: 308.494842\n",
      "Average loss at step 284200: 304.412722\n",
      "Average loss at step 284300: 309.485675\n",
      "Average loss at step 284400: 315.102624\n",
      "Average loss at step 284500: 309.773863\n",
      "Average loss at step 284600: 311.357572\n",
      "Average loss at step 284700: 307.926928\n",
      "Average loss at step 284800: 308.122418\n",
      "Average loss at step 284900: 313.902757\n",
      "Average loss at step 285000: 306.634231\n",
      "Graph 57: 21 nodes\n",
      "Average loss at step 285100: 347.508136\n",
      "Average loss at step 285200: 338.080555\n",
      "Average loss at step 285300: 334.744483\n",
      "Average loss at step 285400: 334.458157\n",
      "Average loss at step 285500: 331.584146\n",
      "Average loss at step 285600: 331.656034\n",
      "Average loss at step 285700: 327.335494\n",
      "Average loss at step 285800: 328.667753\n",
      "Average loss at step 285900: 327.409771\n",
      "Average loss at step 286000: 330.980862\n",
      "Average loss at step 286100: 327.079612\n",
      "Average loss at step 286200: 328.708476\n",
      "Average loss at step 286300: 328.653613\n",
      "Average loss at step 286400: 330.182334\n",
      "Average loss at step 286500: 330.477487\n",
      "Average loss at step 286600: 328.589696\n",
      "Average loss at step 286700: 330.422905\n",
      "Average loss at step 286800: 331.819915\n",
      "Average loss at step 286900: 329.423496\n",
      "Average loss at step 287000: 327.373983\n",
      "Average loss at step 287100: 331.228103\n",
      "Average loss at step 287200: 327.486534\n",
      "Average loss at step 287300: 330.932214\n",
      "Average loss at step 287400: 330.924786\n",
      "Average loss at step 287500: 326.672833\n",
      "Average loss at step 287600: 325.898094\n",
      "Average loss at step 287700: 328.495944\n",
      "Average loss at step 287800: 332.983829\n",
      "Average loss at step 287900: 331.240209\n",
      "Average loss at step 288000: 328.797996\n",
      "Average loss at step 288100: 327.271477\n",
      "Average loss at step 288200: 326.571675\n",
      "Average loss at step 288300: 325.260632\n",
      "Average loss at step 288400: 327.829975\n",
      "Average loss at step 288500: 326.721357\n",
      "Average loss at step 288600: 330.242963\n",
      "Average loss at step 288700: 326.434212\n",
      "Average loss at step 288800: 331.240264\n",
      "Average loss at step 288900: 332.363702\n",
      "Average loss at step 289000: 327.199729\n",
      "Average loss at step 289100: 326.311714\n",
      "Average loss at step 289200: 328.986933\n",
      "Average loss at step 289300: 328.096790\n",
      "Average loss at step 289400: 326.225154\n",
      "Average loss at step 289500: 328.626027\n",
      "Average loss at step 289600: 328.918642\n",
      "Average loss at step 289700: 328.694296\n",
      "Average loss at step 289800: 325.479873\n",
      "Average loss at step 289900: 329.718887\n",
      "Average loss at step 290000: 328.244395\n",
      "Graph 58: 31 nodes\n",
      "Average loss at step 290100: 345.468194\n",
      "Average loss at step 290200: 330.545320\n",
      "Average loss at step 290300: 324.701216\n",
      "Average loss at step 290400: 324.947943\n",
      "Average loss at step 290500: 322.424019\n",
      "Average loss at step 290600: 326.134547\n",
      "Average loss at step 290700: 324.078161\n",
      "Average loss at step 290800: 321.014195\n",
      "Average loss at step 290900: 321.579671\n",
      "Average loss at step 291000: 320.042833\n",
      "Average loss at step 291100: 321.239858\n",
      "Average loss at step 291200: 322.464361\n",
      "Average loss at step 291300: 323.610622\n",
      "Average loss at step 291400: 318.483680\n",
      "Average loss at step 291500: 319.677822\n",
      "Average loss at step 291600: 321.381370\n",
      "Average loss at step 291700: 320.541333\n",
      "Average loss at step 291800: 322.263760\n",
      "Average loss at step 291900: 320.145039\n",
      "Average loss at step 292000: 321.726077\n",
      "Average loss at step 292100: 321.761485\n",
      "Average loss at step 292200: 324.617639\n",
      "Average loss at step 292300: 318.345853\n",
      "Average loss at step 292400: 321.900415\n",
      "Average loss at step 292500: 319.408185\n",
      "Average loss at step 292600: 317.128542\n",
      "Average loss at step 292700: 319.318918\n",
      "Average loss at step 292800: 320.178966\n",
      "Average loss at step 292900: 317.410032\n",
      "Average loss at step 293000: 319.686613\n",
      "Average loss at step 293100: 321.488969\n",
      "Average loss at step 293200: 323.803241\n",
      "Average loss at step 293300: 321.939147\n",
      "Average loss at step 293400: 320.474108\n",
      "Average loss at step 293500: 317.598964\n",
      "Average loss at step 293600: 323.147521\n",
      "Average loss at step 293700: 321.639791\n",
      "Average loss at step 293800: 316.986059\n",
      "Average loss at step 293900: 320.789577\n",
      "Average loss at step 294000: 315.428944\n",
      "Average loss at step 294100: 321.380790\n",
      "Average loss at step 294200: 319.990817\n",
      "Average loss at step 294300: 316.634838\n",
      "Average loss at step 294400: 319.427081\n",
      "Average loss at step 294500: 323.150001\n",
      "Average loss at step 294600: 320.004366\n",
      "Average loss at step 294700: 319.225534\n",
      "Average loss at step 294800: 318.635133\n",
      "Average loss at step 294900: 323.818772\n",
      "Average loss at step 295000: 317.449792\n",
      "Graph 59: 20 nodes\n",
      "Average loss at step 295100: 346.848305\n",
      "Average loss at step 295200: 339.142638\n",
      "Average loss at step 295300: 338.083870\n",
      "Average loss at step 295400: 337.343590\n",
      "Average loss at step 295500: 337.981246\n",
      "Average loss at step 295600: 336.875566\n",
      "Average loss at step 295700: 339.267079\n",
      "Average loss at step 295800: 335.287504\n",
      "Average loss at step 295900: 338.326509\n",
      "Average loss at step 296000: 332.682249\n",
      "Average loss at step 296100: 333.201820\n",
      "Average loss at step 296200: 334.831061\n",
      "Average loss at step 296300: 330.784613\n",
      "Average loss at step 296400: 330.895717\n",
      "Average loss at step 296500: 335.110021\n",
      "Average loss at step 296600: 331.205664\n",
      "Average loss at step 296700: 334.897088\n",
      "Average loss at step 296800: 332.034019\n",
      "Average loss at step 296900: 333.163408\n",
      "Average loss at step 297000: 332.075284\n",
      "Average loss at step 297100: 333.871825\n",
      "Average loss at step 297200: 331.849415\n",
      "Average loss at step 297300: 334.476930\n",
      "Average loss at step 297400: 335.281761\n",
      "Average loss at step 297500: 336.067441\n",
      "Average loss at step 297600: 331.632129\n",
      "Average loss at step 297700: 336.494610\n",
      "Average loss at step 297800: 335.687534\n",
      "Average loss at step 297900: 332.867610\n",
      "Average loss at step 298000: 332.021336\n",
      "Average loss at step 298100: 333.101034\n",
      "Average loss at step 298200: 333.119667\n",
      "Average loss at step 298300: 335.420251\n",
      "Average loss at step 298400: 333.390872\n",
      "Average loss at step 298500: 334.246537\n",
      "Average loss at step 298600: 331.555965\n",
      "Average loss at step 298700: 332.933595\n",
      "Average loss at step 298800: 334.349786\n",
      "Average loss at step 298900: 334.547546\n",
      "Average loss at step 299000: 333.075201\n",
      "Average loss at step 299100: 334.073409\n",
      "Average loss at step 299200: 332.694164\n",
      "Average loss at step 299300: 332.962442\n",
      "Average loss at step 299400: 335.798879\n",
      "Average loss at step 299500: 332.465700\n",
      "Average loss at step 299600: 333.980232\n",
      "Average loss at step 299700: 331.424175\n",
      "Average loss at step 299800: 332.957379\n",
      "Average loss at step 299900: 333.623672\n",
      "Average loss at step 300000: 334.469819\n",
      "Graph 60: 18 nodes\n",
      "Average loss at step 300100: 351.783175\n",
      "Average loss at step 300200: 343.680286\n",
      "Average loss at step 300300: 341.247528\n",
      "Average loss at step 300400: 342.370150\n",
      "Average loss at step 300500: 336.842353\n",
      "Average loss at step 300600: 338.744699\n",
      "Average loss at step 300700: 339.994151\n",
      "Average loss at step 300800: 335.726916\n",
      "Average loss at step 300900: 334.481018\n",
      "Average loss at step 301000: 336.330103\n",
      "Average loss at step 301100: 335.779685\n",
      "Average loss at step 301200: 337.353338\n",
      "Average loss at step 301300: 335.586559\n",
      "Average loss at step 301400: 336.469353\n",
      "Average loss at step 301500: 338.043897\n",
      "Average loss at step 301600: 337.298870\n",
      "Average loss at step 301700: 335.146002\n",
      "Average loss at step 301800: 340.995492\n",
      "Average loss at step 301900: 337.250721\n",
      "Average loss at step 302000: 337.022669\n",
      "Average loss at step 302100: 340.896429\n",
      "Average loss at step 302200: 336.629634\n",
      "Average loss at step 302300: 335.154082\n",
      "Average loss at step 302400: 334.598483\n",
      "Average loss at step 302500: 336.305790\n",
      "Average loss at step 302600: 335.028033\n",
      "Average loss at step 302700: 332.410998\n",
      "Average loss at step 302800: 336.044844\n",
      "Average loss at step 302900: 336.179207\n",
      "Average loss at step 303000: 335.819514\n",
      "Average loss at step 303100: 335.433875\n",
      "Average loss at step 303200: 335.309731\n",
      "Average loss at step 303300: 338.359711\n",
      "Average loss at step 303400: 337.087632\n",
      "Average loss at step 303500: 335.245574\n",
      "Average loss at step 303600: 334.397188\n",
      "Average loss at step 303700: 332.793025\n",
      "Average loss at step 303800: 335.595702\n",
      "Average loss at step 303900: 337.508100\n",
      "Average loss at step 304000: 335.124194\n",
      "Average loss at step 304100: 334.117423\n",
      "Average loss at step 304200: 334.448672\n",
      "Average loss at step 304300: 333.283497\n",
      "Average loss at step 304400: 335.721508\n",
      "Average loss at step 304500: 334.546696\n",
      "Average loss at step 304600: 334.192636\n",
      "Average loss at step 304700: 335.479283\n",
      "Average loss at step 304800: 335.975695\n",
      "Average loss at step 304900: 332.066399\n",
      "Average loss at step 305000: 334.315597\n",
      "Time: 19.7466359138\n",
      "Graph 61: 16 nodes\n",
      "Average loss at step 305100: 373.352484\n",
      "Average loss at step 305200: 335.149749\n",
      "Average loss at step 305300: 334.028430\n",
      "Average loss at step 305400: 332.212661\n",
      "Average loss at step 305500: 334.008960\n",
      "Average loss at step 305600: 335.101635\n",
      "Average loss at step 305700: 332.229875\n",
      "Average loss at step 305800: 330.200913\n",
      "Average loss at step 305900: 333.464956\n",
      "Average loss at step 306000: 332.371522\n",
      "Average loss at step 306100: 333.625955\n",
      "Average loss at step 306200: 334.915168\n",
      "Average loss at step 306300: 331.363160\n",
      "Average loss at step 306400: 332.243468\n",
      "Average loss at step 306500: 328.386423\n",
      "Average loss at step 306600: 335.398919\n",
      "Average loss at step 306700: 329.962541\n",
      "Average loss at step 306800: 331.449279\n",
      "Average loss at step 306900: 329.275738\n",
      "Average loss at step 307000: 332.830653\n",
      "Average loss at step 307100: 332.878093\n",
      "Average loss at step 307200: 334.515554\n",
      "Average loss at step 307300: 337.063335\n",
      "Average loss at step 307400: 335.228422\n",
      "Average loss at step 307500: 334.107817\n",
      "Average loss at step 307600: 333.852708\n",
      "Average loss at step 307700: 332.406662\n",
      "Average loss at step 307800: 333.703342\n",
      "Average loss at step 307900: 332.818974\n",
      "Average loss at step 308000: 328.355439\n",
      "Average loss at step 308100: 332.936780\n",
      "Average loss at step 308200: 330.551407\n",
      "Average loss at step 308300: 331.656330\n",
      "Average loss at step 308400: 333.747296\n",
      "Average loss at step 308500: 334.951236\n",
      "Average loss at step 308600: 331.085864\n",
      "Average loss at step 308700: 331.773978\n",
      "Average loss at step 308800: 331.509994\n",
      "Average loss at step 308900: 329.823716\n",
      "Average loss at step 309000: 331.723513\n",
      "Average loss at step 309100: 330.222709\n",
      "Average loss at step 309200: 333.160078\n",
      "Average loss at step 309300: 328.908273\n",
      "Average loss at step 309400: 331.697119\n",
      "Average loss at step 309500: 333.638761\n",
      "Average loss at step 309600: 331.688196\n",
      "Average loss at step 309700: 333.150318\n",
      "Average loss at step 309800: 335.625633\n",
      "Average loss at step 309900: 332.791112\n",
      "Average loss at step 310000: 329.763607\n",
      "Graph 62: 20 nodes\n",
      "Average loss at step 310100: 351.697805\n",
      "Average loss at step 310200: 342.278466\n",
      "Average loss at step 310300: 337.582326\n",
      "Average loss at step 310400: 334.173529\n",
      "Average loss at step 310500: 332.223079\n",
      "Average loss at step 310600: 335.650440\n",
      "Average loss at step 310700: 334.965140\n",
      "Average loss at step 310800: 335.475842\n",
      "Average loss at step 310900: 335.439927\n",
      "Average loss at step 311000: 333.072657\n",
      "Average loss at step 311100: 336.043484\n",
      "Average loss at step 311200: 332.716172\n",
      "Average loss at step 311300: 335.925809\n",
      "Average loss at step 311400: 338.001534\n",
      "Average loss at step 311500: 334.491222\n",
      "Average loss at step 311600: 332.965592\n",
      "Average loss at step 311700: 333.458665\n",
      "Average loss at step 311800: 333.338464\n",
      "Average loss at step 311900: 336.517063\n",
      "Average loss at step 312000: 332.748130\n",
      "Average loss at step 312100: 333.444912\n",
      "Average loss at step 312200: 331.585582\n",
      "Average loss at step 312300: 329.015143\n",
      "Average loss at step 312400: 335.470759\n",
      "Average loss at step 312500: 330.782301\n",
      "Average loss at step 312600: 330.481545\n",
      "Average loss at step 312700: 333.934207\n",
      "Average loss at step 312800: 332.295119\n",
      "Average loss at step 312900: 333.331481\n",
      "Average loss at step 313000: 332.573636\n",
      "Average loss at step 313100: 332.712987\n",
      "Average loss at step 313200: 334.166914\n",
      "Average loss at step 313300: 330.694100\n",
      "Average loss at step 313400: 328.953978\n",
      "Average loss at step 313500: 328.066757\n",
      "Average loss at step 313600: 335.413137\n",
      "Average loss at step 313700: 330.754059\n",
      "Average loss at step 313800: 334.949425\n",
      "Average loss at step 313900: 330.502602\n",
      "Average loss at step 314000: 333.767609\n",
      "Average loss at step 314100: 332.095138\n",
      "Average loss at step 314200: 331.007140\n",
      "Average loss at step 314300: 332.522873\n",
      "Average loss at step 314400: 331.223192\n",
      "Average loss at step 314500: 334.062763\n",
      "Average loss at step 314600: 330.947552\n",
      "Average loss at step 314700: 333.966309\n",
      "Average loss at step 314800: 334.972653\n",
      "Average loss at step 314900: 334.203518\n",
      "Average loss at step 315000: 333.982178\n",
      "Graph 63: 36 nodes\n",
      "Average loss at step 315100: 346.428607\n",
      "Average loss at step 315200: 341.824999\n",
      "Average loss at step 315300: 332.817240\n",
      "Average loss at step 315400: 337.923251\n",
      "Average loss at step 315500: 336.775353\n",
      "Average loss at step 315600: 337.193921\n",
      "Average loss at step 315700: 337.579112\n",
      "Average loss at step 315800: 336.995266\n",
      "Average loss at step 315900: 337.132762\n",
      "Average loss at step 316000: 337.696980\n",
      "Average loss at step 316100: 335.977397\n",
      "Average loss at step 316200: 330.297378\n",
      "Average loss at step 316300: 335.058097\n",
      "Average loss at step 316400: 335.918030\n",
      "Average loss at step 316500: 335.714167\n",
      "Average loss at step 316600: 336.343085\n",
      "Average loss at step 316700: 332.696631\n",
      "Average loss at step 316800: 334.207612\n",
      "Average loss at step 316900: 340.166666\n",
      "Average loss at step 317000: 336.310513\n",
      "Average loss at step 317100: 333.936181\n",
      "Average loss at step 317200: 336.836074\n",
      "Average loss at step 317300: 334.301082\n",
      "Average loss at step 317400: 337.785031\n",
      "Average loss at step 317500: 332.237702\n",
      "Average loss at step 317600: 333.713237\n",
      "Average loss at step 317700: 334.608059\n",
      "Average loss at step 317800: 333.766243\n",
      "Average loss at step 317900: 334.075103\n",
      "Average loss at step 318000: 334.483991\n",
      "Average loss at step 318100: 333.345916\n",
      "Average loss at step 318200: 332.027385\n",
      "Average loss at step 318300: 337.154381\n",
      "Average loss at step 318400: 335.843331\n",
      "Average loss at step 318500: 333.400856\n",
      "Average loss at step 318600: 333.500230\n",
      "Average loss at step 318700: 338.367256\n",
      "Average loss at step 318800: 332.340240\n",
      "Average loss at step 318900: 333.599559\n",
      "Average loss at step 319000: 336.682077\n",
      "Average loss at step 319100: 335.230997\n",
      "Average loss at step 319200: 336.585518\n",
      "Average loss at step 319300: 333.998071\n",
      "Average loss at step 319400: 332.901130\n",
      "Average loss at step 319500: 333.070530\n",
      "Average loss at step 319600: 333.572285\n",
      "Average loss at step 319700: 336.910037\n",
      "Average loss at step 319800: 335.421013\n",
      "Average loss at step 319900: 332.839237\n",
      "Average loss at step 320000: 333.144725\n",
      "Graph 64: 21 nodes\n",
      "Average loss at step 320100: 333.910483\n",
      "Average loss at step 320200: 328.742681\n",
      "Average loss at step 320300: 328.110757\n",
      "Average loss at step 320400: 333.925836\n",
      "Average loss at step 320500: 330.790207\n",
      "Average loss at step 320600: 328.124140\n",
      "Average loss at step 320700: 321.301037\n",
      "Average loss at step 320800: 328.801507\n",
      "Average loss at step 320900: 327.420547\n",
      "Average loss at step 321000: 327.068290\n",
      "Average loss at step 321100: 328.688846\n",
      "Average loss at step 321200: 329.417233\n",
      "Average loss at step 321300: 326.580078\n",
      "Average loss at step 321400: 324.955781\n",
      "Average loss at step 321500: 326.319601\n",
      "Average loss at step 321600: 327.598696\n",
      "Average loss at step 321700: 326.173532\n",
      "Average loss at step 321800: 327.927365\n",
      "Average loss at step 321900: 322.664482\n",
      "Average loss at step 322000: 323.787138\n",
      "Average loss at step 322100: 328.762697\n",
      "Average loss at step 322200: 324.672625\n",
      "Average loss at step 322300: 326.891554\n",
      "Average loss at step 322400: 325.480788\n",
      "Average loss at step 322500: 324.985303\n",
      "Average loss at step 322600: 329.728071\n",
      "Average loss at step 322700: 330.371320\n",
      "Average loss at step 322800: 325.340125\n",
      "Average loss at step 322900: 328.597216\n",
      "Average loss at step 323000: 323.290661\n",
      "Average loss at step 323100: 327.765447\n",
      "Average loss at step 323200: 326.462603\n",
      "Average loss at step 323300: 327.037981\n",
      "Average loss at step 323400: 324.221588\n",
      "Average loss at step 323500: 325.986145\n",
      "Average loss at step 323600: 326.553522\n",
      "Average loss at step 323700: 324.367256\n",
      "Average loss at step 323800: 324.540829\n",
      "Average loss at step 323900: 326.223970\n",
      "Average loss at step 324000: 324.656148\n",
      "Average loss at step 324100: 326.865474\n",
      "Average loss at step 324200: 324.395077\n",
      "Average loss at step 324300: 326.810455\n",
      "Average loss at step 324400: 327.471023\n",
      "Average loss at step 324500: 324.423947\n",
      "Average loss at step 324600: 326.146817\n",
      "Average loss at step 324700: 324.221551\n",
      "Average loss at step 324800: 324.859552\n",
      "Average loss at step 324900: 326.329146\n",
      "Average loss at step 325000: 326.256714\n",
      "Graph 65: 31 nodes\n",
      "Average loss at step 325100: 328.388544\n",
      "Average loss at step 325200: 325.867679\n",
      "Average loss at step 325300: 322.731240\n",
      "Average loss at step 325400: 324.934083\n",
      "Average loss at step 325500: 318.130831\n",
      "Average loss at step 325600: 320.920642\n",
      "Average loss at step 325700: 323.071814\n",
      "Average loss at step 325800: 314.319345\n",
      "Average loss at step 325900: 319.207393\n",
      "Average loss at step 326000: 319.374535\n",
      "Average loss at step 326100: 316.936570\n",
      "Average loss at step 326200: 316.850615\n",
      "Average loss at step 326300: 316.400510\n",
      "Average loss at step 326400: 314.617591\n",
      "Average loss at step 326500: 323.253572\n",
      "Average loss at step 326600: 320.599784\n",
      "Average loss at step 326700: 318.786049\n",
      "Average loss at step 326800: 313.649690\n",
      "Average loss at step 326900: 320.558776\n",
      "Average loss at step 327000: 317.645933\n",
      "Average loss at step 327100: 316.328308\n",
      "Average loss at step 327200: 316.501820\n",
      "Average loss at step 327300: 316.653132\n",
      "Average loss at step 327400: 315.302142\n",
      "Average loss at step 327500: 315.862971\n",
      "Average loss at step 327600: 313.669848\n",
      "Average loss at step 327700: 316.288293\n",
      "Average loss at step 327800: 316.829551\n",
      "Average loss at step 327900: 314.201697\n",
      "Average loss at step 328000: 317.028472\n",
      "Average loss at step 328100: 317.428725\n",
      "Average loss at step 328200: 316.445819\n",
      "Average loss at step 328300: 315.627069\n",
      "Average loss at step 328400: 315.814730\n",
      "Average loss at step 328500: 315.608562\n",
      "Average loss at step 328600: 313.065730\n",
      "Average loss at step 328700: 318.731325\n",
      "Average loss at step 328800: 319.920643\n",
      "Average loss at step 328900: 319.109806\n",
      "Average loss at step 329000: 315.622718\n",
      "Average loss at step 329100: 318.787200\n",
      "Average loss at step 329200: 318.687660\n",
      "Average loss at step 329300: 317.703621\n",
      "Average loss at step 329400: 316.041888\n",
      "Average loss at step 329500: 319.061013\n",
      "Average loss at step 329600: 313.135723\n",
      "Average loss at step 329700: 317.248951\n",
      "Average loss at step 329800: 318.073034\n",
      "Average loss at step 329900: 314.996809\n",
      "Average loss at step 330000: 315.930197\n",
      "Graph 66: 35 nodes\n",
      "Average loss at step 330100: 325.248520\n",
      "Average loss at step 330200: 315.631518\n",
      "Average loss at step 330300: 313.188108\n",
      "Average loss at step 330400: 315.519765\n",
      "Average loss at step 330500: 312.246244\n",
      "Average loss at step 330600: 313.512408\n",
      "Average loss at step 330700: 310.259590\n",
      "Average loss at step 330800: 311.546566\n",
      "Average loss at step 330900: 310.812195\n",
      "Average loss at step 331000: 315.860271\n",
      "Average loss at step 331100: 310.352958\n",
      "Average loss at step 331200: 315.725206\n",
      "Average loss at step 331300: 307.020516\n",
      "Average loss at step 331400: 316.335526\n",
      "Average loss at step 331500: 311.525146\n",
      "Average loss at step 331600: 312.971298\n",
      "Average loss at step 331700: 311.807949\n",
      "Average loss at step 331800: 313.533124\n",
      "Average loss at step 331900: 308.454552\n",
      "Average loss at step 332000: 308.907434\n",
      "Average loss at step 332100: 311.472383\n",
      "Average loss at step 332200: 308.614048\n",
      "Average loss at step 332300: 314.749525\n",
      "Average loss at step 332400: 308.597104\n",
      "Average loss at step 332500: 305.661819\n",
      "Average loss at step 332600: 314.158253\n",
      "Average loss at step 332700: 314.159108\n",
      "Average loss at step 332800: 308.693825\n",
      "Average loss at step 332900: 310.091156\n",
      "Average loss at step 333000: 312.627116\n",
      "Average loss at step 333100: 311.943637\n",
      "Average loss at step 333200: 314.201515\n",
      "Average loss at step 333300: 306.936328\n",
      "Average loss at step 333400: 314.866223\n",
      "Average loss at step 333500: 312.666409\n",
      "Average loss at step 333600: 314.975375\n",
      "Average loss at step 333700: 312.005202\n",
      "Average loss at step 333800: 311.039299\n",
      "Average loss at step 333900: 310.898230\n",
      "Average loss at step 334000: 307.600338\n",
      "Average loss at step 334100: 315.949167\n",
      "Average loss at step 334200: 311.019673\n",
      "Average loss at step 334300: 310.178605\n",
      "Average loss at step 334400: 309.656470\n",
      "Average loss at step 334500: 310.004469\n",
      "Average loss at step 334600: 306.663759\n",
      "Average loss at step 334700: 308.573866\n",
      "Average loss at step 334800: 309.168726\n",
      "Average loss at step 334900: 311.512844\n",
      "Average loss at step 335000: 312.669233\n",
      "Graph 67: 26 nodes\n",
      "Average loss at step 335100: 344.235812\n",
      "Average loss at step 335200: 335.230021\n",
      "Average loss at step 335300: 333.183734\n",
      "Average loss at step 335400: 335.763154\n",
      "Average loss at step 335500: 328.267627\n",
      "Average loss at step 335600: 331.319290\n",
      "Average loss at step 335700: 327.655936\n",
      "Average loss at step 335800: 323.725766\n",
      "Average loss at step 335900: 328.353063\n",
      "Average loss at step 336000: 330.180602\n",
      "Average loss at step 336100: 324.886238\n",
      "Average loss at step 336200: 331.398573\n",
      "Average loss at step 336300: 328.878504\n",
      "Average loss at step 336400: 327.823337\n",
      "Average loss at step 336500: 326.607618\n",
      "Average loss at step 336600: 328.286174\n",
      "Average loss at step 336700: 324.290354\n",
      "Average loss at step 336800: 327.216584\n",
      "Average loss at step 336900: 329.486474\n",
      "Average loss at step 337000: 324.502369\n",
      "Average loss at step 337100: 329.066070\n",
      "Average loss at step 337200: 331.273167\n",
      "Average loss at step 337300: 329.114562\n",
      "Average loss at step 337400: 331.484562\n",
      "Average loss at step 337500: 325.946557\n",
      "Average loss at step 337600: 327.046857\n",
      "Average loss at step 337700: 328.709195\n",
      "Average loss at step 337800: 327.483242\n",
      "Average loss at step 337900: 330.703745\n",
      "Average loss at step 338000: 325.828820\n",
      "Average loss at step 338100: 330.753418\n",
      "Average loss at step 338200: 328.799932\n",
      "Average loss at step 338300: 329.494154\n",
      "Average loss at step 338400: 328.815858\n",
      "Average loss at step 338500: 329.961055\n",
      "Average loss at step 338600: 329.926824\n",
      "Average loss at step 338700: 331.197421\n",
      "Average loss at step 338800: 327.048548\n",
      "Average loss at step 338900: 332.216948\n",
      "Average loss at step 339000: 330.557384\n",
      "Average loss at step 339100: 324.116552\n",
      "Average loss at step 339200: 332.140853\n",
      "Average loss at step 339300: 326.688867\n",
      "Average loss at step 339400: 332.892822\n",
      "Average loss at step 339500: 325.565804\n",
      "Average loss at step 339600: 329.409652\n",
      "Average loss at step 339700: 331.173632\n",
      "Average loss at step 339800: 328.178029\n",
      "Average loss at step 339900: 327.697665\n",
      "Average loss at step 340000: 325.622761\n",
      "Graph 68: 16 nodes\n",
      "Average loss at step 340100: 347.119621\n",
      "Average loss at step 340200: 326.088484\n",
      "Average loss at step 340300: 323.593616\n",
      "Average loss at step 340400: 323.118459\n",
      "Average loss at step 340500: 323.279550\n",
      "Average loss at step 340600: 323.788650\n",
      "Average loss at step 340700: 325.663290\n",
      "Average loss at step 340800: 324.529953\n",
      "Average loss at step 340900: 324.246653\n",
      "Average loss at step 341000: 322.109290\n",
      "Average loss at step 341100: 319.430562\n",
      "Average loss at step 341200: 321.498899\n",
      "Average loss at step 341300: 321.002649\n",
      "Average loss at step 341400: 317.019992\n",
      "Average loss at step 341500: 320.177904\n",
      "Average loss at step 341600: 320.793644\n",
      "Average loss at step 341700: 322.978196\n",
      "Average loss at step 341800: 319.549864\n",
      "Average loss at step 341900: 324.249034\n",
      "Average loss at step 342000: 321.129676\n",
      "Average loss at step 342100: 318.760601\n",
      "Average loss at step 342200: 322.553540\n",
      "Average loss at step 342300: 322.356582\n",
      "Average loss at step 342400: 319.685928\n",
      "Average loss at step 342500: 317.982188\n",
      "Average loss at step 342600: 317.734257\n",
      "Average loss at step 342700: 317.576665\n",
      "Average loss at step 342800: 320.560256\n",
      "Average loss at step 342900: 319.166743\n",
      "Average loss at step 343000: 318.107283\n",
      "Average loss at step 343100: 316.159322\n",
      "Average loss at step 343200: 320.048502\n",
      "Average loss at step 343300: 320.518902\n",
      "Average loss at step 343400: 321.610667\n",
      "Average loss at step 343500: 320.098239\n",
      "Average loss at step 343600: 321.413952\n",
      "Average loss at step 343700: 319.465049\n",
      "Average loss at step 343800: 320.663220\n",
      "Average loss at step 343900: 319.815623\n",
      "Average loss at step 344000: 319.965472\n",
      "Average loss at step 344100: 318.843545\n",
      "Average loss at step 344200: 320.739345\n",
      "Average loss at step 344300: 317.051260\n",
      "Average loss at step 344400: 319.649456\n",
      "Average loss at step 344500: 319.895732\n",
      "Average loss at step 344600: 316.218706\n",
      "Average loss at step 344700: 318.953753\n",
      "Average loss at step 344800: 321.187538\n",
      "Average loss at step 344900: 321.821536\n",
      "Average loss at step 345000: 320.376929\n",
      "Graph 69: 36 nodes\n",
      "Average loss at step 345100: 345.929668\n",
      "Average loss at step 345200: 336.097978\n",
      "Average loss at step 345300: 336.228104\n",
      "Average loss at step 345400: 335.262644\n",
      "Average loss at step 345500: 334.525059\n",
      "Average loss at step 345600: 336.042419\n",
      "Average loss at step 345700: 333.478816\n",
      "Average loss at step 345800: 331.599562\n",
      "Average loss at step 345900: 334.803612\n",
      "Average loss at step 346000: 333.171309\n",
      "Average loss at step 346100: 337.343636\n",
      "Average loss at step 346200: 330.770302\n",
      "Average loss at step 346300: 331.272749\n",
      "Average loss at step 346400: 335.251508\n",
      "Average loss at step 346500: 331.963979\n",
      "Average loss at step 346600: 334.043615\n",
      "Average loss at step 346700: 332.220003\n",
      "Average loss at step 346800: 331.183137\n",
      "Average loss at step 346900: 332.171624\n",
      "Average loss at step 347000: 330.452258\n",
      "Average loss at step 347100: 331.044555\n",
      "Average loss at step 347200: 332.531934\n",
      "Average loss at step 347300: 337.923547\n",
      "Average loss at step 347400: 327.852407\n",
      "Average loss at step 347500: 335.005371\n",
      "Average loss at step 347600: 331.239404\n",
      "Average loss at step 347700: 332.061690\n",
      "Average loss at step 347800: 333.471391\n",
      "Average loss at step 347900: 332.624374\n",
      "Average loss at step 348000: 335.618377\n",
      "Average loss at step 348100: 332.677555\n",
      "Average loss at step 348200: 333.622091\n",
      "Average loss at step 348300: 334.867205\n",
      "Average loss at step 348400: 335.655068\n",
      "Average loss at step 348500: 332.325781\n",
      "Average loss at step 348600: 332.666093\n",
      "Average loss at step 348700: 335.794708\n",
      "Average loss at step 348800: 329.733181\n",
      "Average loss at step 348900: 334.268915\n",
      "Average loss at step 349000: 332.304807\n",
      "Average loss at step 349100: 329.945973\n",
      "Average loss at step 349200: 333.876451\n",
      "Average loss at step 349300: 334.131766\n",
      "Average loss at step 349400: 331.917474\n",
      "Average loss at step 349500: 331.264408\n",
      "Average loss at step 349600: 333.703032\n",
      "Average loss at step 349700: 332.555759\n",
      "Average loss at step 349800: 331.350612\n",
      "Average loss at step 349900: 335.135394\n",
      "Average loss at step 350000: 332.493434\n",
      "Graph 70: 20 nodes\n",
      "Average loss at step 350100: 340.587977\n",
      "Average loss at step 350200: 331.147331\n",
      "Average loss at step 350300: 330.046114\n",
      "Average loss at step 350400: 326.295317\n",
      "Average loss at step 350500: 327.264964\n",
      "Average loss at step 350600: 327.558836\n",
      "Average loss at step 350700: 326.147711\n",
      "Average loss at step 350800: 327.876944\n",
      "Average loss at step 350900: 327.322279\n",
      "Average loss at step 351000: 330.399758\n",
      "Average loss at step 351100: 325.592734\n",
      "Average loss at step 351200: 324.865616\n",
      "Average loss at step 351300: 322.187062\n",
      "Average loss at step 351400: 325.938684\n",
      "Average loss at step 351500: 326.680247\n",
      "Average loss at step 351600: 324.550448\n",
      "Average loss at step 351700: 323.651721\n",
      "Average loss at step 351800: 326.919032\n",
      "Average loss at step 351900: 322.395564\n",
      "Average loss at step 352000: 322.335882\n",
      "Average loss at step 352100: 326.727315\n",
      "Average loss at step 352200: 328.033581\n",
      "Average loss at step 352300: 326.123062\n",
      "Average loss at step 352400: 325.528970\n",
      "Average loss at step 352500: 322.296014\n",
      "Average loss at step 352600: 325.472595\n",
      "Average loss at step 352700: 321.450955\n",
      "Average loss at step 352800: 326.728306\n",
      "Average loss at step 352900: 324.550098\n",
      "Average loss at step 353000: 325.281149\n",
      "Average loss at step 353100: 323.574167\n",
      "Average loss at step 353200: 323.243812\n",
      "Average loss at step 353300: 321.936471\n",
      "Average loss at step 353400: 324.445216\n",
      "Average loss at step 353500: 323.251845\n",
      "Average loss at step 353600: 321.154056\n",
      "Average loss at step 353700: 325.073338\n",
      "Average loss at step 353800: 324.890662\n",
      "Average loss at step 353900: 321.571833\n",
      "Average loss at step 354000: 324.638212\n",
      "Average loss at step 354100: 324.370041\n",
      "Average loss at step 354200: 326.146498\n",
      "Average loss at step 354300: 322.693360\n",
      "Average loss at step 354400: 325.580587\n",
      "Average loss at step 354500: 322.727766\n",
      "Average loss at step 354600: 321.543928\n",
      "Average loss at step 354700: 319.493600\n",
      "Average loss at step 354800: 323.844942\n",
      "Average loss at step 354900: 327.641862\n",
      "Average loss at step 355000: 325.772043\n",
      "Time: 19.7931380272\n",
      "Graph 71: 23 nodes\n",
      "Average loss at step 355100: 328.626603\n",
      "Average loss at step 355200: 327.805012\n",
      "Average loss at step 355300: 323.018590\n",
      "Average loss at step 355400: 321.880248\n",
      "Average loss at step 355500: 323.621181\n",
      "Average loss at step 355600: 324.319256\n",
      "Average loss at step 355700: 325.723880\n",
      "Average loss at step 355800: 324.520251\n",
      "Average loss at step 355900: 319.256234\n",
      "Average loss at step 356000: 320.311410\n",
      "Average loss at step 356100: 325.629023\n",
      "Average loss at step 356200: 321.616723\n",
      "Average loss at step 356300: 325.040671\n",
      "Average loss at step 356400: 321.061628\n",
      "Average loss at step 356500: 320.902771\n",
      "Average loss at step 356600: 322.373733\n",
      "Average loss at step 356700: 321.341600\n",
      "Average loss at step 356800: 321.509388\n",
      "Average loss at step 356900: 322.594059\n",
      "Average loss at step 357000: 323.261263\n",
      "Average loss at step 357100: 322.899508\n",
      "Average loss at step 357200: 322.535736\n",
      "Average loss at step 357300: 324.187733\n",
      "Average loss at step 357400: 323.459257\n",
      "Average loss at step 357500: 318.487407\n",
      "Average loss at step 357600: 319.687195\n",
      "Average loss at step 357700: 323.574927\n",
      "Average loss at step 357800: 316.957600\n",
      "Average loss at step 357900: 322.379561\n",
      "Average loss at step 358000: 320.754490\n",
      "Average loss at step 358100: 321.599948\n",
      "Average loss at step 358200: 321.541539\n",
      "Average loss at step 358300: 317.180341\n",
      "Average loss at step 358400: 318.507605\n",
      "Average loss at step 358500: 320.826505\n",
      "Average loss at step 358600: 321.487532\n",
      "Average loss at step 358700: 319.299598\n",
      "Average loss at step 358800: 319.031864\n",
      "Average loss at step 358900: 319.392526\n",
      "Average loss at step 359000: 320.753966\n",
      "Average loss at step 359100: 322.071172\n",
      "Average loss at step 359200: 321.978315\n",
      "Average loss at step 359300: 324.524766\n",
      "Average loss at step 359400: 322.541900\n",
      "Average loss at step 359500: 320.369622\n",
      "Average loss at step 359600: 320.047683\n",
      "Average loss at step 359700: 318.725087\n",
      "Average loss at step 359800: 322.194598\n",
      "Average loss at step 359900: 320.462795\n",
      "Average loss at step 360000: 321.591509\n",
      "Graph 72: 28 nodes\n",
      "Average loss at step 360100: 340.875387\n",
      "Average loss at step 360200: 339.321110\n",
      "Average loss at step 360300: 331.803531\n",
      "Average loss at step 360400: 334.977552\n",
      "Average loss at step 360500: 330.092895\n",
      "Average loss at step 360600: 337.357016\n",
      "Average loss at step 360700: 336.801049\n",
      "Average loss at step 360800: 338.409086\n",
      "Average loss at step 360900: 334.787248\n",
      "Average loss at step 361000: 335.652836\n",
      "Average loss at step 361100: 334.598534\n",
      "Average loss at step 361200: 333.964114\n",
      "Average loss at step 361300: 332.576428\n",
      "Average loss at step 361400: 336.784019\n",
      "Average loss at step 361500: 328.935638\n",
      "Average loss at step 361600: 331.939603\n",
      "Average loss at step 361700: 329.042476\n",
      "Average loss at step 361800: 332.920661\n",
      "Average loss at step 361900: 338.031642\n",
      "Average loss at step 362000: 334.758112\n",
      "Average loss at step 362100: 330.074401\n",
      "Average loss at step 362200: 331.557718\n",
      "Average loss at step 362300: 334.153301\n",
      "Average loss at step 362400: 335.038870\n",
      "Average loss at step 362500: 334.989088\n",
      "Average loss at step 362600: 333.957757\n",
      "Average loss at step 362700: 334.077940\n",
      "Average loss at step 362800: 329.681419\n",
      "Average loss at step 362900: 331.266830\n",
      "Average loss at step 363000: 332.919727\n",
      "Average loss at step 363100: 333.564515\n",
      "Average loss at step 363200: 330.634548\n",
      "Average loss at step 363300: 331.744792\n",
      "Average loss at step 363400: 329.926213\n",
      "Average loss at step 363500: 329.828455\n",
      "Average loss at step 363600: 332.715120\n",
      "Average loss at step 363700: 332.789468\n",
      "Average loss at step 363800: 332.338112\n",
      "Average loss at step 363900: 333.383389\n",
      "Average loss at step 364000: 332.711813\n",
      "Average loss at step 364100: 332.320051\n",
      "Average loss at step 364200: 331.676827\n",
      "Average loss at step 364300: 335.627144\n",
      "Average loss at step 364400: 331.379765\n",
      "Average loss at step 364500: 335.086978\n",
      "Average loss at step 364600: 330.193002\n",
      "Average loss at step 364700: 335.234241\n",
      "Average loss at step 364800: 332.989334\n",
      "Average loss at step 364900: 333.028150\n",
      "Average loss at step 365000: 331.619677\n",
      "Graph 73: 29 nodes\n",
      "Average loss at step 365100: 339.743708\n",
      "Average loss at step 365200: 334.753471\n",
      "Average loss at step 365300: 334.818340\n",
      "Average loss at step 365400: 335.020468\n",
      "Average loss at step 365500: 332.105895\n",
      "Average loss at step 365600: 333.523430\n",
      "Average loss at step 365700: 332.655699\n",
      "Average loss at step 365800: 334.676470\n",
      "Average loss at step 365900: 332.885971\n",
      "Average loss at step 366000: 330.296841\n",
      "Average loss at step 366100: 331.931967\n",
      "Average loss at step 366200: 332.018035\n",
      "Average loss at step 366300: 331.081055\n",
      "Average loss at step 366400: 332.052860\n",
      "Average loss at step 366500: 330.516611\n",
      "Average loss at step 366600: 330.924354\n",
      "Average loss at step 366700: 331.644693\n",
      "Average loss at step 366800: 332.873770\n",
      "Average loss at step 366900: 332.996199\n",
      "Average loss at step 367000: 326.802637\n",
      "Average loss at step 367100: 332.767348\n",
      "Average loss at step 367200: 332.092834\n",
      "Average loss at step 367300: 331.015909\n",
      "Average loss at step 367400: 328.558441\n",
      "Average loss at step 367500: 331.232864\n",
      "Average loss at step 367600: 334.140129\n",
      "Average loss at step 367700: 329.891397\n",
      "Average loss at step 367800: 334.321630\n",
      "Average loss at step 367900: 332.746125\n",
      "Average loss at step 368000: 330.614306\n",
      "Average loss at step 368100: 327.486003\n",
      "Average loss at step 368200: 330.995357\n",
      "Average loss at step 368300: 329.182386\n",
      "Average loss at step 368400: 330.794576\n",
      "Average loss at step 368500: 329.614768\n",
      "Average loss at step 368600: 330.488087\n",
      "Average loss at step 368700: 330.096972\n",
      "Average loss at step 368800: 328.885255\n",
      "Average loss at step 368900: 333.529752\n",
      "Average loss at step 369000: 331.683688\n",
      "Average loss at step 369100: 330.590744\n",
      "Average loss at step 369200: 330.341278\n",
      "Average loss at step 369300: 332.300937\n",
      "Average loss at step 369400: 331.495402\n",
      "Average loss at step 369500: 330.034664\n",
      "Average loss at step 369600: 330.853279\n",
      "Average loss at step 369700: 333.109241\n",
      "Average loss at step 369800: 328.148992\n",
      "Average loss at step 369900: 334.126346\n",
      "Average loss at step 370000: 333.586087\n",
      "Graph 74: 9 nodes\n",
      "Average loss at step 370100: 321.363733\n",
      "Average loss at step 370200: 307.606978\n",
      "Average loss at step 370300: 299.341137\n",
      "Average loss at step 370400: 300.265516\n",
      "Average loss at step 370500: 299.011138\n",
      "Average loss at step 370600: 297.910719\n",
      "Average loss at step 370700: 301.982117\n",
      "Average loss at step 370800: 298.724705\n",
      "Average loss at step 370900: 294.041513\n",
      "Average loss at step 371000: 295.247689\n",
      "Average loss at step 371100: 297.784155\n",
      "Average loss at step 371200: 299.997334\n",
      "Average loss at step 371300: 299.135579\n",
      "Average loss at step 371400: 301.113394\n",
      "Average loss at step 371500: 293.431612\n",
      "Average loss at step 371600: 295.359698\n",
      "Average loss at step 371700: 295.578276\n",
      "Average loss at step 371800: 291.086949\n",
      "Average loss at step 371900: 294.194988\n",
      "Average loss at step 372000: 298.728673\n",
      "Average loss at step 372100: 293.077893\n",
      "Average loss at step 372200: 292.643128\n",
      "Average loss at step 372300: 296.831548\n",
      "Average loss at step 372400: 290.004061\n",
      "Average loss at step 372500: 298.349220\n",
      "Average loss at step 372600: 290.194007\n",
      "Average loss at step 372700: 295.343574\n",
      "Average loss at step 372800: 298.054929\n",
      "Average loss at step 372900: 296.134787\n",
      "Average loss at step 373000: 295.203948\n",
      "Average loss at step 373100: 293.360914\n",
      "Average loss at step 373200: 291.711269\n",
      "Average loss at step 373300: 298.969176\n",
      "Average loss at step 373400: 299.426136\n",
      "Average loss at step 373500: 294.354717\n",
      "Average loss at step 373600: 295.006641\n",
      "Average loss at step 373700: 295.559661\n",
      "Average loss at step 373800: 293.579800\n",
      "Average loss at step 373900: 291.626191\n",
      "Average loss at step 374000: 296.084741\n",
      "Average loss at step 374100: 297.064420\n",
      "Average loss at step 374200: 296.627440\n",
      "Average loss at step 374300: 301.518456\n",
      "Average loss at step 374400: 292.124182\n",
      "Average loss at step 374500: 287.987587\n",
      "Average loss at step 374600: 290.284751\n",
      "Average loss at step 374700: 296.183334\n",
      "Average loss at step 374800: 294.578185\n",
      "Average loss at step 374900: 296.519235\n",
      "Average loss at step 375000: 297.536336\n",
      "Graph 75: 23 nodes\n",
      "Average loss at step 375100: 345.425013\n",
      "Average loss at step 375200: 330.463635\n",
      "Average loss at step 375300: 334.413494\n",
      "Average loss at step 375400: 333.655008\n",
      "Average loss at step 375500: 330.298605\n",
      "Average loss at step 375600: 327.410485\n",
      "Average loss at step 375700: 330.535379\n",
      "Average loss at step 375800: 330.231291\n",
      "Average loss at step 375900: 322.393961\n",
      "Average loss at step 376000: 328.271316\n",
      "Average loss at step 376100: 328.090474\n",
      "Average loss at step 376200: 325.338017\n",
      "Average loss at step 376300: 327.229630\n",
      "Average loss at step 376400: 326.560233\n",
      "Average loss at step 376500: 324.929143\n",
      "Average loss at step 376600: 326.089173\n",
      "Average loss at step 376700: 329.795561\n",
      "Average loss at step 376800: 322.287919\n",
      "Average loss at step 376900: 324.441277\n",
      "Average loss at step 377000: 322.605366\n",
      "Average loss at step 377100: 327.803669\n",
      "Average loss at step 377200: 324.101530\n",
      "Average loss at step 377300: 325.076901\n",
      "Average loss at step 377400: 326.048790\n",
      "Average loss at step 377500: 325.450189\n",
      "Average loss at step 377600: 325.081774\n",
      "Average loss at step 377700: 327.027651\n",
      "Average loss at step 377800: 321.139336\n",
      "Average loss at step 377900: 326.225849\n",
      "Average loss at step 378000: 327.183888\n",
      "Average loss at step 378100: 327.450988\n",
      "Average loss at step 378200: 327.011001\n",
      "Average loss at step 378300: 328.266696\n",
      "Average loss at step 378400: 325.837185\n",
      "Average loss at step 378500: 323.466600\n",
      "Average loss at step 378600: 325.914922\n",
      "Average loss at step 378700: 322.523656\n",
      "Average loss at step 378800: 325.660240\n",
      "Average loss at step 378900: 327.651551\n",
      "Average loss at step 379000: 325.020390\n",
      "Average loss at step 379100: 326.561048\n",
      "Average loss at step 379200: 323.860499\n",
      "Average loss at step 379300: 322.539105\n",
      "Average loss at step 379400: 326.117619\n",
      "Average loss at step 379500: 323.262676\n",
      "Average loss at step 379600: 323.696293\n",
      "Average loss at step 379700: 325.217662\n",
      "Average loss at step 379800: 327.109705\n",
      "Average loss at step 379900: 324.546688\n",
      "Average loss at step 380000: 326.593108\n",
      "Graph 76: 12 nodes\n",
      "Average loss at step 380100: 341.168406\n",
      "Average loss at step 380200: 337.476883\n",
      "Average loss at step 380300: 333.305320\n",
      "Average loss at step 380400: 331.735832\n",
      "Average loss at step 380500: 334.420809\n",
      "Average loss at step 380600: 335.789221\n",
      "Average loss at step 380700: 332.804289\n",
      "Average loss at step 380800: 331.826159\n",
      "Average loss at step 380900: 331.872179\n",
      "Average loss at step 381000: 331.579616\n",
      "Average loss at step 381100: 333.301064\n",
      "Average loss at step 381200: 334.267037\n",
      "Average loss at step 381300: 328.927630\n",
      "Average loss at step 381400: 328.994136\n",
      "Average loss at step 381500: 331.232246\n",
      "Average loss at step 381600: 328.095400\n",
      "Average loss at step 381700: 330.371050\n",
      "Average loss at step 381800: 328.443839\n",
      "Average loss at step 381900: 330.714133\n",
      "Average loss at step 382000: 328.745578\n",
      "Average loss at step 382100: 331.914333\n",
      "Average loss at step 382200: 330.593127\n",
      "Average loss at step 382300: 331.743900\n",
      "Average loss at step 382400: 327.499561\n",
      "Average loss at step 382500: 333.369555\n",
      "Average loss at step 382600: 329.633900\n",
      "Average loss at step 382700: 326.919079\n",
      "Average loss at step 382800: 330.456561\n",
      "Average loss at step 382900: 330.169868\n",
      "Average loss at step 383000: 329.218324\n",
      "Average loss at step 383100: 327.943286\n",
      "Average loss at step 383200: 328.957273\n",
      "Average loss at step 383300: 330.372831\n",
      "Average loss at step 383400: 330.058461\n",
      "Average loss at step 383500: 330.305097\n",
      "Average loss at step 383600: 333.287869\n",
      "Average loss at step 383700: 330.613821\n",
      "Average loss at step 383800: 335.254311\n",
      "Average loss at step 383900: 331.490160\n",
      "Average loss at step 384000: 330.732074\n",
      "Average loss at step 384100: 328.902977\n",
      "Average loss at step 384200: 331.682303\n",
      "Average loss at step 384300: 328.307722\n",
      "Average loss at step 384400: 330.692422\n",
      "Average loss at step 384500: 330.271308\n",
      "Average loss at step 384600: 333.400521\n",
      "Average loss at step 384700: 328.047737\n",
      "Average loss at step 384800: 330.432531\n",
      "Average loss at step 384900: 328.331030\n",
      "Average loss at step 385000: 331.962155\n",
      "Graph 77: 21 nodes\n",
      "Average loss at step 385100: 338.522034\n",
      "Average loss at step 385200: 329.763424\n",
      "Average loss at step 385300: 329.373544\n",
      "Average loss at step 385400: 329.948502\n",
      "Average loss at step 385500: 328.156142\n",
      "Average loss at step 385600: 327.125896\n",
      "Average loss at step 385700: 324.599497\n",
      "Average loss at step 385800: 324.431649\n",
      "Average loss at step 385900: 324.786243\n",
      "Average loss at step 386000: 324.129847\n",
      "Average loss at step 386100: 325.895346\n",
      "Average loss at step 386200: 324.905672\n",
      "Average loss at step 386300: 326.671879\n",
      "Average loss at step 386400: 319.766989\n",
      "Average loss at step 386500: 322.589173\n",
      "Average loss at step 386600: 328.255583\n",
      "Average loss at step 386700: 329.015476\n",
      "Average loss at step 386800: 326.416782\n",
      "Average loss at step 386900: 322.481467\n",
      "Average loss at step 387000: 326.904357\n",
      "Average loss at step 387100: 325.946331\n",
      "Average loss at step 387200: 327.350518\n",
      "Average loss at step 387300: 327.766380\n",
      "Average loss at step 387400: 324.382705\n",
      "Average loss at step 387500: 326.031812\n",
      "Average loss at step 387600: 323.407308\n",
      "Average loss at step 387700: 321.900625\n",
      "Average loss at step 387800: 325.761876\n",
      "Average loss at step 387900: 329.510189\n",
      "Average loss at step 388000: 321.615617\n",
      "Average loss at step 388100: 321.253969\n",
      "Average loss at step 388200: 321.179814\n",
      "Average loss at step 388300: 324.070578\n",
      "Average loss at step 388400: 322.966754\n",
      "Average loss at step 388500: 327.534356\n",
      "Average loss at step 388600: 323.648605\n",
      "Average loss at step 388700: 327.466799\n",
      "Average loss at step 388800: 321.843996\n",
      "Average loss at step 388900: 324.524997\n",
      "Average loss at step 389000: 324.860935\n",
      "Average loss at step 389100: 325.951838\n",
      "Average loss at step 389200: 326.599657\n",
      "Average loss at step 389300: 322.997401\n",
      "Average loss at step 389400: 323.809573\n",
      "Average loss at step 389500: 326.218786\n",
      "Average loss at step 389600: 324.002540\n",
      "Average loss at step 389700: 323.447090\n",
      "Average loss at step 389800: 318.681233\n",
      "Average loss at step 389900: 321.292887\n",
      "Average loss at step 390000: 323.424539\n",
      "Graph 78: 39 nodes\n",
      "Average loss at step 390100: 343.621786\n",
      "Average loss at step 390200: 334.373846\n",
      "Average loss at step 390300: 334.141669\n",
      "Average loss at step 390400: 333.566111\n",
      "Average loss at step 390500: 336.101602\n",
      "Average loss at step 390600: 335.528646\n",
      "Average loss at step 390700: 334.606134\n",
      "Average loss at step 390800: 336.386409\n",
      "Average loss at step 390900: 334.538388\n",
      "Average loss at step 391000: 334.795729\n",
      "Average loss at step 391100: 330.892363\n",
      "Average loss at step 391200: 332.702201\n",
      "Average loss at step 391300: 332.000002\n",
      "Average loss at step 391400: 333.350855\n",
      "Average loss at step 391500: 334.127686\n",
      "Average loss at step 391600: 331.993444\n",
      "Average loss at step 391700: 331.934707\n",
      "Average loss at step 391800: 332.107829\n",
      "Average loss at step 391900: 334.705044\n",
      "Average loss at step 392000: 327.732076\n",
      "Average loss at step 392100: 333.239656\n",
      "Average loss at step 392200: 328.118305\n",
      "Average loss at step 392300: 332.147710\n",
      "Average loss at step 392400: 330.744747\n",
      "Average loss at step 392500: 336.672920\n",
      "Average loss at step 392600: 333.198515\n",
      "Average loss at step 392700: 331.314947\n",
      "Average loss at step 392800: 331.412063\n",
      "Average loss at step 392900: 335.319167\n",
      "Average loss at step 393000: 330.623882\n",
      "Average loss at step 393100: 332.333272\n",
      "Average loss at step 393200: 331.578963\n",
      "Average loss at step 393300: 326.651172\n",
      "Average loss at step 393400: 333.490581\n",
      "Average loss at step 393500: 329.509526\n",
      "Average loss at step 393600: 332.445836\n",
      "Average loss at step 393700: 332.233003\n",
      "Average loss at step 393800: 329.664273\n",
      "Average loss at step 393900: 327.335148\n",
      "Average loss at step 394000: 332.172185\n",
      "Average loss at step 394100: 333.435494\n",
      "Average loss at step 394200: 331.434857\n",
      "Average loss at step 394300: 329.954339\n",
      "Average loss at step 394400: 332.668608\n",
      "Average loss at step 394500: 328.202049\n",
      "Average loss at step 394600: 333.976776\n",
      "Average loss at step 394700: 335.323669\n",
      "Average loss at step 394800: 332.756688\n",
      "Average loss at step 394900: 328.909955\n",
      "Average loss at step 395000: 332.935073\n",
      "Graph 79: 15 nodes\n",
      "Average loss at step 395100: 335.328728\n",
      "Average loss at step 395200: 329.588758\n",
      "Average loss at step 395300: 332.352247\n",
      "Average loss at step 395400: 328.732080\n",
      "Average loss at step 395500: 328.555449\n",
      "Average loss at step 395600: 328.885063\n",
      "Average loss at step 395700: 329.468261\n",
      "Average loss at step 395800: 326.461136\n",
      "Average loss at step 395900: 325.433782\n",
      "Average loss at step 396000: 327.109621\n",
      "Average loss at step 396100: 326.312720\n",
      "Average loss at step 396200: 329.058707\n",
      "Average loss at step 396300: 327.380363\n",
      "Average loss at step 396400: 327.354787\n",
      "Average loss at step 396500: 324.963647\n",
      "Average loss at step 396600: 327.037032\n",
      "Average loss at step 396700: 324.241251\n",
      "Average loss at step 396800: 328.925548\n",
      "Average loss at step 396900: 325.258904\n",
      "Average loss at step 397000: 322.755488\n",
      "Average loss at step 397100: 327.225791\n",
      "Average loss at step 397200: 326.345135\n",
      "Average loss at step 397300: 324.553479\n",
      "Average loss at step 397400: 325.439539\n",
      "Average loss at step 397500: 324.581206\n",
      "Average loss at step 397600: 327.747155\n",
      "Average loss at step 397700: 322.669467\n",
      "Average loss at step 397800: 325.226342\n",
      "Average loss at step 397900: 328.421364\n",
      "Average loss at step 398000: 323.038778\n",
      "Average loss at step 398100: 326.324514\n",
      "Average loss at step 398200: 324.757608\n",
      "Average loss at step 398300: 325.830804\n",
      "Average loss at step 398400: 322.229956\n",
      "Average loss at step 398500: 322.887689\n",
      "Average loss at step 398600: 324.619840\n",
      "Average loss at step 398700: 324.606983\n",
      "Average loss at step 398800: 325.555631\n",
      "Average loss at step 398900: 322.405823\n",
      "Average loss at step 399000: 324.873081\n",
      "Average loss at step 399100: 326.080906\n",
      "Average loss at step 399200: 327.098117\n",
      "Average loss at step 399300: 322.502358\n",
      "Average loss at step 399400: 324.185338\n",
      "Average loss at step 399500: 326.008648\n",
      "Average loss at step 399600: 325.907891\n",
      "Average loss at step 399700: 326.012287\n",
      "Average loss at step 399800: 321.646594\n",
      "Average loss at step 399900: 323.763764\n",
      "Average loss at step 400000: 324.352058\n",
      "Graph 80: 39 nodes\n",
      "Average loss at step 400100: 353.562398\n",
      "Average loss at step 400200: 334.265667\n",
      "Average loss at step 400300: 334.719909\n",
      "Average loss at step 400400: 333.700350\n",
      "Average loss at step 400500: 332.798081\n",
      "Average loss at step 400600: 332.565893\n",
      "Average loss at step 400700: 336.116332\n",
      "Average loss at step 400800: 333.423205\n",
      "Average loss at step 400900: 335.394756\n",
      "Average loss at step 401000: 334.196540\n",
      "Average loss at step 401100: 331.183420\n",
      "Average loss at step 401200: 334.482813\n",
      "Average loss at step 401300: 329.324364\n",
      "Average loss at step 401400: 330.657568\n",
      "Average loss at step 401500: 335.164056\n",
      "Average loss at step 401600: 331.656504\n",
      "Average loss at step 401700: 332.889364\n",
      "Average loss at step 401800: 332.227527\n",
      "Average loss at step 401900: 329.007083\n",
      "Average loss at step 402000: 330.543150\n",
      "Average loss at step 402100: 333.202737\n",
      "Average loss at step 402200: 333.081373\n",
      "Average loss at step 402300: 330.801349\n",
      "Average loss at step 402400: 330.526512\n",
      "Average loss at step 402500: 330.978899\n",
      "Average loss at step 402600: 331.489076\n",
      "Average loss at step 402700: 337.317639\n",
      "Average loss at step 402800: 335.204657\n",
      "Average loss at step 402900: 334.233115\n",
      "Average loss at step 403000: 331.643608\n",
      "Average loss at step 403100: 331.208080\n",
      "Average loss at step 403200: 333.748136\n",
      "Average loss at step 403300: 335.424875\n",
      "Average loss at step 403400: 330.873849\n",
      "Average loss at step 403500: 330.182507\n",
      "Average loss at step 403600: 330.955047\n",
      "Average loss at step 403700: 330.042244\n",
      "Average loss at step 403800: 328.445657\n",
      "Average loss at step 403900: 330.773845\n",
      "Average loss at step 404000: 328.648802\n",
      "Average loss at step 404100: 332.994671\n",
      "Average loss at step 404200: 333.676014\n",
      "Average loss at step 404300: 329.323803\n",
      "Average loss at step 404400: 330.490240\n",
      "Average loss at step 404500: 332.688623\n",
      "Average loss at step 404600: 332.543571\n",
      "Average loss at step 404700: 333.948265\n",
      "Average loss at step 404800: 332.781205\n",
      "Average loss at step 404900: 333.562362\n",
      "Average loss at step 405000: 330.513612\n",
      "Time: 21.5095970631\n",
      "Graph 81: 21 nodes\n",
      "Average loss at step 405100: 350.006818\n",
      "Average loss at step 405200: 329.165715\n",
      "Average loss at step 405300: 330.496983\n",
      "Average loss at step 405400: 327.841797\n",
      "Average loss at step 405500: 328.056300\n",
      "Average loss at step 405600: 323.476132\n",
      "Average loss at step 405700: 322.408103\n",
      "Average loss at step 405800: 322.422558\n",
      "Average loss at step 405900: 327.246460\n",
      "Average loss at step 406000: 325.956529\n",
      "Average loss at step 406100: 325.868413\n",
      "Average loss at step 406200: 328.606951\n",
      "Average loss at step 406300: 322.665321\n",
      "Average loss at step 406400: 327.322452\n",
      "Average loss at step 406500: 323.119016\n",
      "Average loss at step 406600: 325.950447\n",
      "Average loss at step 406700: 326.486710\n",
      "Average loss at step 406800: 328.514282\n",
      "Average loss at step 406900: 327.291745\n",
      "Average loss at step 407000: 323.397165\n",
      "Average loss at step 407100: 326.659401\n",
      "Average loss at step 407200: 327.951215\n",
      "Average loss at step 407300: 324.022466\n",
      "Average loss at step 407400: 325.676037\n",
      "Average loss at step 407500: 326.890745\n",
      "Average loss at step 407600: 327.314639\n",
      "Average loss at step 407700: 325.401509\n",
      "Average loss at step 407800: 329.214888\n",
      "Average loss at step 407900: 325.582209\n",
      "Average loss at step 408000: 325.426929\n",
      "Average loss at step 408100: 325.288268\n",
      "Average loss at step 408200: 329.568519\n",
      "Average loss at step 408300: 326.757626\n",
      "Average loss at step 408400: 326.183733\n",
      "Average loss at step 408500: 325.189765\n",
      "Average loss at step 408600: 326.322587\n",
      "Average loss at step 408700: 324.384413\n",
      "Average loss at step 408800: 327.311856\n",
      "Average loss at step 408900: 325.479532\n",
      "Average loss at step 409000: 330.431213\n",
      "Average loss at step 409100: 325.199921\n",
      "Average loss at step 409200: 325.716516\n",
      "Average loss at step 409300: 323.494584\n",
      "Average loss at step 409400: 324.546584\n",
      "Average loss at step 409500: 324.353482\n",
      "Average loss at step 409600: 328.019577\n",
      "Average loss at step 409700: 325.450631\n",
      "Average loss at step 409800: 328.987513\n",
      "Average loss at step 409900: 327.364164\n",
      "Average loss at step 410000: 328.200053\n",
      "Graph 82: 22 nodes\n",
      "Average loss at step 410100: 340.868065\n",
      "Average loss at step 410200: 336.988281\n",
      "Average loss at step 410300: 328.877121\n",
      "Average loss at step 410400: 333.510002\n",
      "Average loss at step 410500: 334.659518\n",
      "Average loss at step 410600: 332.716231\n",
      "Average loss at step 410700: 330.640749\n",
      "Average loss at step 410800: 330.894843\n",
      "Average loss at step 410900: 329.296454\n",
      "Average loss at step 411000: 329.646426\n",
      "Average loss at step 411100: 329.978614\n",
      "Average loss at step 411200: 330.249097\n",
      "Average loss at step 411300: 330.924050\n",
      "Average loss at step 411400: 329.786630\n",
      "Average loss at step 411500: 329.808563\n",
      "Average loss at step 411600: 329.140075\n",
      "Average loss at step 411700: 328.070405\n",
      "Average loss at step 411800: 334.745008\n",
      "Average loss at step 411900: 328.602143\n",
      "Average loss at step 412000: 328.615397\n",
      "Average loss at step 412100: 329.671549\n",
      "Average loss at step 412200: 330.042362\n",
      "Average loss at step 412300: 331.972785\n",
      "Average loss at step 412400: 327.686960\n",
      "Average loss at step 412500: 328.904699\n",
      "Average loss at step 412600: 330.884078\n",
      "Average loss at step 412700: 329.407954\n",
      "Average loss at step 412800: 330.916231\n",
      "Average loss at step 412900: 329.726682\n",
      "Average loss at step 413000: 327.051863\n",
      "Average loss at step 413100: 326.194734\n",
      "Average loss at step 413200: 327.118635\n",
      "Average loss at step 413300: 328.985822\n",
      "Average loss at step 413400: 331.081536\n",
      "Average loss at step 413500: 329.229266\n",
      "Average loss at step 413600: 330.419537\n",
      "Average loss at step 413700: 327.378978\n",
      "Average loss at step 413800: 325.624561\n",
      "Average loss at step 413900: 328.964580\n",
      "Average loss at step 414000: 331.582647\n",
      "Average loss at step 414100: 330.607426\n",
      "Average loss at step 414200: 331.829186\n",
      "Average loss at step 414300: 331.160400\n",
      "Average loss at step 414400: 329.543984\n",
      "Average loss at step 414500: 330.249937\n",
      "Average loss at step 414600: 327.595508\n",
      "Average loss at step 414700: 328.578924\n",
      "Average loss at step 414800: 329.391433\n",
      "Average loss at step 414900: 326.614274\n",
      "Average loss at step 415000: 330.141880\n",
      "Graph 83: 26 nodes\n",
      "Average loss at step 415100: 339.099183\n",
      "Average loss at step 415200: 335.385152\n",
      "Average loss at step 415300: 335.265955\n",
      "Average loss at step 415400: 333.933040\n",
      "Average loss at step 415500: 334.266926\n",
      "Average loss at step 415600: 334.778988\n",
      "Average loss at step 415700: 337.075845\n",
      "Average loss at step 415800: 337.770283\n",
      "Average loss at step 415900: 333.757608\n",
      "Average loss at step 416000: 334.940162\n",
      "Average loss at step 416100: 333.317715\n",
      "Average loss at step 416200: 334.066181\n",
      "Average loss at step 416300: 330.803885\n",
      "Average loss at step 416400: 333.141049\n",
      "Average loss at step 416500: 335.584152\n",
      "Average loss at step 416600: 330.137950\n",
      "Average loss at step 416700: 329.449208\n",
      "Average loss at step 416800: 333.460202\n",
      "Average loss at step 416900: 330.423971\n",
      "Average loss at step 417000: 335.546859\n",
      "Average loss at step 417100: 333.965581\n",
      "Average loss at step 417200: 334.695840\n",
      "Average loss at step 417300: 336.688909\n",
      "Average loss at step 417400: 333.593928\n",
      "Average loss at step 417500: 336.093086\n",
      "Average loss at step 417600: 331.087372\n",
      "Average loss at step 417700: 331.132980\n",
      "Average loss at step 417800: 331.917133\n",
      "Average loss at step 417900: 334.099391\n",
      "Average loss at step 418000: 333.733149\n",
      "Average loss at step 418100: 332.736496\n",
      "Average loss at step 418200: 332.166552\n",
      "Average loss at step 418300: 331.362133\n",
      "Average loss at step 418400: 332.766465\n",
      "Average loss at step 418500: 332.497854\n",
      "Average loss at step 418600: 331.792030\n",
      "Average loss at step 418700: 331.570194\n",
      "Average loss at step 418800: 332.177105\n",
      "Average loss at step 418900: 332.981194\n",
      "Average loss at step 419000: 331.651730\n",
      "Average loss at step 419100: 331.418361\n",
      "Average loss at step 419200: 330.474000\n",
      "Average loss at step 419300: 333.903893\n",
      "Average loss at step 419400: 331.955491\n",
      "Average loss at step 419500: 333.698146\n",
      "Average loss at step 419600: 332.995238\n",
      "Average loss at step 419700: 332.598367\n",
      "Average loss at step 419800: 332.942411\n",
      "Average loss at step 419900: 334.462246\n",
      "Average loss at step 420000: 336.419043\n",
      "Graph 84: 22 nodes\n",
      "Average loss at step 420100: 344.408164\n",
      "Average loss at step 420200: 336.831489\n",
      "Average loss at step 420300: 339.302861\n",
      "Average loss at step 420400: 334.471898\n",
      "Average loss at step 420500: 332.956141\n",
      "Average loss at step 420600: 336.781526\n",
      "Average loss at step 420700: 336.524575\n",
      "Average loss at step 420800: 335.906754\n",
      "Average loss at step 420900: 336.396812\n",
      "Average loss at step 421000: 337.417537\n",
      "Average loss at step 421100: 331.602673\n",
      "Average loss at step 421200: 332.608301\n",
      "Average loss at step 421300: 333.531445\n",
      "Average loss at step 421400: 333.854187\n",
      "Average loss at step 421500: 330.887586\n",
      "Average loss at step 421600: 337.879430\n",
      "Average loss at step 421700: 335.547372\n",
      "Average loss at step 421800: 335.153815\n",
      "Average loss at step 421900: 336.699408\n",
      "Average loss at step 422000: 336.031677\n",
      "Average loss at step 422100: 333.306013\n",
      "Average loss at step 422200: 331.069374\n",
      "Average loss at step 422300: 334.640024\n",
      "Average loss at step 422400: 334.529383\n",
      "Average loss at step 422500: 334.083674\n",
      "Average loss at step 422600: 338.242794\n",
      "Average loss at step 422700: 336.197451\n",
      "Average loss at step 422800: 331.674376\n",
      "Average loss at step 422900: 335.415048\n",
      "Average loss at step 423000: 335.838794\n",
      "Average loss at step 423100: 333.449507\n",
      "Average loss at step 423200: 336.542985\n",
      "Average loss at step 423300: 335.970042\n",
      "Average loss at step 423400: 332.250142\n",
      "Average loss at step 423500: 334.637807\n",
      "Average loss at step 423600: 332.578304\n",
      "Average loss at step 423700: 332.323461\n",
      "Average loss at step 423800: 336.544185\n",
      "Average loss at step 423900: 334.605461\n",
      "Average loss at step 424000: 333.997670\n",
      "Average loss at step 424100: 334.779531\n",
      "Average loss at step 424200: 333.894732\n",
      "Average loss at step 424300: 333.070408\n",
      "Average loss at step 424400: 334.491624\n",
      "Average loss at step 424500: 333.878592\n",
      "Average loss at step 424600: 335.901522\n",
      "Average loss at step 424700: 334.706821\n",
      "Average loss at step 424800: 332.771946\n",
      "Average loss at step 424900: 333.413632\n",
      "Average loss at step 425000: 335.218498\n",
      "Graph 85: 21 nodes\n",
      "Average loss at step 425100: 340.372265\n",
      "Average loss at step 425200: 330.688407\n",
      "Average loss at step 425300: 333.302700\n",
      "Average loss at step 425400: 333.512704\n",
      "Average loss at step 425500: 330.810458\n",
      "Average loss at step 425600: 329.628740\n",
      "Average loss at step 425700: 333.215260\n",
      "Average loss at step 425800: 331.633241\n",
      "Average loss at step 425900: 333.614316\n",
      "Average loss at step 426000: 331.707767\n",
      "Average loss at step 426100: 328.715728\n",
      "Average loss at step 426200: 330.945301\n",
      "Average loss at step 426300: 330.761446\n",
      "Average loss at step 426400: 331.795439\n",
      "Average loss at step 426500: 330.053699\n",
      "Average loss at step 426600: 330.003056\n",
      "Average loss at step 426700: 331.089874\n",
      "Average loss at step 426800: 329.546826\n",
      "Average loss at step 426900: 330.839473\n",
      "Average loss at step 427000: 329.314224\n",
      "Average loss at step 427100: 330.179976\n",
      "Average loss at step 427200: 329.259543\n",
      "Average loss at step 427300: 328.952088\n",
      "Average loss at step 427400: 330.776945\n",
      "Average loss at step 427500: 330.022439\n",
      "Average loss at step 427600: 331.020928\n",
      "Average loss at step 427700: 332.182601\n",
      "Average loss at step 427800: 328.906801\n",
      "Average loss at step 427900: 328.303960\n",
      "Average loss at step 428000: 332.601170\n",
      "Average loss at step 428100: 329.724316\n",
      "Average loss at step 428200: 325.755643\n",
      "Average loss at step 428300: 333.710634\n",
      "Average loss at step 428400: 331.614388\n",
      "Average loss at step 428500: 330.381420\n",
      "Average loss at step 428600: 330.060534\n",
      "Average loss at step 428700: 327.479922\n",
      "Average loss at step 428800: 329.502348\n",
      "Average loss at step 428900: 328.528041\n",
      "Average loss at step 429000: 331.982243\n",
      "Average loss at step 429100: 329.085245\n",
      "Average loss at step 429200: 332.055718\n",
      "Average loss at step 429300: 327.629373\n",
      "Average loss at step 429400: 330.552594\n",
      "Average loss at step 429500: 332.642793\n",
      "Average loss at step 429600: 331.071709\n",
      "Average loss at step 429700: 332.155949\n",
      "Average loss at step 429800: 331.853071\n",
      "Average loss at step 429900: 330.390169\n",
      "Average loss at step 430000: 326.200077\n",
      "Graph 86: 20 nodes\n",
      "Average loss at step 430100: 336.152247\n",
      "Average loss at step 430200: 330.968784\n",
      "Average loss at step 430300: 330.478599\n",
      "Average loss at step 430400: 328.591427\n",
      "Average loss at step 430500: 327.443779\n",
      "Average loss at step 430600: 331.137482\n",
      "Average loss at step 430700: 330.383962\n",
      "Average loss at step 430800: 331.057765\n",
      "Average loss at step 430900: 329.751415\n",
      "Average loss at step 431000: 328.679113\n",
      "Average loss at step 431100: 328.767543\n",
      "Average loss at step 431200: 326.238400\n",
      "Average loss at step 431300: 332.081225\n",
      "Average loss at step 431400: 331.167670\n",
      "Average loss at step 431500: 329.374844\n",
      "Average loss at step 431600: 327.300748\n",
      "Average loss at step 431700: 324.232248\n",
      "Average loss at step 431800: 330.136291\n",
      "Average loss at step 431900: 328.465681\n",
      "Average loss at step 432000: 330.028337\n",
      "Average loss at step 432100: 329.984762\n",
      "Average loss at step 432200: 329.780811\n",
      "Average loss at step 432300: 326.231028\n",
      "Average loss at step 432400: 326.882357\n",
      "Average loss at step 432500: 330.116363\n",
      "Average loss at step 432600: 329.740966\n",
      "Average loss at step 432700: 329.218701\n",
      "Average loss at step 432800: 328.690892\n",
      "Average loss at step 432900: 328.198382\n",
      "Average loss at step 433000: 329.437932\n",
      "Average loss at step 433100: 327.657065\n",
      "Average loss at step 433200: 329.990422\n",
      "Average loss at step 433300: 330.809617\n",
      "Average loss at step 433400: 327.486214\n",
      "Average loss at step 433500: 329.263396\n",
      "Average loss at step 433600: 331.050631\n",
      "Average loss at step 433700: 330.145479\n",
      "Average loss at step 433800: 329.270409\n",
      "Average loss at step 433900: 325.199444\n",
      "Average loss at step 434000: 330.329608\n",
      "Average loss at step 434100: 327.739414\n",
      "Average loss at step 434200: 329.384712\n",
      "Average loss at step 434300: 324.347517\n",
      "Average loss at step 434400: 330.784650\n",
      "Average loss at step 434500: 328.529751\n",
      "Average loss at step 434600: 329.204585\n",
      "Average loss at step 434700: 327.589712\n",
      "Average loss at step 434800: 326.960039\n",
      "Average loss at step 434900: 332.436337\n",
      "Average loss at step 435000: 330.008963\n",
      "Graph 87: 25 nodes\n",
      "Average loss at step 435100: 336.513604\n",
      "Average loss at step 435200: 335.126992\n",
      "Average loss at step 435300: 333.006764\n",
      "Average loss at step 435400: 333.236232\n",
      "Average loss at step 435500: 331.936276\n",
      "Average loss at step 435600: 328.248643\n",
      "Average loss at step 435700: 333.814262\n",
      "Average loss at step 435800: 335.166603\n",
      "Average loss at step 435900: 329.282262\n",
      "Average loss at step 436000: 328.674599\n",
      "Average loss at step 436100: 331.532312\n",
      "Average loss at step 436200: 330.795193\n",
      "Average loss at step 436300: 330.010215\n",
      "Average loss at step 436400: 326.675874\n",
      "Average loss at step 436500: 331.289516\n",
      "Average loss at step 436600: 331.617270\n",
      "Average loss at step 436700: 329.173902\n",
      "Average loss at step 436800: 331.823485\n",
      "Average loss at step 436900: 328.143249\n",
      "Average loss at step 437000: 322.298649\n",
      "Average loss at step 437100: 327.408569\n",
      "Average loss at step 437200: 330.388027\n",
      "Average loss at step 437300: 329.820633\n",
      "Average loss at step 437400: 329.341175\n",
      "Average loss at step 437500: 328.397388\n",
      "Average loss at step 437600: 332.300450\n",
      "Average loss at step 437700: 331.734399\n",
      "Average loss at step 437800: 333.400717\n",
      "Average loss at step 437900: 334.360405\n",
      "Average loss at step 438000: 325.219343\n",
      "Average loss at step 438100: 327.168578\n",
      "Average loss at step 438200: 331.139071\n",
      "Average loss at step 438300: 330.950434\n",
      "Average loss at step 438400: 332.489947\n",
      "Average loss at step 438500: 331.231194\n",
      "Average loss at step 438600: 328.922580\n",
      "Average loss at step 438700: 333.975312\n",
      "Average loss at step 438800: 329.508617\n",
      "Average loss at step 438900: 328.970834\n",
      "Average loss at step 439000: 331.847077\n",
      "Average loss at step 439100: 326.035133\n",
      "Average loss at step 439200: 332.093419\n",
      "Average loss at step 439300: 332.154701\n",
      "Average loss at step 439400: 329.620605\n",
      "Average loss at step 439500: 329.839259\n",
      "Average loss at step 439600: 330.338971\n",
      "Average loss at step 439700: 332.418886\n",
      "Average loss at step 439800: 327.585804\n",
      "Average loss at step 439900: 328.758652\n",
      "Average loss at step 440000: 329.637880\n",
      "Graph 88: 22 nodes\n",
      "Average loss at step 440100: 337.614161\n",
      "Average loss at step 440200: 329.477066\n",
      "Average loss at step 440300: 334.289879\n",
      "Average loss at step 440400: 331.144347\n",
      "Average loss at step 440500: 331.339780\n",
      "Average loss at step 440600: 328.608053\n",
      "Average loss at step 440700: 334.153397\n",
      "Average loss at step 440800: 331.418617\n",
      "Average loss at step 440900: 337.188860\n",
      "Average loss at step 441000: 325.665329\n",
      "Average loss at step 441100: 330.488377\n",
      "Average loss at step 441200: 330.196983\n",
      "Average loss at step 441300: 331.255870\n",
      "Average loss at step 441400: 329.551643\n",
      "Average loss at step 441500: 328.166257\n",
      "Average loss at step 441600: 327.120375\n",
      "Average loss at step 441700: 327.562161\n",
      "Average loss at step 441800: 331.227385\n",
      "Average loss at step 441900: 327.798604\n",
      "Average loss at step 442000: 328.559484\n",
      "Average loss at step 442100: 329.014572\n",
      "Average loss at step 442200: 328.010118\n",
      "Average loss at step 442300: 331.418296\n",
      "Average loss at step 442400: 330.245628\n",
      "Average loss at step 442500: 329.437511\n",
      "Average loss at step 442600: 330.004670\n",
      "Average loss at step 442700: 330.040032\n",
      "Average loss at step 442800: 333.341399\n",
      "Average loss at step 442900: 329.412102\n",
      "Average loss at step 443000: 328.633370\n",
      "Average loss at step 443100: 333.239648\n",
      "Average loss at step 443200: 329.485376\n",
      "Average loss at step 443300: 327.986525\n",
      "Average loss at step 443400: 329.288947\n",
      "Average loss at step 443500: 327.984137\n",
      "Average loss at step 443600: 326.669991\n",
      "Average loss at step 443700: 326.709406\n",
      "Average loss at step 443800: 330.931221\n",
      "Average loss at step 443900: 331.765176\n",
      "Average loss at step 444000: 329.098526\n",
      "Average loss at step 444100: 327.395490\n",
      "Average loss at step 444200: 332.245901\n",
      "Average loss at step 444300: 329.210889\n",
      "Average loss at step 444400: 330.947298\n",
      "Average loss at step 444500: 332.380724\n",
      "Average loss at step 444600: 329.234737\n",
      "Average loss at step 444700: 328.353272\n",
      "Average loss at step 444800: 330.530432\n",
      "Average loss at step 444900: 333.515996\n",
      "Average loss at step 445000: 327.331537\n",
      "Graph 89: 27 nodes\n",
      "Average loss at step 445100: 336.453591\n",
      "Average loss at step 445200: 326.216880\n",
      "Average loss at step 445300: 328.108230\n",
      "Average loss at step 445400: 326.785418\n",
      "Average loss at step 445500: 325.151097\n",
      "Average loss at step 445600: 323.204259\n",
      "Average loss at step 445700: 323.168753\n",
      "Average loss at step 445800: 323.056262\n",
      "Average loss at step 445900: 322.555757\n",
      "Average loss at step 446000: 325.433369\n",
      "Average loss at step 446100: 324.820863\n",
      "Average loss at step 446200: 325.246930\n",
      "Average loss at step 446300: 324.042204\n",
      "Average loss at step 446400: 323.021621\n",
      "Average loss at step 446500: 325.255916\n",
      "Average loss at step 446600: 321.466158\n",
      "Average loss at step 446700: 325.972792\n",
      "Average loss at step 446800: 320.077189\n",
      "Average loss at step 446900: 324.359075\n",
      "Average loss at step 447000: 320.558049\n",
      "Average loss at step 447100: 323.923816\n",
      "Average loss at step 447200: 321.954270\n",
      "Average loss at step 447300: 318.828956\n",
      "Average loss at step 447400: 320.542563\n",
      "Average loss at step 447500: 323.967199\n",
      "Average loss at step 447600: 324.362537\n",
      "Average loss at step 447700: 326.463104\n",
      "Average loss at step 447800: 325.514033\n",
      "Average loss at step 447900: 320.183789\n",
      "Average loss at step 448000: 320.550050\n",
      "Average loss at step 448100: 324.424154\n",
      "Average loss at step 448200: 323.694027\n",
      "Average loss at step 448300: 319.085904\n",
      "Average loss at step 448400: 322.588807\n",
      "Average loss at step 448500: 319.498736\n",
      "Average loss at step 448600: 322.771003\n",
      "Average loss at step 448700: 318.411439\n",
      "Average loss at step 448800: 326.194955\n",
      "Average loss at step 448900: 321.760649\n",
      "Average loss at step 449000: 322.325919\n",
      "Average loss at step 449100: 318.694817\n",
      "Average loss at step 449200: 319.532018\n",
      "Average loss at step 449300: 317.246557\n",
      "Average loss at step 449400: 323.598619\n",
      "Average loss at step 449500: 328.544794\n",
      "Average loss at step 449600: 324.426618\n",
      "Average loss at step 449700: 321.717531\n",
      "Average loss at step 449800: 323.321688\n",
      "Average loss at step 449900: 320.845033\n",
      "Average loss at step 450000: 326.213598\n",
      "Graph 90: 16 nodes\n",
      "Average loss at step 450100: 351.524713\n",
      "Average loss at step 450200: 344.257526\n",
      "Average loss at step 450300: 345.395796\n",
      "Average loss at step 450400: 344.680359\n",
      "Average loss at step 450500: 340.984127\n",
      "Average loss at step 450600: 341.449304\n",
      "Average loss at step 450700: 343.985448\n",
      "Average loss at step 450800: 338.582002\n",
      "Average loss at step 450900: 338.256048\n",
      "Average loss at step 451000: 339.904346\n",
      "Average loss at step 451100: 340.129242\n",
      "Average loss at step 451200: 344.266168\n",
      "Average loss at step 451300: 342.602492\n",
      "Average loss at step 451400: 341.553211\n",
      "Average loss at step 451500: 337.965606\n",
      "Average loss at step 451600: 338.327545\n",
      "Average loss at step 451700: 337.698189\n",
      "Average loss at step 451800: 336.785815\n",
      "Average loss at step 451900: 339.131549\n",
      "Average loss at step 452000: 341.901829\n",
      "Average loss at step 452100: 343.491696\n",
      "Average loss at step 452200: 338.651141\n",
      "Average loss at step 452300: 334.673791\n",
      "Average loss at step 452400: 341.386726\n",
      "Average loss at step 452500: 338.591160\n",
      "Average loss at step 452600: 338.505508\n",
      "Average loss at step 452700: 341.954770\n",
      "Average loss at step 452800: 337.767132\n",
      "Average loss at step 452900: 342.885475\n",
      "Average loss at step 453000: 335.797690\n",
      "Average loss at step 453100: 337.268654\n",
      "Average loss at step 453200: 338.059628\n",
      "Average loss at step 453300: 342.996611\n",
      "Average loss at step 453400: 339.847373\n",
      "Average loss at step 453500: 339.469508\n",
      "Average loss at step 453600: 337.705180\n",
      "Average loss at step 453700: 338.400438\n",
      "Average loss at step 453800: 336.588532\n",
      "Average loss at step 453900: 339.815196\n",
      "Average loss at step 454000: 339.697781\n",
      "Average loss at step 454100: 338.193053\n",
      "Average loss at step 454200: 336.480691\n",
      "Average loss at step 454300: 339.315888\n",
      "Average loss at step 454400: 338.516038\n",
      "Average loss at step 454500: 340.573958\n",
      "Average loss at step 454600: 339.815510\n",
      "Average loss at step 454700: 339.701475\n",
      "Average loss at step 454800: 338.803404\n",
      "Average loss at step 454900: 336.619537\n",
      "Average loss at step 455000: 339.762676\n",
      "Time: 21.6044359207\n",
      "Graph 91: 36 nodes\n",
      "Average loss at step 455100: 328.337891\n",
      "Average loss at step 455200: 324.720616\n",
      "Average loss at step 455300: 319.792273\n",
      "Average loss at step 455400: 324.441686\n",
      "Average loss at step 455500: 319.085232\n",
      "Average loss at step 455600: 321.683585\n",
      "Average loss at step 455700: 321.258457\n",
      "Average loss at step 455800: 319.013031\n",
      "Average loss at step 455900: 321.137290\n",
      "Average loss at step 456000: 323.356867\n",
      "Average loss at step 456100: 325.743369\n",
      "Average loss at step 456200: 325.546618\n",
      "Average loss at step 456300: 322.814481\n",
      "Average loss at step 456400: 321.623779\n",
      "Average loss at step 456500: 320.462428\n",
      "Average loss at step 456600: 320.155223\n",
      "Average loss at step 456700: 322.729552\n",
      "Average loss at step 456800: 323.209410\n",
      "Average loss at step 456900: 318.433214\n",
      "Average loss at step 457000: 324.664186\n",
      "Average loss at step 457100: 319.756850\n",
      "Average loss at step 457200: 317.028763\n",
      "Average loss at step 457300: 319.775547\n",
      "Average loss at step 457400: 321.179807\n",
      "Average loss at step 457500: 321.578270\n",
      "Average loss at step 457600: 323.488028\n",
      "Average loss at step 457700: 319.146948\n",
      "Average loss at step 457800: 322.385293\n",
      "Average loss at step 457900: 323.599241\n",
      "Average loss at step 458000: 324.896849\n",
      "Average loss at step 458100: 320.452773\n",
      "Average loss at step 458200: 317.532522\n",
      "Average loss at step 458300: 317.608822\n",
      "Average loss at step 458400: 318.395202\n",
      "Average loss at step 458500: 319.723128\n",
      "Average loss at step 458600: 318.482729\n",
      "Average loss at step 458700: 323.552815\n",
      "Average loss at step 458800: 320.667302\n",
      "Average loss at step 458900: 321.319923\n",
      "Average loss at step 459000: 318.369570\n",
      "Average loss at step 459100: 317.152692\n",
      "Average loss at step 459200: 318.622123\n",
      "Average loss at step 459300: 318.597766\n",
      "Average loss at step 459400: 316.430931\n",
      "Average loss at step 459500: 321.839170\n",
      "Average loss at step 459600: 318.934844\n",
      "Average loss at step 459700: 323.515926\n",
      "Average loss at step 459800: 320.426969\n",
      "Average loss at step 459900: 320.425065\n",
      "Average loss at step 460000: 322.009850\n",
      "Graph 92: 79 nodes\n",
      "Average loss at step 460100: 335.339447\n",
      "Average loss at step 460200: 335.792923\n",
      "Average loss at step 460300: 335.255245\n",
      "Average loss at step 460400: 331.433935\n",
      "Average loss at step 460500: 330.975548\n",
      "Average loss at step 460600: 330.138052\n",
      "Average loss at step 460700: 330.247767\n",
      "Average loss at step 460800: 329.204459\n",
      "Average loss at step 460900: 329.600629\n",
      "Average loss at step 461000: 331.924049\n",
      "Average loss at step 461100: 330.507367\n",
      "Average loss at step 461200: 327.388697\n",
      "Average loss at step 461300: 329.366273\n",
      "Average loss at step 461400: 330.119065\n",
      "Average loss at step 461500: 327.347868\n",
      "Average loss at step 461600: 328.279005\n",
      "Average loss at step 461700: 326.314170\n",
      "Average loss at step 461800: 331.616776\n",
      "Average loss at step 461900: 329.471372\n",
      "Average loss at step 462000: 329.954729\n",
      "Average loss at step 462100: 330.593801\n",
      "Average loss at step 462200: 330.244781\n",
      "Average loss at step 462300: 328.161552\n",
      "Average loss at step 462400: 326.385595\n",
      "Average loss at step 462500: 328.888475\n",
      "Average loss at step 462600: 329.595296\n",
      "Average loss at step 462700: 327.781643\n",
      "Average loss at step 462800: 328.002159\n",
      "Average loss at step 462900: 325.053399\n",
      "Average loss at step 463000: 328.573437\n",
      "Average loss at step 463100: 327.836741\n",
      "Average loss at step 463200: 329.583623\n",
      "Average loss at step 463300: 329.713561\n",
      "Average loss at step 463400: 328.962909\n",
      "Average loss at step 463500: 325.480089\n",
      "Average loss at step 463600: 329.491199\n",
      "Average loss at step 463700: 327.645690\n",
      "Average loss at step 463800: 329.392886\n",
      "Average loss at step 463900: 329.717689\n",
      "Average loss at step 464000: 323.881084\n",
      "Average loss at step 464100: 327.936812\n",
      "Average loss at step 464200: 325.924157\n",
      "Average loss at step 464300: 331.259327\n",
      "Average loss at step 464400: 328.713675\n",
      "Average loss at step 464500: 328.166674\n",
      "Average loss at step 464600: 328.947679\n",
      "Average loss at step 464700: 328.473915\n",
      "Average loss at step 464800: 332.917407\n",
      "Average loss at step 464900: 328.732541\n",
      "Average loss at step 465000: 329.102161\n",
      "Graph 93: 27 nodes\n",
      "Average loss at step 465100: 348.955964\n",
      "Average loss at step 465200: 334.299472\n",
      "Average loss at step 465300: 333.301687\n",
      "Average loss at step 465400: 330.038806\n",
      "Average loss at step 465500: 330.866629\n",
      "Average loss at step 465600: 333.775476\n",
      "Average loss at step 465700: 332.728794\n",
      "Average loss at step 465800: 326.518939\n",
      "Average loss at step 465900: 332.739135\n",
      "Average loss at step 466000: 330.072669\n",
      "Average loss at step 466100: 331.497059\n",
      "Average loss at step 466200: 330.457338\n",
      "Average loss at step 466300: 335.720576\n",
      "Average loss at step 466400: 332.993776\n",
      "Average loss at step 466500: 332.034232\n",
      "Average loss at step 466600: 329.199322\n",
      "Average loss at step 466700: 327.802856\n",
      "Average loss at step 466800: 328.904755\n",
      "Average loss at step 466900: 326.898965\n",
      "Average loss at step 467000: 328.800216\n",
      "Average loss at step 467100: 328.616713\n",
      "Average loss at step 467200: 332.025349\n",
      "Average loss at step 467300: 328.038533\n",
      "Average loss at step 467400: 326.692575\n",
      "Average loss at step 467500: 326.211565\n",
      "Average loss at step 467600: 326.876645\n",
      "Average loss at step 467700: 330.958393\n",
      "Average loss at step 467800: 330.318002\n",
      "Average loss at step 467900: 328.457360\n",
      "Average loss at step 468000: 329.691119\n",
      "Average loss at step 468100: 330.639254\n",
      "Average loss at step 468200: 330.607635\n",
      "Average loss at step 468300: 327.808562\n",
      "Average loss at step 468400: 332.198051\n",
      "Average loss at step 468500: 326.231601\n",
      "Average loss at step 468600: 329.348451\n",
      "Average loss at step 468700: 329.475825\n",
      "Average loss at step 468800: 328.116260\n",
      "Average loss at step 468900: 329.557939\n",
      "Average loss at step 469000: 326.806813\n",
      "Average loss at step 469100: 325.057901\n",
      "Average loss at step 469200: 328.577782\n",
      "Average loss at step 469300: 330.840532\n",
      "Average loss at step 469400: 329.352788\n",
      "Average loss at step 469500: 330.440806\n",
      "Average loss at step 469600: 331.593905\n",
      "Average loss at step 469700: 325.034947\n",
      "Average loss at step 469800: 330.462305\n",
      "Average loss at step 469900: 327.057014\n",
      "Average loss at step 470000: 330.319818\n",
      "Graph 94: 42 nodes\n",
      "Average loss at step 470100: 355.775044\n",
      "Average loss at step 470200: 330.559533\n",
      "Average loss at step 470300: 326.396951\n",
      "Average loss at step 470400: 329.923915\n",
      "Average loss at step 470500: 325.739251\n",
      "Average loss at step 470600: 326.854341\n",
      "Average loss at step 470700: 329.920166\n",
      "Average loss at step 470800: 328.471965\n",
      "Average loss at step 470900: 323.812265\n",
      "Average loss at step 471000: 328.907964\n",
      "Average loss at step 471100: 328.027711\n",
      "Average loss at step 471200: 325.952374\n",
      "Average loss at step 471300: 328.590505\n",
      "Average loss at step 471400: 325.423168\n",
      "Average loss at step 471500: 331.310633\n",
      "Average loss at step 471600: 325.033704\n",
      "Average loss at step 471700: 327.955249\n",
      "Average loss at step 471800: 326.394919\n",
      "Average loss at step 471900: 324.766913\n",
      "Average loss at step 472000: 330.796405\n",
      "Average loss at step 472100: 324.473140\n",
      "Average loss at step 472200: 330.559580\n",
      "Average loss at step 472300: 322.669585\n",
      "Average loss at step 472400: 325.962797\n",
      "Average loss at step 472500: 323.890276\n",
      "Average loss at step 472600: 321.591550\n",
      "Average loss at step 472700: 326.070797\n",
      "Average loss at step 472800: 326.797643\n",
      "Average loss at step 472900: 327.201829\n",
      "Average loss at step 473000: 328.981349\n",
      "Average loss at step 473100: 326.611682\n",
      "Average loss at step 473200: 324.346581\n",
      "Average loss at step 473300: 327.481654\n",
      "Average loss at step 473400: 327.578185\n",
      "Average loss at step 473500: 326.582343\n",
      "Average loss at step 473600: 328.930662\n",
      "Average loss at step 473700: 329.106673\n",
      "Average loss at step 473800: 323.684940\n",
      "Average loss at step 473900: 329.938596\n",
      "Average loss at step 474000: 326.783194\n",
      "Average loss at step 474100: 326.082279\n",
      "Average loss at step 474200: 328.502682\n",
      "Average loss at step 474300: 329.400749\n",
      "Average loss at step 474400: 329.866024\n",
      "Average loss at step 474500: 326.280355\n",
      "Average loss at step 474600: 324.817712\n",
      "Average loss at step 474700: 327.510183\n",
      "Average loss at step 474800: 328.003044\n",
      "Average loss at step 474900: 329.179023\n",
      "Average loss at step 475000: 324.825969\n",
      "Graph 95: 47 nodes\n",
      "Average loss at step 475100: 335.459844\n",
      "Average loss at step 475200: 329.165413\n",
      "Average loss at step 475300: 329.729640\n",
      "Average loss at step 475400: 333.131621\n",
      "Average loss at step 475500: 330.016564\n",
      "Average loss at step 475600: 331.557523\n",
      "Average loss at step 475700: 331.342861\n",
      "Average loss at step 475800: 328.736835\n",
      "Average loss at step 475900: 327.955189\n",
      "Average loss at step 476000: 327.387531\n",
      "Average loss at step 476100: 328.782057\n",
      "Average loss at step 476200: 331.523825\n",
      "Average loss at step 476300: 329.350606\n",
      "Average loss at step 476400: 326.940726\n",
      "Average loss at step 476500: 329.688424\n",
      "Average loss at step 476600: 329.430916\n",
      "Average loss at step 476700: 326.515489\n",
      "Average loss at step 476800: 329.372132\n",
      "Average loss at step 476900: 331.468020\n",
      "Average loss at step 477000: 331.349010\n",
      "Average loss at step 477100: 328.245196\n",
      "Average loss at step 477200: 322.368990\n",
      "Average loss at step 477300: 329.413227\n",
      "Average loss at step 477400: 329.050615\n",
      "Average loss at step 477500: 327.878972\n",
      "Average loss at step 477600: 329.723310\n",
      "Average loss at step 477700: 329.219488\n",
      "Average loss at step 477800: 326.632610\n",
      "Average loss at step 477900: 329.096275\n",
      "Average loss at step 478000: 330.864264\n",
      "Average loss at step 478100: 327.323757\n",
      "Average loss at step 478200: 329.021298\n",
      "Average loss at step 478300: 328.434222\n",
      "Average loss at step 478400: 324.309298\n",
      "Average loss at step 478500: 324.559395\n",
      "Average loss at step 478600: 327.543200\n",
      "Average loss at step 478700: 330.210942\n",
      "Average loss at step 478800: 324.747414\n",
      "Average loss at step 478900: 324.659125\n",
      "Average loss at step 479000: 328.073437\n",
      "Average loss at step 479100: 330.931673\n",
      "Average loss at step 479200: 326.341830\n",
      "Average loss at step 479300: 329.328981\n",
      "Average loss at step 479400: 328.106495\n",
      "Average loss at step 479500: 331.750766\n",
      "Average loss at step 479600: 326.673918\n",
      "Average loss at step 479700: 328.197289\n",
      "Average loss at step 479800: 327.815160\n",
      "Average loss at step 479900: 328.151830\n",
      "Average loss at step 480000: 327.107638\n",
      "Graph 96: 38 nodes\n",
      "Average loss at step 480100: 336.818362\n",
      "Average loss at step 480200: 336.629981\n",
      "Average loss at step 480300: 332.526396\n",
      "Average loss at step 480400: 334.053845\n",
      "Average loss at step 480500: 337.512564\n",
      "Average loss at step 480600: 333.841666\n",
      "Average loss at step 480700: 329.634858\n",
      "Average loss at step 480800: 331.391442\n",
      "Average loss at step 480900: 335.133296\n",
      "Average loss at step 481000: 335.231285\n",
      "Average loss at step 481100: 330.668193\n",
      "Average loss at step 481200: 332.692708\n",
      "Average loss at step 481300: 333.438379\n",
      "Average loss at step 481400: 331.744247\n",
      "Average loss at step 481500: 332.362803\n",
      "Average loss at step 481600: 332.832571\n",
      "Average loss at step 481700: 331.654246\n",
      "Average loss at step 481800: 331.712100\n",
      "Average loss at step 481900: 329.830758\n",
      "Average loss at step 482000: 333.743310\n",
      "Average loss at step 482100: 331.380172\n",
      "Average loss at step 482200: 330.448064\n",
      "Average loss at step 482300: 333.339715\n",
      "Average loss at step 482400: 332.919624\n",
      "Average loss at step 482500: 330.964596\n",
      "Average loss at step 482600: 328.320508\n",
      "Average loss at step 482700: 332.194905\n",
      "Average loss at step 482800: 333.087684\n",
      "Average loss at step 482900: 328.235787\n",
      "Average loss at step 483000: 334.033589\n",
      "Average loss at step 483100: 332.961491\n",
      "Average loss at step 483200: 333.559346\n",
      "Average loss at step 483300: 333.124508\n",
      "Average loss at step 483400: 332.491981\n",
      "Average loss at step 483500: 330.365722\n",
      "Average loss at step 483600: 331.796797\n",
      "Average loss at step 483700: 331.782978\n",
      "Average loss at step 483800: 330.819984\n",
      "Average loss at step 483900: 337.071083\n",
      "Average loss at step 484000: 325.887885\n",
      "Average loss at step 484100: 329.460085\n",
      "Average loss at step 484200: 328.835759\n",
      "Average loss at step 484300: 332.313485\n",
      "Average loss at step 484400: 330.987079\n",
      "Average loss at step 484500: 331.463926\n",
      "Average loss at step 484600: 331.812468\n",
      "Average loss at step 484700: 330.411427\n",
      "Average loss at step 484800: 332.669330\n",
      "Average loss at step 484900: 330.903814\n",
      "Average loss at step 485000: 336.133421\n",
      "Graph 97: 38 nodes\n",
      "Average loss at step 485100: 334.979194\n",
      "Average loss at step 485200: 333.632091\n",
      "Average loss at step 485300: 332.767782\n",
      "Average loss at step 485400: 333.403581\n",
      "Average loss at step 485500: 334.405344\n",
      "Average loss at step 485600: 333.799186\n",
      "Average loss at step 485700: 335.231079\n",
      "Average loss at step 485800: 337.680857\n",
      "Average loss at step 485900: 331.781471\n",
      "Average loss at step 486000: 333.366638\n",
      "Average loss at step 486100: 334.022007\n",
      "Average loss at step 486200: 332.755197\n",
      "Average loss at step 486300: 329.955072\n",
      "Average loss at step 486400: 334.678297\n",
      "Average loss at step 486500: 334.314828\n",
      "Average loss at step 486600: 335.694967\n",
      "Average loss at step 486700: 334.367307\n",
      "Average loss at step 486800: 332.454096\n",
      "Average loss at step 486900: 332.935652\n",
      "Average loss at step 487000: 332.287629\n",
      "Average loss at step 487100: 333.724951\n",
      "Average loss at step 487200: 329.240265\n",
      "Average loss at step 487300: 333.406404\n",
      "Average loss at step 487400: 332.059265\n",
      "Average loss at step 487500: 331.458745\n",
      "Average loss at step 487600: 334.027734\n",
      "Average loss at step 487700: 329.314331\n",
      "Average loss at step 487800: 332.442786\n",
      "Average loss at step 487900: 334.429104\n",
      "Average loss at step 488000: 334.610439\n",
      "Average loss at step 488100: 335.516374\n",
      "Average loss at step 488200: 334.563543\n",
      "Average loss at step 488300: 333.490034\n",
      "Average loss at step 488400: 330.230609\n",
      "Average loss at step 488500: 332.848903\n",
      "Average loss at step 488600: 331.624380\n",
      "Average loss at step 488700: 333.551705\n",
      "Average loss at step 488800: 334.850676\n",
      "Average loss at step 488900: 330.236598\n",
      "Average loss at step 489000: 332.327869\n",
      "Average loss at step 489100: 334.008819\n",
      "Average loss at step 489200: 335.266414\n",
      "Average loss at step 489300: 331.570496\n",
      "Average loss at step 489400: 335.155738\n",
      "Average loss at step 489500: 335.883337\n",
      "Average loss at step 489600: 330.178846\n",
      "Average loss at step 489700: 334.266793\n",
      "Average loss at step 489800: 329.683836\n",
      "Average loss at step 489900: 330.389914\n",
      "Average loss at step 490000: 336.127964\n",
      "Graph 98: 34 nodes\n",
      "Average loss at step 490100: 335.117786\n",
      "Average loss at step 490200: 329.243457\n",
      "Average loss at step 490300: 332.157262\n",
      "Average loss at step 490400: 330.820578\n",
      "Average loss at step 490500: 330.064914\n",
      "Average loss at step 490600: 328.412459\n",
      "Average loss at step 490700: 330.036986\n",
      "Average loss at step 490800: 330.579008\n",
      "Average loss at step 490900: 329.705365\n",
      "Average loss at step 491000: 332.963532\n",
      "Average loss at step 491100: 325.365057\n",
      "Average loss at step 491200: 328.989868\n",
      "Average loss at step 491300: 329.220373\n",
      "Average loss at step 491400: 327.883387\n",
      "Average loss at step 491500: 329.466191\n",
      "Average loss at step 491600: 330.613375\n",
      "Average loss at step 491700: 328.456676\n",
      "Average loss at step 491800: 332.966045\n",
      "Average loss at step 491900: 324.780463\n",
      "Average loss at step 492000: 331.009711\n",
      "Average loss at step 492100: 329.646928\n",
      "Average loss at step 492200: 329.455166\n",
      "Average loss at step 492300: 327.680602\n",
      "Average loss at step 492400: 330.331539\n",
      "Average loss at step 492500: 323.096164\n",
      "Average loss at step 492600: 326.166875\n",
      "Average loss at step 492700: 329.018930\n",
      "Average loss at step 492800: 325.440920\n",
      "Average loss at step 492900: 326.281137\n",
      "Average loss at step 493000: 326.328571\n",
      "Average loss at step 493100: 331.260821\n",
      "Average loss at step 493200: 329.630290\n",
      "Average loss at step 493300: 329.319301\n",
      "Average loss at step 493400: 328.825719\n",
      "Average loss at step 493500: 327.831556\n",
      "Average loss at step 493600: 329.005836\n",
      "Average loss at step 493700: 329.086494\n",
      "Average loss at step 493800: 331.101230\n",
      "Average loss at step 493900: 331.589940\n",
      "Average loss at step 494000: 326.511883\n",
      "Average loss at step 494100: 330.021759\n",
      "Average loss at step 494200: 330.845362\n",
      "Average loss at step 494300: 326.517296\n",
      "Average loss at step 494400: 328.775063\n",
      "Average loss at step 494500: 330.898356\n",
      "Average loss at step 494600: 329.550821\n",
      "Average loss at step 494700: 327.687963\n",
      "Average loss at step 494800: 329.391401\n",
      "Average loss at step 494900: 324.893356\n",
      "Average loss at step 495000: 327.917876\n",
      "Graph 99: 29 nodes\n",
      "Average loss at step 495100: 332.575328\n",
      "Average loss at step 495200: 330.511976\n",
      "Average loss at step 495300: 328.460571\n",
      "Average loss at step 495400: 324.004377\n",
      "Average loss at step 495500: 328.956041\n",
      "Average loss at step 495600: 327.624335\n",
      "Average loss at step 495700: 327.251136\n",
      "Average loss at step 495800: 326.545291\n",
      "Average loss at step 495900: 325.420060\n",
      "Average loss at step 496000: 323.345610\n",
      "Average loss at step 496100: 326.537278\n",
      "Average loss at step 496200: 325.976397\n",
      "Average loss at step 496300: 326.799562\n",
      "Average loss at step 496400: 328.777795\n",
      "Average loss at step 496500: 327.599504\n",
      "Average loss at step 496600: 328.467158\n",
      "Average loss at step 496700: 324.902380\n",
      "Average loss at step 496800: 327.634932\n",
      "Average loss at step 496900: 327.949532\n",
      "Average loss at step 497000: 326.619113\n",
      "Average loss at step 497100: 323.598817\n",
      "Average loss at step 497200: 327.251622\n",
      "Average loss at step 497300: 322.632800\n",
      "Average loss at step 497400: 324.828122\n",
      "Average loss at step 497500: 322.775913\n",
      "Average loss at step 497600: 329.678648\n",
      "Average loss at step 497700: 326.156897\n",
      "Average loss at step 497800: 326.768616\n",
      "Average loss at step 497900: 323.178488\n",
      "Average loss at step 498000: 322.568203\n",
      "Average loss at step 498100: 324.999508\n",
      "Average loss at step 498200: 325.336908\n",
      "Average loss at step 498300: 325.103350\n",
      "Average loss at step 498400: 325.945117\n",
      "Average loss at step 498500: 324.742573\n",
      "Average loss at step 498600: 323.502439\n",
      "Average loss at step 498700: 321.318319\n",
      "Average loss at step 498800: 325.808645\n",
      "Average loss at step 498900: 325.532978\n",
      "Average loss at step 499000: 321.697924\n",
      "Average loss at step 499100: 328.134486\n",
      "Average loss at step 499200: 325.001895\n",
      "Average loss at step 499300: 325.748173\n",
      "Average loss at step 499400: 330.698043\n",
      "Average loss at step 499500: 323.711358\n",
      "Average loss at step 499600: 324.856483\n",
      "Average loss at step 499700: 324.435243\n",
      "Average loss at step 499800: 327.507120\n",
      "Average loss at step 499900: 326.811274\n",
      "Average loss at step 500000: 323.859444\n",
      "Graph 100: 20 nodes\n",
      "Average loss at step 500100: 341.097373\n",
      "Average loss at step 500200: 334.687691\n",
      "Average loss at step 500300: 336.926615\n",
      "Average loss at step 500400: 336.737230\n",
      "Average loss at step 500500: 336.555037\n",
      "Average loss at step 500600: 334.344684\n",
      "Average loss at step 500700: 333.610845\n",
      "Average loss at step 500800: 335.704475\n",
      "Average loss at step 500900: 334.918780\n",
      "Average loss at step 501000: 330.869732\n",
      "Average loss at step 501100: 334.090358\n",
      "Average loss at step 501200: 333.117021\n",
      "Average loss at step 501300: 334.798488\n",
      "Average loss at step 501400: 328.744380\n",
      "Average loss at step 501500: 332.223003\n",
      "Average loss at step 501600: 332.430882\n",
      "Average loss at step 501700: 330.735629\n",
      "Average loss at step 501800: 332.918710\n",
      "Average loss at step 501900: 332.133401\n",
      "Average loss at step 502000: 336.818471\n",
      "Average loss at step 502100: 333.627411\n",
      "Average loss at step 502200: 332.775133\n",
      "Average loss at step 502300: 330.915106\n",
      "Average loss at step 502400: 331.937854\n",
      "Average loss at step 502500: 329.153100\n",
      "Average loss at step 502600: 334.148485\n",
      "Average loss at step 502700: 332.750464\n",
      "Average loss at step 502800: 330.017577\n",
      "Average loss at step 502900: 333.902662\n",
      "Average loss at step 503000: 327.466624\n",
      "Average loss at step 503100: 331.907068\n",
      "Average loss at step 503200: 329.908738\n",
      "Average loss at step 503300: 330.796533\n",
      "Average loss at step 503400: 330.724566\n",
      "Average loss at step 503500: 331.258142\n",
      "Average loss at step 503600: 333.247925\n",
      "Average loss at step 503700: 331.432239\n",
      "Average loss at step 503800: 330.359097\n",
      "Average loss at step 503900: 332.281984\n",
      "Average loss at step 504000: 334.141849\n",
      "Average loss at step 504100: 330.957471\n",
      "Average loss at step 504200: 330.786608\n",
      "Average loss at step 504300: 332.399963\n",
      "Average loss at step 504400: 332.274356\n",
      "Average loss at step 504500: 331.062011\n",
      "Average loss at step 504600: 331.407311\n",
      "Average loss at step 504700: 332.415589\n",
      "Average loss at step 504800: 331.623003\n",
      "Average loss at step 504900: 332.047936\n",
      "Average loss at step 505000: 331.625503\n",
      "Time: 23.7638919353\n",
      "Graph 101: 17 nodes\n",
      "Average loss at step 505100: 336.232212\n",
      "Average loss at step 505200: 330.426617\n",
      "Average loss at step 505300: 327.168006\n",
      "Average loss at step 505400: 328.685324\n",
      "Average loss at step 505500: 327.799012\n",
      "Average loss at step 505600: 328.608886\n",
      "Average loss at step 505700: 323.711581\n",
      "Average loss at step 505800: 328.370919\n",
      "Average loss at step 505900: 329.283142\n",
      "Average loss at step 506000: 323.516541\n",
      "Average loss at step 506100: 327.179609\n",
      "Average loss at step 506200: 328.138437\n",
      "Average loss at step 506300: 324.642193\n",
      "Average loss at step 506400: 328.060112\n",
      "Average loss at step 506500: 326.013742\n",
      "Average loss at step 506600: 327.190310\n",
      "Average loss at step 506700: 326.052575\n",
      "Average loss at step 506800: 327.449040\n",
      "Average loss at step 506900: 323.728719\n",
      "Average loss at step 507000: 326.139319\n",
      "Average loss at step 507100: 323.868668\n",
      "Average loss at step 507200: 326.867106\n",
      "Average loss at step 507300: 322.999658\n",
      "Average loss at step 507400: 325.873066\n",
      "Average loss at step 507500: 327.332130\n",
      "Average loss at step 507600: 327.264761\n",
      "Average loss at step 507700: 323.157307\n",
      "Average loss at step 507800: 327.938076\n",
      "Average loss at step 507900: 325.669754\n",
      "Average loss at step 508000: 325.859604\n",
      "Average loss at step 508100: 332.973669\n",
      "Average loss at step 508200: 326.478053\n",
      "Average loss at step 508300: 324.322223\n",
      "Average loss at step 508400: 327.621156\n",
      "Average loss at step 508500: 325.428957\n",
      "Average loss at step 508600: 325.749725\n",
      "Average loss at step 508700: 328.377121\n",
      "Average loss at step 508800: 326.356791\n",
      "Average loss at step 508900: 323.740847\n",
      "Average loss at step 509000: 326.688016\n",
      "Average loss at step 509100: 325.740509\n",
      "Average loss at step 509200: 323.207881\n",
      "Average loss at step 509300: 327.141544\n",
      "Average loss at step 509400: 322.649162\n",
      "Average loss at step 509500: 324.111526\n",
      "Average loss at step 509600: 328.452336\n",
      "Average loss at step 509700: 324.604933\n",
      "Average loss at step 509800: 324.487684\n",
      "Average loss at step 509900: 323.484391\n",
      "Average loss at step 510000: 326.609882\n",
      "Graph 102: 41 nodes\n",
      "Average loss at step 510100: 338.467866\n",
      "Average loss at step 510200: 332.180368\n",
      "Average loss at step 510300: 337.743633\n",
      "Average loss at step 510400: 332.243088\n",
      "Average loss at step 510500: 333.292830\n",
      "Average loss at step 510600: 333.872434\n",
      "Average loss at step 510700: 332.401816\n",
      "Average loss at step 510800: 334.431754\n",
      "Average loss at step 510900: 333.739108\n",
      "Average loss at step 511000: 331.111798\n",
      "Average loss at step 511100: 333.084655\n",
      "Average loss at step 511200: 330.336535\n",
      "Average loss at step 511300: 333.619550\n",
      "Average loss at step 511400: 332.302188\n",
      "Average loss at step 511500: 331.872950\n",
      "Average loss at step 511600: 332.121890\n",
      "Average loss at step 511700: 330.424484\n",
      "Average loss at step 511800: 334.160509\n",
      "Average loss at step 511900: 333.178096\n",
      "Average loss at step 512000: 329.738192\n",
      "Average loss at step 512100: 334.235255\n",
      "Average loss at step 512200: 329.124500\n",
      "Average loss at step 512300: 333.910928\n",
      "Average loss at step 512400: 330.952055\n",
      "Average loss at step 512500: 330.329963\n",
      "Average loss at step 512600: 331.442078\n",
      "Average loss at step 512700: 332.228956\n",
      "Average loss at step 512800: 326.628115\n",
      "Average loss at step 512900: 333.846683\n",
      "Average loss at step 513000: 331.573609\n",
      "Average loss at step 513100: 330.913537\n",
      "Average loss at step 513200: 332.614079\n",
      "Average loss at step 513300: 333.755373\n",
      "Average loss at step 513400: 329.497888\n",
      "Average loss at step 513500: 327.454088\n",
      "Average loss at step 513600: 326.793996\n",
      "Average loss at step 513700: 330.435702\n",
      "Average loss at step 513800: 332.606186\n",
      "Average loss at step 513900: 330.844958\n",
      "Average loss at step 514000: 332.212549\n",
      "Average loss at step 514100: 332.394727\n",
      "Average loss at step 514200: 326.491257\n",
      "Average loss at step 514300: 329.722167\n",
      "Average loss at step 514400: 330.559543\n",
      "Average loss at step 514500: 332.826606\n",
      "Average loss at step 514600: 332.536625\n",
      "Average loss at step 514700: 334.805855\n",
      "Average loss at step 514800: 330.688067\n",
      "Average loss at step 514900: 329.925112\n",
      "Average loss at step 515000: 332.164497\n",
      "Graph 103: 24 nodes\n",
      "Average loss at step 515100: 340.098606\n",
      "Average loss at step 515200: 334.252752\n",
      "Average loss at step 515300: 335.783936\n",
      "Average loss at step 515400: 336.031732\n",
      "Average loss at step 515500: 334.428724\n",
      "Average loss at step 515600: 334.827234\n",
      "Average loss at step 515700: 332.205413\n",
      "Average loss at step 515800: 330.662632\n",
      "Average loss at step 515900: 336.776359\n",
      "Average loss at step 516000: 329.685159\n",
      "Average loss at step 516100: 334.961487\n",
      "Average loss at step 516200: 331.746874\n",
      "Average loss at step 516300: 329.644965\n",
      "Average loss at step 516400: 333.770533\n",
      "Average loss at step 516500: 331.614210\n",
      "Average loss at step 516600: 331.829984\n",
      "Average loss at step 516700: 330.597741\n",
      "Average loss at step 516800: 330.207869\n",
      "Average loss at step 516900: 335.758494\n",
      "Average loss at step 517000: 333.897143\n",
      "Average loss at step 517100: 332.361668\n",
      "Average loss at step 517200: 334.225338\n",
      "Average loss at step 517300: 334.771019\n",
      "Average loss at step 517400: 333.218560\n",
      "Average loss at step 517500: 332.854534\n",
      "Average loss at step 517600: 333.847731\n",
      "Average loss at step 517700: 332.263352\n",
      "Average loss at step 517800: 331.822518\n",
      "Average loss at step 517900: 334.209037\n",
      "Average loss at step 518000: 332.841369\n",
      "Average loss at step 518100: 329.256737\n",
      "Average loss at step 518200: 331.740649\n",
      "Average loss at step 518300: 331.866803\n",
      "Average loss at step 518400: 332.758506\n",
      "Average loss at step 518500: 333.558455\n",
      "Average loss at step 518600: 333.252344\n",
      "Average loss at step 518700: 333.595040\n",
      "Average loss at step 518800: 328.437593\n",
      "Average loss at step 518900: 336.405369\n",
      "Average loss at step 519000: 331.288370\n",
      "Average loss at step 519100: 328.052876\n",
      "Average loss at step 519200: 337.570328\n",
      "Average loss at step 519300: 331.687798\n",
      "Average loss at step 519400: 329.251506\n",
      "Average loss at step 519500: 332.769543\n",
      "Average loss at step 519600: 330.820244\n",
      "Average loss at step 519700: 335.073447\n",
      "Average loss at step 519800: 330.765721\n",
      "Average loss at step 519900: 328.419878\n",
      "Average loss at step 520000: 332.225892\n",
      "Graph 104: 50 nodes\n",
      "Average loss at step 520100: 340.190671\n",
      "Average loss at step 520200: 334.498302\n",
      "Average loss at step 520300: 337.099864\n",
      "Average loss at step 520400: 330.739645\n",
      "Average loss at step 520500: 336.604744\n",
      "Average loss at step 520600: 335.047836\n",
      "Average loss at step 520700: 332.915251\n",
      "Average loss at step 520800: 328.045183\n",
      "Average loss at step 520900: 332.560083\n",
      "Average loss at step 521000: 334.602295\n",
      "Average loss at step 521100: 337.706372\n",
      "Average loss at step 521200: 338.318880\n",
      "Average loss at step 521300: 335.110219\n",
      "Average loss at step 521400: 336.036109\n",
      "Average loss at step 521500: 338.426029\n",
      "Average loss at step 521600: 335.953836\n",
      "Average loss at step 521700: 332.999557\n",
      "Average loss at step 521800: 330.848015\n",
      "Average loss at step 521900: 334.970391\n",
      "Average loss at step 522000: 334.602133\n",
      "Average loss at step 522100: 334.289759\n",
      "Average loss at step 522200: 337.123691\n",
      "Average loss at step 522300: 335.305231\n",
      "Average loss at step 522400: 332.925336\n",
      "Average loss at step 522500: 333.367377\n",
      "Average loss at step 522600: 337.513572\n",
      "Average loss at step 522700: 334.716321\n",
      "Average loss at step 522800: 332.269328\n",
      "Average loss at step 522900: 331.851036\n",
      "Average loss at step 523000: 334.465157\n",
      "Average loss at step 523100: 335.528722\n",
      "Average loss at step 523200: 333.758457\n",
      "Average loss at step 523300: 330.842199\n",
      "Average loss at step 523400: 335.660765\n",
      "Average loss at step 523500: 334.998828\n",
      "Average loss at step 523600: 328.896829\n",
      "Average loss at step 523700: 333.888845\n",
      "Average loss at step 523800: 333.253206\n",
      "Average loss at step 523900: 332.972958\n",
      "Average loss at step 524000: 332.609366\n",
      "Average loss at step 524100: 334.311043\n",
      "Average loss at step 524200: 333.458399\n",
      "Average loss at step 524300: 330.911147\n",
      "Average loss at step 524400: 333.231480\n",
      "Average loss at step 524500: 337.330028\n",
      "Average loss at step 524600: 333.493835\n",
      "Average loss at step 524700: 333.765182\n",
      "Average loss at step 524800: 335.330307\n",
      "Average loss at step 524900: 331.396341\n",
      "Average loss at step 525000: 333.458239\n",
      "Graph 105: 21 nodes\n",
      "Average loss at step 525100: 335.101726\n",
      "Average loss at step 525200: 329.650979\n",
      "Average loss at step 525300: 330.726254\n",
      "Average loss at step 620600: 334.950087\n",
      "Average loss at step 620700: 334.422268\n",
      "Average loss at step 620800: 334.535108\n",
      "Average loss at step 620900: 332.588868\n",
      "Average loss at step 621000: 330.022949\n",
      "Average loss at step 621100: 333.010069\n",
      "Average loss at step 621200: 330.389891\n",
      "Average loss at step 621300: 331.566095\n",
      "Average loss at step 621400: 332.342638\n",
      "Average loss at step 621500: 332.043105\n",
      "Average loss at step 621600: 332.736998\n",
      "Average loss at step 621700: 330.613146\n",
      "Average loss at step 621800: 332.610980\n",
      "Average loss at step 621900: 334.150986\n",
      "Average loss at step 622000: 328.813245\n",
      "Average loss at step 622100: 330.059357\n",
      "Average loss at step 622200: 328.465009\n",
      "Average loss at step 622300: 329.469719\n",
      "Average loss at step 622400: 331.214587\n",
      "Average loss at step 622500: 330.109880\n",
      "Average loss at step 622600: 331.903031\n",
      "Average loss at step 622700: 329.782228\n",
      "Average loss at step 622800: 328.643180\n",
      "Average loss at step 622900: 331.703959\n",
      "Average loss at step 623000: 330.493330\n",
      "Average loss at step 623100: 331.795396\n",
      "Average loss at step 623200: 330.047263\n",
      "Average loss at step 623300: 330.888224\n",
      "Average loss at step 623400: 331.135169\n",
      "Average loss at step 623500: 335.842819\n",
      "Average loss at step 623600: 334.709841\n",
      "Average loss at step 623700: 327.155876\n",
      "Average loss at step 623800: 331.854011\n",
      "Average loss at step 623900: 328.488231\n",
      "Average loss at step 624000: 329.539291\n",
      "Average loss at step 624100: 330.078687\n",
      "Average loss at step 624200: 332.352292\n",
      "Average loss at step 624300: 327.139237\n",
      "Average loss at step 624400: 334.617386\n",
      "Average loss at step 624500: 328.962770\n",
      "Average loss at step 624600: 333.615495\n",
      "Average loss at step 624700: 332.491566\n",
      "Average loss at step 624800: 326.900512\n",
      "Average loss at step 624900: 329.315018\n",
      "Average loss at step 625000: 333.997261\n",
      "Graph 125: 19 nodes\n",
      "Average loss at step 625100: 424.147036\n",
      "Average loss at step 625200: 351.684508\n",
      "Average loss at step 625300: 353.310737\n",
      "Average loss at step 625400: 360.195491\n",
      "Average loss at step 625500: 346.068055\n",
      "Average loss at step 625600: 345.775471\n",
      "Average loss at step 625700: 351.449435\n",
      "Average loss at step 625800: 346.027692\n",
      "Average loss at step 625900: 345.924905\n",
      "Average loss at step 626000: 342.905856\n",
      "Average loss at step 626100: 343.966875\n",
      "Average loss at step 626200: 337.947597\n",
      "Average loss at step 626300: 342.437232\n",
      "Average loss at step 626400: 347.474619\n",
      "Average loss at step 626500: 342.920866\n",
      "Average loss at step 626600: 347.914351\n",
      "Average loss at step 626700: 338.913712\n",
      "Average loss at step 626800: 346.224465\n",
      "Average loss at step 626900: 344.231892\n",
      "Average loss at step 627000: 348.008264\n",
      "Average loss at step 627100: 342.014077\n",
      "Average loss at step 627200: 339.689605\n",
      "Average loss at step 627300: 346.660319\n",
      "Average loss at step 627400: 344.825404\n",
      "Average loss at step 627500: 342.552318\n",
      "Average loss at step 627600: 343.391790\n",
      "Average loss at step 627700: 343.686920\n",
      "Average loss at step 627800: 339.778762\n",
      "Average loss at step 627900: 341.688698\n",
      "Average loss at step 628000: 344.777688\n",
      "Average loss at step 628100: 347.213141\n",
      "Average loss at step 628200: 343.796040\n",
      "Average loss at step 628300: 342.934886\n",
      "Average loss at step 628400: 339.378717\n",
      "Average loss at step 628500: 342.574013\n",
      "Average loss at step 628600: 337.561726\n",
      "Average loss at step 628700: 343.237892\n",
      "Average loss at step 628800: 343.288112\n",
      "Average loss at step 628900: 341.607115\n",
      "Average loss at step 629000: 347.607147\n",
      "Average loss at step 629100: 340.086940\n",
      "Average loss at step 629200: 341.824846\n",
      "Average loss at step 629300: 338.302579\n",
      "Average loss at step 629400: 341.538606\n",
      "Average loss at step 629500: 347.898921\n",
      "Average loss at step 629600: 336.862262\n",
      "Average loss at step 629700: 344.881100\n",
      "Average loss at step 629800: 344.014308\n",
      "Average loss at step 629900: 342.924079\n",
      "Average loss at step 630000: 340.734496\n",
      "Graph 126: 22 nodes\n",
      "Average loss at step 630100: 427.701849\n",
      "Average loss at step 630200: 350.445462\n",
      "Average loss at step 630300: 338.537376\n",
      "Average loss at step 630400: 341.100699\n",
      "Average loss at step 630500: 331.210916\n",
      "Average loss at step 630600: 333.494473\n",
      "Average loss at step 630700: 333.582937\n",
      "Average loss at step 630800: 338.516650\n",
      "Average loss at step 630900: 335.321576\n",
      "Average loss at step 631000: 337.424387\n",
      "Average loss at step 631100: 333.738580\n",
      "Average loss at step 631200: 335.491328\n",
      "Average loss at step 631300: 331.532574\n",
      "Average loss at step 631400: 330.476354\n",
      "Average loss at step 631500: 334.990051\n",
      "Average loss at step 631600: 332.068213\n",
      "Average loss at step 631700: 335.622806\n",
      "Average loss at step 631800: 332.295157\n",
      "Average loss at step 631900: 334.672967\n",
      "Average loss at step 632000: 333.066364\n",
      "Average loss at step 632100: 335.727089\n",
      "Average loss at step 632200: 333.433006\n",
      "Average loss at step 632300: 334.986249\n",
      "Average loss at step 632400: 334.284918\n",
      "Average loss at step 632500: 334.450214\n",
      "Average loss at step 632600: 338.026246\n",
      "Average loss at step 632700: 334.243489\n",
      "Average loss at step 632800: 331.848494\n",
      "Average loss at step 632900: 332.680365\n",
      "Average loss at step 633000: 339.656520\n",
      "Average loss at step 633100: 332.593821\n",
      "Average loss at step 633200: 331.976528\n",
      "Average loss at step 633300: 332.113123\n",
      "Average loss at step 633400: 332.720502\n",
      "Average loss at step 633500: 333.552564\n",
      "Average loss at step 633600: 332.905264\n",
      "Average loss at step 633700: 331.346485\n",
      "Average loss at step 633800: 330.701970\n",
      "Average loss at step 633900: 335.237860\n",
      "Average loss at step 634000: 339.615771\n",
      "Average loss at step 634100: 331.810127\n",
      "Average loss at step 634200: 325.044551\n",
      "Average loss at step 634300: 333.761380\n",
      "Average loss at step 634400: 333.834411\n",
      "Average loss at step 634500: 336.691720\n",
      "Average loss at step 634600: 335.318214\n",
      "Average loss at step 634700: 336.841773\n",
      "Average loss at step 634800: 330.689480\n",
      "Average loss at step 634900: 335.765958\n",
      "Average loss at step 635000: 337.228844\n",
      "Graph 127: 21 nodes\n",
      "Average loss at step 635100: 396.538032\n",
      "Average loss at step 635200: 344.487706\n",
      "Average loss at step 635300: 342.987952\n",
      "Average loss at step 635400: 344.032321\n",
      "Average loss at step 635500: 340.445176\n",
      "Average loss at step 635600: 343.190768\n",
      "Average loss at step 635700: 341.720463\n",
      "Average loss at step 635800: 343.648535\n",
      "Average loss at step 635900: 337.395805\n",
      "Average loss at step 636000: 345.512238\n",
      "Average loss at step 636100: 343.678920\n",
      "Average loss at step 636200: 340.328167\n",
      "Average loss at step 636300: 340.728502\n",
      "Average loss at step 636400: 344.712244\n",
      "Average loss at step 636500: 339.860805\n",
      "Average loss at step 636600: 338.017387\n",
      "Average loss at step 636700: 338.307116\n",
      "Average loss at step 636800: 336.060905\n",
      "Average loss at step 636900: 338.429945\n",
      "Average loss at step 637000: 339.532172\n",
      "Average loss at step 637100: 341.363470\n",
      "Average loss at step 637200: 335.641723\n",
      "Average loss at step 637300: 336.943605\n",
      "Average loss at step 637400: 340.917825\n",
      "Average loss at step 637500: 340.829535\n",
      "Average loss at step 637600: 338.986235\n",
      "Average loss at step 637700: 340.897571\n",
      "Average loss at step 637800: 336.234454\n",
      "Average loss at step 637900: 335.988274\n",
      "Average loss at step 638000: 339.458245\n",
      "Average loss at step 638100: 337.836135\n",
      "Average loss at step 638200: 337.650450\n",
      "Average loss at step 638300: 335.645466\n",
      "Average loss at step 638400: 337.070974\n",
      "Average loss at step 638500: 338.393055\n",
      "Average loss at step 638600: 341.582515\n",
      "Average loss at step 638700: 340.972311\n",
      "Average loss at step 638800: 338.420370\n",
      "Average loss at step 638900: 340.470804\n",
      "Average loss at step 639000: 344.437299\n",
      "Average loss at step 639100: 336.917846\n",
      "Average loss at step 639200: 333.670172\n",
      "Average loss at step 639300: 339.847093\n",
      "Average loss at step 639400: 337.535675\n",
      "Average loss at step 639500: 335.484226\n",
      "Average loss at step 639600: 334.552544\n",
      "Average loss at step 639700: 334.865766\n",
      "Average loss at step 639800: 337.635973\n",
      "Average loss at step 639900: 339.163265\n",
      "Average loss at step 640000: 337.526082\n",
      "Graph 128: 20 nodes\n",
      "Average loss at step 640100: 413.345684\n",
      "Average loss at step 640200: 342.711735\n",
      "Average loss at step 640300: 339.278974\n",
      "Average loss at step 640400: 341.419483\n",
      "Average loss at step 640500: 338.459882\n",
      "Average loss at step 640600: 342.563371\n",
      "Average loss at step 640700: 341.303522\n",
      "Average loss at step 640800: 335.780877\n",
      "Average loss at step 640900: 337.132259\n",
      "Average loss at step 641000: 337.705509\n",
      "Average loss at step 641100: 336.797975\n",
      "Average loss at step 641200: 334.362890\n",
      "Average loss at step 697300: 328.070594\n",
      "Average loss at step 697400: 326.675050\n",
      "Average loss at step 697500: 325.413532\n",
      "Average loss at step 697600: 326.405067\n",
      "Average loss at step 697700: 326.247171\n",
      "Average loss at step 697800: 322.721264\n",
      "Average loss at step 697900: 327.521584\n",
      "Average loss at step 698000: 326.595868\n",
      "Average loss at step 698100: 325.433293\n",
      "Average loss at step 698200: 327.457626\n",
      "Average loss at step 698300: 324.402279\n",
      "Average loss at step 698400: 324.442131\n",
      "Average loss at step 698500: 322.049558\n",
      "Average loss at step 698600: 325.563820\n",
      "Average loss at step 698700: 324.960174\n",
      "Average loss at step 698800: 326.550915\n",
      "Average loss at step 698900: 324.355993\n",
      "Average loss at step 699000: 321.146218\n",
      "Average loss at step 699100: 326.448559\n",
      "Average loss at step 699200: 322.022732\n",
      "Average loss at step 699300: 327.083263\n",
      "Average loss at step 699400: 327.269104\n",
      "Average loss at step 699500: 322.960415\n",
      "Average loss at step 699600: 319.836082\n",
      "Average loss at step 699700: 324.977191\n",
      "Average loss at step 699800: 326.510858\n",
      "Average loss at step 699900: 321.625684\n",
      "Average loss at step 700000: 325.554468\n",
      "Graph 140: 45 nodes\n",
      "Average loss at step 700100: 330.054377\n",
      "Average loss at step 700200: 321.134110\n",
      "Average loss at step 700300: 322.576687\n",
      "Average loss at step 700400: 325.291714\n",
      "Average loss at step 700500: 326.529246\n",
      "Average loss at step 700600: 326.013804\n",
      "Average loss at step 700700: 322.885803\n",
      "Average loss at step 700800: 324.841511\n",
      "Average loss at step 700900: 327.116494\n",
      "Average loss at step 701000: 322.218163\n",
      "Average loss at step 701100: 321.565060\n",
      "Average loss at step 701200: 323.243234\n",
      "Average loss at step 701300: 321.748702\n",
      "Average loss at step 701400: 325.000907\n",
      "Average loss at step 701500: 323.482406\n",
      "Average loss at step 701600: 326.298234\n",
      "Average loss at step 701700: 318.522096\n",
      "Average loss at step 701800: 322.789581\n",
      "Average loss at step 701900: 322.761569\n",
      "Average loss at step 702000: 327.716801\n",
      "Average loss at step 702100: 323.537520\n",
      "Average loss at step 702200: 325.759612\n",
      "Average loss at step 702300: 322.944195\n",
      "Average loss at step 702400: 322.590434\n",
      "Average loss at step 702500: 316.937255\n",
      "Average loss at step 702600: 324.599308\n",
      "Average loss at step 702700: 325.778333\n",
      "Average loss at step 702800: 324.587000\n",
      "Average loss at step 702900: 319.712749\n",
      "Average loss at step 703000: 322.655568\n",
      "Average loss at step 703100: 323.928861\n",
      "Average loss at step 703200: 323.179487\n",
      "Average loss at step 703300: 319.526006\n",
      "Average loss at step 703400: 322.456230\n",
      "Average loss at step 703500: 325.210006\n",
      "Average loss at step 703600: 322.394480\n",
      "Average loss at step 703700: 320.941236\n",
      "Average loss at step 703800: 321.140068\n",
      "Average loss at step 703900: 326.833534\n",
      "Average loss at step 704000: 325.306967\n",
      "Average loss at step 704100: 321.724951\n",
      "Average loss at step 704200: 325.972169\n",
      "Average loss at step 704300: 321.941122\n",
      "Average loss at step 704400: 326.871274\n",
      "Average loss at step 704500: 323.712669\n",
      "Average loss at step 704600: 320.713514\n",
      "Average loss at step 704700: 322.451543\n",
      "Average loss at step 704800: 316.132004\n",
      "Average loss at step 704900: 324.157765\n",
      "Average loss at step 705000: 325.179664\n",
      "Time: 23.9209330082\n",
      "Graph 141: 30 nodes\n",
      "Average loss at step 705100: 338.310560\n",
      "Average loss at step 705200: 331.546875\n",
      "Average loss at step 705300: 337.396217\n",
      "Average loss at step 705400: 332.279711\n",
      "Average loss at step 705500: 329.418307\n",
      "Average loss at step 705600: 332.066177\n",
      "Average loss at step 705700: 326.567638\n",
      "Average loss at step 705800: 332.739538\n",
      "Average loss at step 705900: 328.671478\n",
      "Average loss at step 706000: 333.780656\n",
      "Average loss at step 706100: 329.605569\n",
      "Average loss at step 706200: 327.924086\n",
      "Average loss at step 706300: 331.041834\n",
      "Average loss at step 706400: 330.415064\n",
      "Average loss at step 706500: 330.146066\n",
      "Average loss at step 706600: 330.744961\n",
      "Average loss at step 706700: 330.984360\n",
      "Average loss at step 706800: 328.784904\n",
      "Average loss at step 706900: 328.457825\n",
      "Average loss at step 707000: 329.299247\n",
      "Average loss at step 707100: 325.076669\n",
      "Average loss at step 707200: 328.580370\n",
      "Average loss at step 707300: 330.747557\n",
      "Average loss at step 707400: 334.170008\n",
      "Average loss at step 707500: 329.164287\n",
      "Average loss at step 707600: 327.455752\n",
      "Average loss at step 707700: 329.280924\n",
      "Average loss at step 707800: 327.784401\n",
      "Average loss at step 707900: 330.630986\n",
      "Average loss at step 708000: 326.369414\n",
      "Average loss at step 708100: 328.113674\n",
      "Average loss at step 708200: 328.112872\n",
      "Average loss at step 708300: 329.773600\n",
      "Average loss at step 708400: 329.589364\n",
      "Average loss at step 708500: 326.579093\n",
      "Average loss at step 708600: 330.596980\n",
      "Average loss at step 708700: 329.738879\n",
      "Average loss at step 708800: 329.354293\n",
      "Average loss at step 708900: 328.262453\n",
      "Average loss at step 709000: 325.417965\n",
      "Average loss at step 709100: 328.884047\n",
      "Average loss at step 709200: 329.403297\n",
      "Average loss at step 709300: 328.386176\n",
      "Average loss at step 709400: 328.525496\n",
      "Average loss at step 709500: 328.771764\n",
      "Average loss at step 709600: 325.802395\n",
      "Average loss at step 709700: 326.163178\n",
      "Average loss at step 709800: 330.650997\n",
      "Average loss at step 709900: 325.511360\n",
      "Average loss at step 710000: 329.824628\n",
      "Graph 142: 48 nodes\n",
      "Average loss at step 710100: 338.030490\n",
      "Average loss at step 710200: 334.777353\n",
      "Average loss at step 710300: 333.664164\n",
      "Average loss at step 710400: 335.018495\n",
      "Average loss at step 710500: 334.464141\n",
      "Average loss at step 710600: 334.463872\n",
      "Average loss at step 710700: 335.104556\n",
      "Average loss at step 710800: 336.364267\n",
      "Average loss at step 710900: 335.531092\n",
      "Average loss at step 711000: 333.496981\n",
      "Average loss at step 711100: 335.797875\n",
      "Average loss at step 711200: 335.439631\n",
      "Average loss at step 711300: 334.914642\n",
      "Average loss at step 711400: 330.819044\n",
      "Average loss at step 711500: 335.777075\n",
      "Average loss at step 711600: 332.978176\n",
      "Average loss at step 711700: 333.463948\n",
      "Average loss at step 711800: 336.784292\n",
      "Average loss at step 711900: 335.643227\n",
      "Average loss at step 712000: 335.836822\n",
      "Average loss at step 712100: 330.806851\n",
      "Average loss at step 712200: 333.809846\n",
      "Average loss at step 712300: 332.912864\n",
      "Average loss at step 712400: 332.919476\n",
      "Average loss at step 712500: 335.776781\n",
      "Average loss at step 712600: 331.550639\n",
      "Average loss at step 712700: 334.879500\n",
      "Average loss at step 712800: 329.063122\n",
      "Average loss at step 712900: 330.833606\n",
      "Average loss at step 713000: 337.368614\n",
      "Average loss at step 713100: 333.069258\n",
      "Average loss at step 713200: 331.833584\n",
      "Average loss at step 713300: 333.749979\n",
      "Average loss at step 713400: 334.317013\n",
      "Average loss at step 713500: 333.324271\n",
      "Average loss at step 713600: 334.217171\n",
      "Average loss at step 713700: 330.583573\n",
      "Average loss at step 713800: 332.307298\n",
      "Average loss at step 713900: 334.142864\n",
      "Average loss at step 714000: 338.110565\n",
      "Average loss at step 714100: 335.917292\n",
      "Average loss at step 714200: 331.078208\n",
      "Average loss at step 714300: 333.492216\n",
      "Average loss at step 714400: 332.271277\n",
      "Average loss at step 714500: 331.625598\n",
      "Average loss at step 714600: 333.501167\n",
      "Average loss at step 714700: 331.924831\n",
      "Average loss at step 714800: 332.062189\n",
      "Average loss at step 714900: 332.189011\n",
      "Average loss at step 715000: 330.020768\n",
      "Graph 143: 18 nodes\n",
      "Average loss at step 715100: 342.186174\n",
      "Average loss at step 715200: 333.425082\n",
      "Average loss at step 715300: 330.050661\n",
      "Average loss at step 715400: 329.419827\n",
      "Average loss at step 715500: 326.070460\n",
      "Average loss at step 715600: 327.141413\n",
      "Average loss at step 715700: 323.872725\n",
      "Average loss at step 715800: 323.236075\n",
      "Average loss at step 715900: 326.408510\n",
      "Average loss at step 716000: 326.707209\n",
      "Average loss at step 716100: 325.050320\n",
      "Average loss at step 716200: 325.871251\n",
      "Average loss at step 716300: 322.857223\n",
      "Average loss at step 716400: 324.134254\n",
      "Average loss at step 716500: 325.136455\n",
      "Average loss at step 716600: 322.557480\n",
      "Average loss at step 716700: 324.511979\n",
      "Average loss at step 716800: 323.813957\n",
      "Average loss at step 716900: 322.589018\n",
      "Average loss at step 717000: 325.964327\n",
      "Average loss at step 717100: 324.190845\n",
      "Average loss at step 717200: 323.278856\n",
      "Average loss at step 717300: 323.256880\n",
      "Average loss at step 717400: 321.392037\n",
      "Average loss at step 717500: 325.789193\n",
      "Average loss at step 717600: 326.258245\n",
      "Average loss at step 717700: 323.692981\n",
      "Average loss at step 717800: 322.602196\n",
      "Average loss at step 717900: 326.486184\n",
      "Average loss at step 718000: 325.433485\n",
      "Average loss at step 718100: 325.695741\n",
      "Average loss at step 718200: 323.348559\n",
      "Average loss at step 718300: 322.166724\n",
      "Average loss at step 718400: 321.927195\n",
      "Average loss at step 718500: 321.588670\n",
      "Average loss at step 718600: 324.733194\n",
      "Average loss at step 718700: 320.337020\n",
      "Average loss at step 718800: 324.170118\n",
      "Average loss at step 718900: 324.646298\n",
      "Average loss at step 719000: 324.045858\n",
      "Average loss at step 719100: 324.117831\n",
      "Average loss at step 719200: 321.251117\n",
      "Average loss at step 719300: 321.035851\n",
      "Average loss at step 719400: 323.602693\n",
      "Average loss at step 719500: 322.468606\n",
      "Average loss at step 719600: 324.638829\n",
      "Average loss at step 719700: 323.666063\n",
      "Average loss at step 719800: 322.955211\n",
      "Average loss at step 719900: 323.388997\n",
      "Average loss at step 720000: 323.039076\n",
      "Graph 144: 17 nodes\n",
      "Average loss at step 720100: 344.422296\n",
      "Average loss at step 720200: 337.515895\n",
      "Average loss at step 720300: 336.912440\n",
      "Average loss at step 720400: 338.526452\n",
      "Average loss at step 720500: 337.947857\n",
      "Average loss at step 720600: 334.051207\n",
      "Average loss at step 720700: 332.559454\n",
      "Average loss at step 720800: 337.654335\n",
      "Average loss at step 720900: 332.706336\n",
      "Average loss at step 721000: 337.140118\n",
      "Average loss at step 721100: 333.611307\n",
      "Average loss at step 721200: 335.697780\n",
      "Average loss at step 721300: 331.875279\n",
      "Average loss at step 721400: 339.022213\n",
      "Average loss at step 721500: 333.572055\n",
      "Average loss at step 721600: 338.540713\n",
      "Average loss at step 721700: 336.281352\n",
      "Average loss at step 721800: 336.206527\n",
      "Average loss at step 721900: 331.115186\n",
      "Average loss at step 722000: 333.885966\n",
      "Average loss at step 722100: 335.905148\n",
      "Average loss at step 722200: 336.410257\n",
      "Average loss at step 722300: 334.098811\n",
      "Average loss at step 722400: 333.893998\n",
      "Average loss at step 722500: 336.978360\n",
      "Average loss at step 722600: 336.049934\n",
      "Average loss at step 722700: 335.557573\n",
      "Average loss at step 722800: 334.302340\n",
      "Average loss at step 722900: 334.320859\n",
      "Average loss at step 723000: 330.609075\n",
      "Average loss at step 723100: 335.837991\n",
      "Average loss at step 723200: 336.411942\n",
      "Average loss at step 723300: 335.148204\n",
      "Average loss at step 723400: 333.942599\n",
      "Average loss at step 723500: 332.171878\n",
      "Average loss at step 723600: 333.305715\n",
      "Average loss at step 723700: 330.351669\n",
      "Average loss at step 723800: 336.317063\n",
      "Average loss at step 723900: 334.101123\n",
      "Average loss at step 724000: 333.513407\n",
      "Average loss at step 724100: 334.312563\n",
      "Average loss at step 724200: 334.373571\n",
      "Average loss at step 724300: 334.741692\n",
      "Average loss at step 724400: 335.328443\n",
      "Average loss at step 724500: 337.218454\n",
      "Average loss at step 724600: 337.536262\n",
      "Average loss at step 724700: 336.467795\n",
      "Average loss at step 724800: 332.124685\n",
      "Average loss at step 724900: 332.345428\n",
      "Average loss at step 725000: 333.136636\n",
      "Graph 145: 19 nodes\n",
      "Average loss at step 725100: 343.751647\n",
      "Average loss at step 725200: 335.336746\n",
      "Average loss at step 725300: 333.583698\n",
      "Average loss at step 725400: 337.834015\n",
      "Average loss at step 725500: 337.818791\n",
      "Average loss at step 725600: 333.487264\n",
      "Average loss at step 725700: 333.771402\n",
      "Average loss at step 725800: 333.625481\n",
      "Average loss at step 725900: 336.605308\n",
      "Average loss at step 726000: 336.488210\n",
      "Average loss at step 726100: 333.228024\n",
      "Average loss at step 726200: 333.102926\n",
      "Average loss at step 726300: 331.258698\n",
      "Average loss at step 726400: 333.250470\n",
      "Average loss at step 726500: 331.000978\n",
      "Average loss at step 726600: 335.211859\n",
      "Average loss at step 726700: 330.591571\n",
      "Average loss at step 726800: 338.237407\n",
      "Average loss at step 726900: 333.313283\n",
      "Average loss at step 727000: 336.698547\n",
      "Average loss at step 727100: 336.692118\n",
      "Average loss at step 727200: 331.316244\n",
      "Average loss at step 727300: 330.285705\n",
      "Average loss at step 727400: 332.196394\n",
      "Average loss at step 727500: 330.587770\n",
      "Average loss at step 727600: 331.272933\n",
      "Average loss at step 727700: 334.966868\n",
      "Average loss at step 727800: 337.840267\n",
      "Average loss at step 727900: 331.722388\n",
      "Average loss at step 728000: 333.513208\n",
      "Average loss at step 728100: 333.587476\n",
      "Average loss at step 728200: 332.752952\n",
      "Average loss at step 728300: 331.886738\n",
      "Average loss at step 728400: 336.273693\n",
      "Average loss at step 728500: 335.314635\n",
      "Average loss at step 728600: 333.855533\n",
      "Average loss at step 728700: 333.503237\n",
      "Average loss at step 728800: 332.142000\n",
      "Average loss at step 728900: 337.027666\n",
      "Average loss at step 729000: 333.283553\n",
      "Average loss at step 729100: 328.523698\n",
      "Average loss at step 729200: 331.889112\n",
      "Average loss at step 729300: 335.624186\n",
      "Average loss at step 729400: 337.592632\n",
      "Average loss at step 729500: 332.000543\n",
      "Average loss at step 729600: 331.574466\n",
      "Average loss at step 729700: 336.388368\n",
      "Average loss at step 729800: 331.409481\n",
      "Average loss at step 729900: 332.206849\n",
      "Average loss at step 730000: 334.238229\n",
      "Graph 146: 38 nodes\n",
      "Average loss at step 730100: 335.574892\n",
      "Average loss at step 730200: 330.336987\n",
      "Average loss at step 730300: 326.783625\n",
      "Average loss at step 730400: 327.039433\n",
      "Average loss at step 730500: 329.156484\n",
      "Average loss at step 730600: 325.408997\n",
      "Average loss at step 730700: 327.306547\n",
      "Average loss at step 730800: 329.555378\n",
      "Average loss at step 730900: 323.281836\n",
      "Average loss at step 731000: 322.754458\n",
      "Average loss at step 731100: 325.450116\n",
      "Average loss at step 731200: 325.565797\n",
      "Average loss at step 731300: 324.427604\n",
      "Average loss at step 731400: 324.155901\n",
      "Average loss at step 731500: 322.501557\n",
      "Average loss at step 731600: 321.822990\n",
      "Average loss at step 731700: 327.375214\n",
      "Average loss at step 731800: 322.804393\n",
      "Average loss at step 731900: 325.589402\n",
      "Average loss at step 732000: 319.437418\n",
      "Average loss at step 732100: 324.806220\n",
      "Average loss at step 732200: 322.050785\n",
      "Average loss at step 732300: 319.990067\n",
      "Average loss at step 732400: 323.500507\n",
      "Average loss at step 732500: 323.612758\n",
      "Average loss at step 732600: 326.695809\n",
      "Average loss at step 732700: 323.986373\n",
      "Average loss at step 732800: 323.940391\n",
      "Average loss at step 732900: 321.662655\n",
      "Average loss at step 733000: 324.582374\n",
      "Average loss at step 733100: 321.178209\n",
      "Average loss at step 733200: 322.956182\n",
      "Average loss at step 733300: 323.247793\n",
      "Average loss at step 733400: 319.936856\n",
      "Average loss at step 733500: 321.348782\n",
      "Average loss at step 839100: 314.658187\n",
      "Average loss at step 839200: 316.961183\n",
      "Average loss at step 839300: 314.356509\n",
      "Average loss at step 839400: 317.135501\n",
      "Average loss at step 839500: 319.493211\n",
      "Average loss at step 839600: 314.712853\n",
      "Average loss at step 839700: 316.642215\n",
      "Average loss at step 839800: 314.523722\n",
      "Average loss at step 839900: 314.560661\n",
      "Average loss at step 840000: 319.223345\n",
      "Graph 168: 29 nodes\n",
      "Average loss at step 840100: 336.093979\n",
      "Average loss at step 840200: 332.308580\n",
      "Average loss at step 840300: 331.862521\n",
      "Average loss at step 840400: 331.478900\n",
      "Average loss at step 840500: 328.961539\n",
      "Average loss at step 840600: 330.137718\n",
      "Average loss at step 840700: 329.668774\n",
      "Average loss at step 840800: 331.802861\n",
      "Average loss at step 840900: 329.923093\n",
      "Average loss at step 841000: 330.465905\n",
      "Average loss at step 841100: 334.795351\n",
      "Average loss at step 841200: 329.370749\n",
      "Average loss at step 841300: 329.457333\n",
      "Average loss at step 841400: 327.564073\n",
      "Average loss at step 841500: 333.285627\n",
      "Average loss at step 841600: 330.997691\n",
      "Average loss at step 841700: 326.028626\n",
      "Average loss at step 841800: 331.053276\n",
      "Average loss at step 841900: 325.776780\n",
      "Average loss at step 842000: 327.148101\n",
      "Average loss at step 842100: 330.730093\n",
      "Average loss at step 842200: 327.706163\n",
      "Average loss at step 842300: 328.551220\n",
      "Average loss at step 842400: 331.933589\n",
      "Average loss at step 842500: 326.085268\n",
      "Average loss at step 842600: 330.192125\n",
      "Average loss at step 842700: 326.921567\n",
      "Average loss at step 842800: 333.309093\n",
      "Average loss at step 842900: 334.949634\n",
      "Average loss at step 843000: 329.008758\n",
      "Average loss at step 843100: 334.511859\n",
      "Average loss at step 843200: 330.794782\n",
      "Average loss at step 843300: 330.398770\n",
      "Average loss at step 843400: 328.602344\n",
      "Average loss at step 843500: 332.025352\n",
      "Average loss at step 843600: 326.065993\n",
      "Average loss at step 843700: 330.225859\n",
      "Average loss at step 843800: 331.044879\n",
      "Average loss at step 843900: 330.015133\n",
      "Average loss at step 844000: 328.601889\n",
      "Average loss at step 844100: 327.430593\n",
      "Average loss at step 844200: 327.793361\n",
      "Average loss at step 844300: 329.452215\n",
      "Average loss at step 844400: 331.276891\n",
      "Average loss at step 844500: 328.600995\n",
      "Average loss at step 844600: 328.825844\n",
      "Average loss at step 844700: 328.295620\n",
      "Average loss at step 844800: 329.719156\n",
      "Average loss at step 844900: 331.703471\n",
      "Average loss at step 845000: 328.935670\n",
      "Graph 169: 17 nodes\n",
      "Average loss at step 845100: 342.201808\n",
      "Average loss at step 845200: 339.315011\n",
      "Average loss at step 845300: 336.683777\n",
      "Average loss at step 845400: 334.768900\n",
      "Average loss at step 845500: 337.131203\n",
      "Average loss at step 845600: 338.190278\n",
      "Average loss at step 845700: 332.156878\n",
      "Average loss at step 845800: 334.222857\n",
      "Average loss at step 845900: 338.504711\n",
      "Average loss at step 846000: 334.100984\n",
      "Average loss at step 846100: 333.300367\n",
      "Average loss at step 846200: 331.148760\n",
      "Average loss at step 846300: 334.746613\n",
      "Average loss at step 846400: 333.838208\n",
      "Average loss at step 846500: 339.167994\n",
      "Average loss at step 846600: 330.848627\n",
      "Average loss at step 846700: 332.136116\n",
      "Average loss at step 846800: 331.808060\n",
      "Average loss at step 846900: 336.753235\n",
      "Average loss at step 847000: 335.439917\n",
      "Average loss at step 847100: 334.437858\n",
      "Average loss at step 847200: 334.444235\n",
      "Average loss at step 847300: 332.173677\n",
      "Average loss at step 847400: 335.013983\n",
      "Average loss at step 847500: 332.057827\n",
      "Average loss at step 847600: 334.078814\n",
      "Average loss at step 847700: 331.795543\n",
      "Average loss at step 847800: 331.726730\n",
      "Average loss at step 847900: 330.093415\n",
      "Average loss at step 848000: 331.889638\n",
      "Average loss at step 848100: 334.174889\n",
      "Average loss at step 848200: 333.561134\n",
      "Average loss at step 848300: 334.254615\n",
      "Average loss at step 848400: 334.527676\n",
      "Average loss at step 848500: 335.581405\n",
      "Average loss at step 848600: 334.853041\n",
      "Average loss at step 848700: 335.871150\n",
      "Average loss at step 848800: 331.643352\n",
      "Average loss at step 848900: 334.100977\n",
      "Average loss at step 849000: 334.503033\n",
      "Average loss at step 849100: 336.053517\n",
      "Average loss at step 849200: 335.288134\n",
      "Average loss at step 849300: 332.705433\n",
      "Average loss at step 849400: 335.781364\n",
      "Average loss at step 849500: 332.818182\n",
      "Average loss at step 849600: 336.269680\n",
      "Average loss at step 849700: 333.495698\n",
      "Average loss at step 849800: 333.029070\n",
      "Average loss at step 849900: 335.793613\n",
      "Average loss at step 850000: 332.196187\n",
      "Graph 170: 16 nodes\n",
      "Average loss at step 850100: 338.583197\n",
      "Average loss at step 850200: 337.160073\n",
      "Average loss at step 850300: 338.004866\n",
      "Average loss at step 850400: 336.884314\n",
      "Average loss at step 850500: 334.388032\n",
      "Average loss at step 850600: 334.801736\n",
      "Average loss at step 850700: 332.163476\n",
      "Average loss at step 850800: 330.155234\n",
      "Average loss at step 850900: 331.298471\n",
      "Average loss at step 851000: 335.920052\n",
      "Average loss at step 851100: 332.211110\n",
      "Average loss at step 851200: 334.048316\n",
      "Average loss at step 851300: 336.114962\n",
      "Average loss at step 851400: 328.699841\n",
      "Average loss at step 851500: 336.874371\n",
      "Average loss at step 851600: 330.642957\n",
      "Average loss at step 851700: 331.927087\n",
      "Average loss at step 851800: 331.709289\n",
      "Average loss at step 851900: 335.876184\n",
      "Average loss at step 852000: 329.026842\n",
      "Average loss at step 852100: 334.908834\n",
      "Average loss at step 852200: 330.878253\n",
      "Average loss at step 852300: 332.931556\n",
      "Average loss at step 852400: 332.123784\n",
      "Average loss at step 852500: 333.219821\n",
      "Average loss at step 852600: 334.318350\n",
      "Average loss at step 852700: 336.414412\n",
      "Average loss at step 852800: 328.127156\n",
      "Average loss at step 852900: 334.112865\n",
      "Average loss at step 853000: 333.481086\n",
      "Average loss at step 853100: 332.766172\n",
      "Average loss at step 853200: 334.650455\n",
      "Average loss at step 853300: 331.043907\n",
      "Average loss at step 853400: 335.099126\n",
      "Average loss at step 853500: 332.451750\n",
      "Average loss at step 853600: 333.407310\n",
      "Average loss at step 853700: 332.106515\n",
      "Average loss at step 853800: 337.600517\n",
      "Average loss at step 853900: 331.695286\n",
      "Average loss at step 854000: 330.919113\n",
      "Average loss at step 854100: 329.702350\n",
      "Average loss at step 854200: 332.686921\n",
      "Average loss at step 854300: 329.717893\n",
      "Average loss at step 854400: 330.861001\n",
      "Average loss at step 854500: 328.842439\n",
      "Average loss at step 854600: 328.839329\n",
      "Average loss at step 854700: 333.324099\n",
      "Average loss at step 854800: 332.177317\n",
      "Average loss at step 854900: 334.943481\n",
      "Average loss at step 855000: 335.709684\n",
      "Time: 23.5011811256\n",
      "Graph 171: 16 nodes\n",
      "Average loss at step 855100: 332.036529\n",
      "Average loss at step 855200: 329.886295\n",
      "Average loss at step 855300: 334.249987\n",
      "Average loss at step 855400: 328.312279\n",
      "Average loss at step 855500: 326.291796\n",
      "Average loss at step 855600: 328.251288\n",
      "Average loss at step 855700: 328.121102\n",
      "Average loss at step 855800: 329.335521\n",
      "Average loss at step 855900: 328.158537\n",
      "Average loss at step 856000: 330.249014\n",
      "Average loss at step 856100: 329.711218\n",
      "Average loss at step 856200: 329.013623\n",
      "Average loss at step 856300: 329.383137\n",
      "Average loss at step 856400: 331.007648\n",
      "Average loss at step 856500: 332.820987\n",
      "Average loss at step 856600: 329.636900\n",
      "Average loss at step 856700: 328.490711\n",
      "Average loss at step 856800: 329.134293\n",
      "Average loss at step 856900: 327.683210\n",
      "Average loss at step 857000: 328.056942\n",
      "Average loss at step 857100: 328.574599\n",
      "Average loss at step 857200: 327.870212\n",
      "Average loss at step 857300: 327.272173\n",
      "Average loss at step 857400: 325.764241\n",
      "Average loss at step 857500: 327.297697\n",
      "Average loss at step 857600: 327.583879\n",
      "Average loss at step 857700: 329.602226\n",
      "Average loss at step 857800: 326.571793\n",
      "Average loss at step 857900: 327.399717\n",
      "Average loss at step 858000: 331.132863\n",
      "Average loss at step 858100: 327.801105\n",
      "Average loss at step 858200: 326.853631\n",
      "Average loss at step 858300: 327.585670\n",
      "Average loss at step 858400: 332.372572\n",
      "Average loss at step 858500: 328.010664\n",
      "Average loss at step 858600: 328.266232\n",
      "Average loss at step 858700: 326.821361\n",
      "Average loss at step 858800: 329.147085\n",
      "Average loss at step 858900: 325.699479\n",
      "Average loss at step 859000: 326.258070\n",
      "Average loss at step 859100: 323.679572\n",
      "Average loss at step 859200: 329.196239\n",
      "Average loss at step 859300: 324.845199\n",
      "Average loss at step 859400: 327.541693\n",
      "Average loss at step 859500: 324.969808\n",
      "Average loss at step 859600: 328.903145\n",
      "Average loss at step 859700: 327.873590\n",
      "Average loss at step 859800: 325.982722\n",
      "Average loss at step 859900: 327.311864\n",
      "Average loss at step 860000: 325.153987\n",
      "Graph 172: 17 nodes\n",
      "Average loss at step 860100: 336.247829\n",
      "Average loss at step 860200: 331.881704\n",
      "Average loss at step 860300: 330.592935\n",
      "Average loss at step 860400: 330.559842\n",
      "Average loss at step 860500: 330.955722\n",
      "Average loss at step 860600: 332.604112\n",
      "Average loss at step 860700: 332.323206\n",
      "Average loss at step 860800: 330.339117\n",
      "Average loss at step 860900: 331.117566\n",
      "Average loss at step 861000: 333.263960\n",
      "Average loss at step 861100: 330.310663\n",
      "Average loss at step 861200: 333.383308\n",
      "Average loss at step 861300: 327.829472\n",
      "Average loss at step 861400: 331.234560\n",
      "Average loss at step 861500: 332.519406\n",
      "Average loss at step 861600: 332.297937\n",
      "Average loss at step 861700: 329.558498\n",
      "Average loss at step 861800: 329.194629\n",
      "Average loss at step 861900: 329.098729\n",
      "Average loss at step 862000: 327.662051\n",
      "Average loss at step 862100: 329.603979\n",
      "Average loss at step 862200: 331.217770\n",
      "Average loss at step 862300: 330.692697\n",
      "Average loss at step 862400: 329.508463\n",
      "Average loss at step 862500: 330.819004\n",
      "Average loss at step 862600: 330.234303\n",
      "Average loss at step 862700: 331.832479\n",
      "Average loss at step 862800: 329.661488\n",
      "Average loss at step 862900: 331.571235\n",
      "Average loss at step 863000: 328.429122\n",
      "Average loss at step 863100: 328.140282\n",
      "Average loss at step 863200: 329.936050\n",
      "Average loss at step 863300: 330.211990\n",
      "Average loss at step 863400: 329.186962\n",
      "Average loss at step 863500: 330.366477\n",
      "Average loss at step 863600: 330.125576\n",
      "Average loss at step 863700: 329.867921\n",
      "Average loss at step 863800: 328.041237\n",
      "Average loss at step 863900: 329.637984\n",
      "Average loss at step 864000: 329.989902\n",
      "Average loss at step 864100: 327.706850\n",
      "Average loss at step 864200: 331.494987\n",
      "Average loss at step 864300: 328.820340\n",
      "Average loss at step 864400: 332.960038\n",
      "Average loss at step 864500: 328.584055\n",
      "Average loss at step 864600: 325.373102\n",
      "Average loss at step 864700: 326.704751\n",
      "Average loss at step 864800: 330.338858\n",
      "Average loss at step 864900: 330.095685\n",
      "Average loss at step 865000: 329.764578\n",
      "Graph 173: 26 nodes\n",
      "Average loss at step 865100: 340.438727\n",
      "Average loss at step 865200: 338.829702\n",
      "Average loss at step 865300: 335.552314\n",
      "Average loss at step 865400: 337.734786\n",
      "Average loss at step 865500: 335.231337\n",
      "Average loss at step 865600: 336.285699\n",
      "Average loss at step 865700: 335.685523\n",
      "Average loss at step 865800: 333.818120\n",
      "Average loss at step 865900: 337.446090\n",
      "Average loss at step 866000: 336.816107\n",
      "Average loss at step 866100: 335.274616\n",
      "Average loss at step 866200: 337.206826\n",
      "Average loss at step 866300: 337.251189\n",
      "Average loss at step 866400: 338.022308\n",
      "Average loss at step 866500: 335.623781\n",
      "Average loss at step 866600: 334.736374\n",
      "Average loss at step 866700: 331.807498\n",
      "Average loss at step 866800: 335.626329\n",
      "Average loss at step 866900: 333.650628\n",
      "Average loss at step 867000: 334.496458\n",
      "Average loss at step 867100: 338.134827\n",
      "Average loss at step 867200: 337.295752\n",
      "Average loss at step 867300: 337.109202\n",
      "Average loss at step 867400: 337.573182\n",
      "Average loss at step 867500: 337.282271\n",
      "Average loss at step 867600: 337.508108\n",
      "Average loss at step 867700: 338.189416\n",
      "Average loss at step 867800: 337.605558\n",
      "Average loss at step 867900: 337.261275\n",
      "Average loss at step 868000: 335.892764\n",
      "Average loss at step 868100: 335.250441\n",
      "Average loss at step 868200: 336.441947\n",
      "Average loss at step 868300: 335.628716\n",
      "Average loss at step 868400: 333.264981\n",
      "Average loss at step 868500: 335.383131\n",
      "Average loss at step 868600: 333.877577\n",
      "Average loss at step 868700: 336.737473\n",
      "Average loss at step 868800: 332.940255\n",
      "Average loss at step 868900: 334.082020\n",
      "Average loss at step 869000: 337.248965\n",
      "Average loss at step 869100: 333.229408\n",
      "Average loss at step 869200: 337.427496\n",
      "Average loss at step 869300: 336.347656\n",
      "Average loss at step 869400: 336.313060\n",
      "Average loss at step 869500: 335.706567\n",
      "Average loss at step 869600: 335.439263\n",
      "Average loss at step 869700: 339.481563\n",
      "Average loss at step 869800: 337.947605\n",
      "Average loss at step 869900: 335.341126\n",
      "Average loss at step 870000: 335.176450\n",
      "Graph 174: 14 nodes\n",
      "Average loss at step 870100: 342.239619\n",
      "Average loss at step 870200: 336.138078\n",
      "Average loss at step 870300: 332.982306\n",
      "Average loss at step 870400: 328.464332\n",
      "Average loss at step 870500: 332.178147\n",
      "Average loss at step 870600: 332.316286\n",
      "Average loss at step 870700: 330.380864\n",
      "Average loss at step 870800: 333.473284\n",
      "Average loss at step 870900: 327.070044\n",
      "Average loss at step 871000: 326.544519\n",
      "Average loss at step 871100: 333.475616\n",
      "Average loss at step 871200: 330.294511\n",
      "Average loss at step 871300: 330.864113\n",
      "Average loss at step 871400: 329.428349\n",
      "Average loss at step 871500: 331.236031\n",
      "Average loss at step 871600: 331.620516\n",
      "Average loss at step 871700: 331.527930\n",
      "Average loss at step 871800: 329.846589\n",
      "Average loss at step 871900: 327.896173\n",
      "Average loss at step 872000: 330.489711\n",
      "Average loss at step 872100: 333.008416\n",
      "Average loss at step 872200: 328.706108\n",
      "Average loss at step 872300: 327.590105\n",
      "Average loss at step 872400: 330.966553\n",
      "Average loss at step 872500: 334.059017\n",
      "Average loss at step 872600: 329.484626\n",
      "Average loss at step 872700: 332.809564\n",
      "Average loss at step 872800: 330.415371\n",
      "Average loss at step 872900: 329.090886\n",
      "Average loss at step 873000: 329.053185\n",
      "Average loss at step 873100: 331.144954\n",
      "Average loss at step 873200: 331.066702\n",
      "Average loss at step 873300: 329.306639\n",
      "Average loss at step 873400: 331.229635\n",
      "Average loss at step 873500: 331.837411\n",
      "Average loss at step 873600: 332.596477\n",
      "Average loss at step 873700: 330.713645\n",
      "Average loss at step 873800: 330.495757\n",
      "Average loss at step 873900: 332.785064\n",
      "Average loss at step 874000: 328.332973\n",
      "Average loss at step 874100: 329.146491\n",
      "Average loss at step 874200: 330.053114\n",
      "Average loss at step 874300: 327.230260\n",
      "Average loss at step 874400: 325.795628\n",
      "Average loss at step 874500: 328.966441\n",
      "Average loss at step 874600: 327.329929\n",
      "Average loss at step 874700: 331.910483\n",
      "Average loss at step 874800: 333.578130\n",
      "Average loss at step 874900: 328.586587\n",
      "Average loss at step 875000: 327.638831\n",
      "Graph 175: 14 nodes\n",
      "Average loss at step 875100: 376.707197\n",
      "Average loss at step 875200: 345.662353\n",
      "Average loss at step 875300: 332.723733\n",
      "Average loss at step 993300: 329.900975\n",
      "Average loss at step 993400: 330.439474\n",
      "Average loss at step 993500: 331.426847\n",
      "Average loss at step 993600: 332.809244\n",
      "Average loss at step 993700: 329.387040\n",
      "Average loss at step 993800: 332.331426\n",
      "Average loss at step 993900: 332.062967\n",
      "Average loss at step 994000: 330.617353\n",
      "Average loss at step 994100: 328.901334\n",
      "Average loss at step 994200: 330.373657\n",
      "Average loss at step 994300: 332.953362\n",
      "Average loss at step 994400: 329.065248\n",
      "Average loss at step 994500: 331.166823\n",
      "Average loss at step 994600: 329.903876\n",
      "Average loss at step 994700: 331.765370\n",
      "Average loss at step 994800: 330.217792\n",
      "Average loss at step 994900: 333.499843\n",
      "Average loss at step 995000: 330.632350\n",
      "Graph 199: 38 nodes\n",
      "Average loss at step 995100: 332.565027\n",
      "Average loss at step 995200: 332.122935\n",
      "Average loss at step 995300: 336.121785\n",
      "Average loss at step 995400: 331.035845\n",
      "Average loss at step 995500: 329.788134\n",
      "Average loss at step 995600: 329.995351\n",
      "Average loss at step 995700: 330.533403\n",
      "Average loss at step 995800: 331.616381\n",
      "Average loss at step 995900: 332.765385\n",
      "Average loss at step 996000: 333.471417\n",
      "Average loss at step 996100: 330.538919\n",
      "Average loss at step 996200: 332.493248\n",
      "Average loss at step 996300: 331.131181\n",
      "Average loss at step 996400: 331.881130\n",
      "Average loss at step 996500: 330.413329\n",
      "Average loss at step 996600: 328.970360\n",
      "Average loss at step 996700: 333.509009\n",
      "Average loss at step 996800: 329.386680\n",
      "Average loss at step 996900: 330.488472\n",
      "Average loss at step 997000: 327.177741\n",
      "Average loss at step 997100: 331.140928\n",
      "Average loss at step 997200: 331.803626\n",
      "Average loss at step 997300: 330.296064\n",
      "Average loss at step 997400: 332.234558\n",
      "Average loss at step 997500: 329.771059\n",
      "Average loss at step 997600: 328.993570\n",
      "Average loss at step 997700: 331.184163\n",
      "Average loss at step 997800: 328.976734\n",
      "Average loss at step 997900: 331.575427\n",
      "Average loss at step 998000: 331.114499\n",
      "Average loss at step 998100: 326.665855\n",
      "Average loss at step 998200: 329.229141\n",
      "Average loss at step 998300: 328.334970\n",
      "Average loss at step 998400: 328.159965\n",
      "Average loss at step 998500: 332.880268\n",
      "Average loss at step 998600: 331.146397\n",
      "Average loss at step 998700: 335.419837\n",
      "Average loss at step 998800: 330.721395\n",
      "Average loss at step 998900: 329.021544\n",
      "Average loss at step 999000: 328.900066\n",
      "Average loss at step 999100: 331.315582\n",
      "Average loss at step 999200: 329.534657\n",
      "Average loss at step 999300: 329.300762\n",
      "Average loss at step 999400: 331.209888\n",
      "Average loss at step 999500: 329.819231\n",
      "Average loss at step 999600: 331.271696\n",
      "Average loss at step 999700: 330.100146\n",
      "Average loss at step 999800: 331.190986\n",
      "Average loss at step 999900: 333.747843\n",
      "Average loss at step 1000000: 331.804138\n",
      "Graph 200: 25 nodes\n",
      "Average loss at step 1000100: 340.293925\n",
      "Average loss at step 1000200: 338.526528\n",
      "Average loss at step 1000300: 334.571299\n",
      "Average loss at step 1000400: 335.599301\n",
      "Average loss at step 1000500: 335.153758\n",
      "Average loss at step 1000600: 334.563729\n",
      "Average loss at step 1000700: 335.458901\n",
      "Average loss at step 1000800: 338.782673\n",
      "Average loss at step 1000900: 334.352701\n",
      "Average loss at step 1001000: 336.470542\n",
      "Average loss at step 1001100: 335.446921\n",
      "Average loss at step 1001200: 337.770971\n",
      "Average loss at step 1001300: 332.682123\n",
      "Average loss at step 1001400: 337.023235\n",
      "Average loss at step 1001500: 331.026719\n",
      "Average loss at step 1001600: 335.265791\n",
      "Average loss at step 1001700: 337.976978\n",
      "Average loss at step 1001800: 333.084170\n",
      "Average loss at step 1001900: 334.320148\n",
      "Average loss at step 1002000: 332.021262\n",
      "Average loss at step 1002100: 334.800746\n",
      "Average loss at step 1002200: 337.502585\n",
      "Average loss at step 1002300: 334.893189\n",
      "Average loss at step 1002400: 335.220702\n",
      "Average loss at step 1002500: 334.392381\n",
      "Average loss at step 1002600: 335.034874\n",
      "Average loss at step 1002700: 333.728656\n",
      "Average loss at step 1002800: 330.653091\n",
      "Average loss at step 1002900: 331.973186\n",
      "Average loss at step 1003000: 337.257979\n",
      "Average loss at step 1003100: 336.297749\n",
      "Average loss at step 1003200: 332.324373\n",
      "Average loss at step 1003300: 336.664551\n",
      "Average loss at step 1003400: 335.695894\n",
      "Average loss at step 1003500: 333.827695\n",
      "Average loss at step 1003600: 330.508062\n",
      "Average loss at step 1003700: 330.608231\n",
      "Average loss at step 1003800: 330.950142\n",
      "Average loss at step 1003900: 333.102489\n",
      "Average loss at step 1004000: 335.284500\n",
      "Average loss at step 1004100: 336.470583\n",
      "Average loss at step 1004200: 331.606745\n",
      "Average loss at step 1004300: 334.956907\n",
      "Average loss at step 1004400: 336.058684\n",
      "Average loss at step 1004500: 332.417860\n",
      "Average loss at step 1004600: 331.688926\n",
      "Average loss at step 1004700: 332.962264\n",
      "Average loss at step 1004800: 333.549295\n",
      "Average loss at step 1004900: 328.557048\n",
      "Average loss at step 1005000: 333.120983\n",
      "Time: 23.6308469772\n",
      "Graph 201: 49 nodes\n",
      "Average loss at step 1005100: 337.148591\n",
      "Average loss at step 1005200: 332.839688\n",
      "Average loss at step 1005300: 334.135131\n",
      "Average loss at step 1005400: 332.884354\n",
      "Average loss at step 1005500: 335.145540\n",
      "Average loss at step 1005600: 329.207063\n",
      "Average loss at step 1005700: 332.957249\n",
      "Average loss at step 1005800: 328.625012\n",
      "Average loss at step 1005900: 330.906734\n",
      "Average loss at step 1006000: 330.383891\n",
      "Average loss at step 1006100: 329.901384\n",
      "Average loss at step 1006200: 333.593544\n",
      "Average loss at step 1006300: 331.772759\n",
      "Average loss at step 1006400: 334.384962\n",
      "Average loss at step 1006500: 329.423426\n",
      "Average loss at step 1006600: 335.427869\n",
      "Average loss at step 1006700: 334.480522\n",
      "Average loss at step 1006800: 331.572041\n",
      "Average loss at step 1006900: 329.021971\n",
      "Average loss at step 1007000: 327.982762\n",
      "Average loss at step 1007100: 334.125534\n",
      "Average loss at step 1007200: 331.574535\n",
      "Average loss at step 1007300: 331.063962\n",
      "Average loss at step 1007400: 331.543273\n",
      "Average loss at step 1007500: 334.184633\n",
      "Average loss at step 1007600: 328.393612\n",
      "Average loss at step 1007700: 332.221618\n",
      "Average loss at step 1007800: 331.727847\n",
      "Average loss at step 1007900: 329.431051\n",
      "Average loss at step 1008000: 330.555493\n",
      "Average loss at step 1008100: 333.402704\n",
      "Average loss at step 1008200: 332.036904\n",
      "Average loss at step 1008300: 330.724803\n",
      "Average loss at step 1008400: 333.835605\n",
      "Average loss at step 1008500: 329.583094\n",
      "Average loss at step 1008600: 330.628305\n",
      "Average loss at step 1008700: 332.301301\n",
      "Average loss at step 1008800: 329.033945\n",
      "Average loss at step 1008900: 333.438064\n",
      "Average loss at step 1009000: 330.144757\n",
      "Average loss at step 1009100: 333.137019\n",
      "Average loss at step 1009200: 330.748590\n",
      "Average loss at step 1009300: 329.517533\n",
      "Average loss at step 1009400: 326.846302\n",
      "Average loss at step 1009500: 330.995975\n",
      "Average loss at step 1009600: 329.653549\n",
      "Average loss at step 1009700: 334.174126\n",
      "Average loss at step 1009800: 329.164414\n",
      "Average loss at step 1009900: 330.110439\n",
      "Average loss at step 1010000: 331.871703\n",
      "Graph 202: 16 nodes\n",
      "Average loss at step 1010100: 340.968807\n",
      "Average loss at step 1010200: 338.071748\n",
      "Average loss at step 1010300: 339.426232\n",
      "Average loss at step 1010400: 340.698187\n",
      "Average loss at step 1010500: 337.511004\n",
      "Average loss at step 1010600: 340.236849\n",
      "Average loss at step 1010700: 337.555829\n",
      "Average loss at step 1010800: 341.690655\n",
      "Average loss at step 1010900: 334.565546\n",
      "Average loss at step 1011000: 336.075851\n",
      "Average loss at step 1011100: 335.723676\n",
      "Average loss at step 1011200: 338.310327\n",
      "Average loss at step 1011300: 337.324172\n",
      "Average loss at step 1011400: 336.130043\n",
      "Average loss at step 1011500: 335.107646\n",
      "Average loss at step 1011600: 334.621929\n",
      "Average loss at step 1011700: 337.619531\n",
      "Average loss at step 1011800: 332.958032\n",
      "Average loss at step 1011900: 336.934915\n",
      "Average loss at step 1012000: 335.461342\n",
      "Average loss at step 1012100: 334.940598\n",
      "Average loss at step 1012200: 337.309814\n",
      "Average loss at step 1012300: 338.249730\n",
      "Average loss at step 1012400: 335.344274\n",
      "Average loss at step 1012500: 335.052310\n",
      "Average loss at step 1012600: 335.558733\n",
      "Average loss at step 1012700: 334.027421\n",
      "Average loss at step 1012800: 337.250201\n",
      "Average loss at step 1012900: 335.741243\n",
      "Average loss at step 1013000: 335.232045\n",
      "Average loss at step 1013100: 336.074678\n",
      "Average loss at step 1013200: 332.163761\n",
      "Average loss at step 1013300: 339.783593\n",
      "Average loss at step 1013400: 334.987332\n",
      "Average loss at step 1013500: 336.463733\n",
      "Average loss at step 1013600: 334.499998\n",
      "Average loss at step 1013700: 334.536888\n",
      "Average loss at step 1013800: 335.521594\n",
      "Average loss at step 1013900: 338.225366\n",
      "Average loss at step 1014000: 338.879777\n",
      "Average loss at step 1014100: 334.781151\n",
      "Average loss at step 1014200: 336.320742\n",
      "Average loss at step 1014300: 335.600235\n",
      "Average loss at step 1014400: 336.821886\n",
      "Average loss at step 1014500: 334.823683\n",
      "Average loss at step 1014600: 338.046689\n",
      "Average loss at step 1014700: 336.451244\n",
      "Average loss at step 1014800: 336.368824\n",
      "Average loss at step 1014900: 335.229721\n",
      "Average loss at step 1015000: 336.056239\n",
      "Graph 203: 33 nodes\n",
      "Average loss at step 1015100: 341.602931\n",
      "Average loss at step 1015200: 337.912730\n",
      "Average loss at step 1015300: 334.569944\n",
      "Average loss at step 1015400: 336.724618\n",
      "Average loss at step 1015500: 331.958921\n",
      "Average loss at step 1015600: 330.557216\n",
      "Average loss at step 1015700: 334.581087\n",
      "Average loss at step 1015800: 329.145343\n",
      "Average loss at step 1015900: 334.215946\n",
      "Average loss at step 1016000: 335.441930\n",
      "Average loss at step 1016100: 330.130535\n",
      "Average loss at step 1016200: 333.726244\n",
      "Average loss at step 1016300: 336.266265\n",
      "Average loss at step 1016400: 331.212182\n",
      "Average loss at step 1016500: 332.057545\n",
      "Average loss at step 1016600: 331.727152\n",
      "Average loss at step 1016700: 334.344810\n",
      "Average loss at step 1016800: 333.597999\n",
      "Average loss at step 1016900: 332.072020\n",
      "Average loss at step 1017000: 335.190689\n",
      "Average loss at step 1017100: 326.048383\n",
      "Average loss at step 1017200: 330.607520\n",
      "Average loss at step 1017300: 334.207740\n",
      "Average loss at step 1017400: 332.875746\n",
      "Average loss at step 1017500: 331.130670\n",
      "Average loss at step 1017600: 332.287426\n",
      "Average loss at step 1017700: 333.667514\n",
      "Average loss at step 1017800: 330.124375\n",
      "Average loss at step 1017900: 336.056846\n",
      "Average loss at step 1018000: 331.797915\n",
      "Average loss at step 1018100: 332.626251\n",
      "Average loss at step 1018200: 328.426444\n",
      "Average loss at step 1018300: 330.269184\n",
      "Average loss at step 1018400: 331.288348\n",
      "Average loss at step 1018500: 333.643987\n",
      "Average loss at step 1018600: 330.206635\n",
      "Average loss at step 1018700: 328.049244\n",
      "Average loss at step 1018800: 332.129133\n",
      "Average loss at step 1018900: 329.976209\n",
      "Average loss at step 1019000: 331.254500\n",
      "Average loss at step 1019100: 332.670169\n",
      "Average loss at step 1019200: 329.349460\n",
      "Average loss at step 1019300: 329.489798\n",
      "Average loss at step 1019400: 330.910211\n",
      "Average loss at step 1019500: 332.294199\n",
      "Average loss at step 1019600: 328.546639\n",
      "Average loss at step 1019700: 327.743834\n",
      "Average loss at step 1019800: 329.900075\n",
      "Average loss at step 1019900: 331.980368\n",
      "Average loss at step 1020000: 330.937487\n",
      "Graph 204: 22 nodes\n",
      "Average loss at step 1020100: 337.684950\n",
      "Average loss at step 1020200: 339.662969\n",
      "Average loss at step 1020300: 332.458919\n",
      "Average loss at step 1020400: 333.578684\n",
      "Average loss at step 1020500: 333.439125\n",
      "Average loss at step 1020600: 334.128034\n",
      "Average loss at step 1020700: 330.341495\n",
      "Average loss at step 1020800: 334.816772\n",
      "Average loss at step 1020900: 334.428786\n",
      "Average loss at step 1021000: 331.430980\n",
      "Average loss at step 1021100: 333.813296\n",
      "Average loss at step 1021200: 330.939621\n",
      "Average loss at step 1021300: 332.811101\n",
      "Average loss at step 1021400: 329.072126\n",
      "Average loss at step 1021500: 329.253615\n",
      "Average loss at step 1021600: 331.944682\n",
      "Average loss at step 1021700: 332.591969\n",
      "Average loss at step 1021800: 329.723178\n",
      "Average loss at step 1021900: 328.111585\n",
      "Average loss at step 1022000: 332.146365\n",
      "Average loss at step 1022100: 326.223198\n",
      "Average loss at step 1022200: 332.270952\n",
      "Average loss at step 1022300: 328.560452\n",
      "Average loss at step 1022400: 331.782335\n",
      "Average loss at step 1022500: 329.267917\n",
      "Average loss at step 1022600: 331.627166\n",
      "Average loss at step 1022700: 329.418569\n",
      "Average loss at step 1022800: 328.054188\n",
      "Average loss at step 1022900: 331.444849\n",
      "Average loss at step 1023000: 331.037370\n",
      "Average loss at step 1023100: 331.110998\n",
      "Average loss at step 1023200: 330.818202\n",
      "Average loss at step 1023300: 330.117884\n",
      "Average loss at step 1023400: 333.608570\n",
      "Average loss at step 1023500: 330.696514\n",
      "Average loss at step 1023600: 329.863203\n",
      "Average loss at step 1023700: 329.857938\n",
      "Average loss at step 1023800: 328.560650\n",
      "Average loss at step 1023900: 329.746781\n",
      "Average loss at step 1024000: 328.882199\n",
      "Average loss at step 1024100: 327.841210\n",
      "Average loss at step 1024200: 332.470925\n",
      "Average loss at step 1024300: 332.445673\n",
      "Average loss at step 1024400: 330.670469\n",
      "Average loss at step 1024500: 330.345716\n",
      "Average loss at step 1024600: 329.198354\n",
      "Average loss at step 1024700: 329.233040\n",
      "Average loss at step 1024800: 330.985351\n",
      "Average loss at step 1024900: 331.345274\n",
      "Average loss at step 1025000: 329.630544\n",
      "Graph 205: 35 nodes\n",
      "Average loss at step 1025100: 333.714637\n",
      "Average loss at step 1025200: 330.350784\n",
      "Average loss at step 1025300: 332.123935\n",
      "Average loss at step 1025400: 329.508940\n",
      "Average loss at step 1025500: 325.347721\n",
      "Average loss at step 1025600: 329.578622\n",
      "Average loss at step 1025700: 331.342123\n",
      "Average loss at step 1025800: 331.520180\n",
      "Average loss at step 1025900: 327.441406\n",
      "Average loss at step 1026000: 332.243831\n",
      "Average loss at step 1026100: 329.602025\n",
      "Average loss at step 1026200: 331.850116\n",
      "Average loss at step 1026300: 330.187075\n",
      "Average loss at step 1026400: 330.584669\n",
      "Average loss at step 1026500: 328.352230\n",
      "Average loss at step 1026600: 328.611756\n",
      "Average loss at step 1026700: 332.107896\n",
      "Average loss at step 1026800: 329.832305\n",
      "Average loss at step 1026900: 328.864642\n",
      "Average loss at step 1027000: 331.300957\n",
      "Average loss at step 1027100: 330.798489\n",
      "Average loss at step 1027200: 330.583063\n",
      "Average loss at step 1027300: 331.653404\n",
      "Average loss at step 1027400: 329.448968\n",
      "Average loss at step 1027500: 328.366606\n",
      "Average loss at step 1027600: 330.562157\n",
      "Average loss at step 1027700: 330.762352\n",
      "Average loss at step 1027800: 327.764956\n",
      "Average loss at step 1027900: 326.732187\n",
      "Average loss at step 1028000: 333.596916\n",
      "Average loss at step 1028100: 328.317814\n",
      "Average loss at step 1028200: 329.094101\n",
      "Average loss at step 1028300: 329.998887\n",
      "Average loss at step 1028400: 329.270971\n",
      "Average loss at step 1028500: 328.667752\n",
      "Average loss at step 1028600: 328.433981\n",
      "Average loss at step 1028700: 330.556911\n",
      "Average loss at step 1028800: 327.763621\n",
      "Average loss at step 1028900: 327.299903\n",
      "Average loss at step 1029000: 326.201320\n",
      "Average loss at step 1029100: 328.325707\n",
      "Average loss at step 1029200: 330.093492\n",
      "Average loss at step 1029300: 328.941322\n",
      "Average loss at step 1029400: 326.763705\n",
      "Average loss at step 1151000: 333.856218\n",
      "Average loss at step 1151100: 332.995858\n",
      "Average loss at step 1151200: 329.175586\n",
      "Average loss at step 1151300: 334.421552\n",
      "Average loss at step 1151400: 331.542613\n",
      "Average loss at step 1151500: 331.352197\n",
      "Average loss at step 1151600: 332.115603\n",
      "Average loss at step 1151700: 329.942057\n",
      "Average loss at step 1151800: 329.988684\n",
      "Average loss at step 1151900: 330.452331\n",
      "Average loss at step 1152000: 333.281991\n",
      "Average loss at step 1152100: 330.899856\n",
      "Average loss at step 1152200: 328.907454\n",
      "Average loss at step 1152300: 330.627754\n",
      "Average loss at step 1152400: 334.109507\n",
      "Average loss at step 1152500: 328.703255\n",
      "Average loss at step 1152600: 330.925091\n",
      "Average loss at step 1152700: 330.717692\n",
      "Average loss at step 1152800: 332.210545\n",
      "Average loss at step 1152900: 329.177352\n",
      "Average loss at step 1153000: 332.595425\n",
      "Average loss at step 1153100: 330.049862\n",
      "Average loss at step 1153200: 328.425019\n",
      "Average loss at step 1153300: 333.429112\n",
      "Average loss at step 1153400: 332.094553\n",
      "Average loss at step 1153500: 326.077833\n",
      "Average loss at step 1153600: 329.563807\n",
      "Average loss at step 1153700: 326.994055\n",
      "Average loss at step 1153800: 327.766633\n",
      "Average loss at step 1153900: 329.404896\n",
      "Average loss at step 1154000: 328.752443\n",
      "Average loss at step 1154100: 332.415877\n",
      "Average loss at step 1154200: 330.541327\n",
      "Average loss at step 1154300: 329.030081\n",
      "Average loss at step 1154400: 332.409389\n",
      "Average loss at step 1154500: 332.415429\n",
      "Average loss at step 1154600: 331.735895\n",
      "Average loss at step 1154700: 329.191443\n",
      "Average loss at step 1154800: 333.077633\n",
      "Average loss at step 1154900: 329.906972\n",
      "Average loss at step 1155000: 330.072230\n",
      "Time: 24.0446259975\n",
      "Graph 231: 21 nodes\n",
      "Average loss at step 1155100: 333.056913\n",
      "Average loss at step 1155200: 329.274363\n",
      "Average loss at step 1155300: 329.076980\n",
      "Average loss at step 1155400: 331.569090\n",
      "Average loss at step 1155500: 329.378045\n",
      "Average loss at step 1155600: 329.221242\n",
      "Average loss at step 1155700: 327.110306\n",
      "Average loss at step 1155800: 330.192050\n",
      "Average loss at step 1155900: 332.469225\n",
      "Average loss at step 1156000: 328.827024\n",
      "Average loss at step 1156100: 326.016745\n",
      "Average loss at step 1156200: 329.335041\n",
      "Average loss at step 1156300: 328.038490\n",
      "Average loss at step 1156400: 325.888421\n",
      "Average loss at step 1156500: 324.327747\n",
      "Average loss at step 1156600: 328.592484\n",
      "Average loss at step 1156700: 325.297553\n",
      "Average loss at step 1156800: 326.020765\n",
      "Average loss at step 1156900: 329.163766\n",
      "Average loss at step 1157000: 325.503012\n",
      "Average loss at step 1157100: 325.768098\n",
      "Average loss at step 1157200: 328.324104\n",
      "Average loss at step 1157300: 326.482213\n",
      "Average loss at step 1157400: 327.256045\n",
      "Average loss at step 1157500: 326.121325\n",
      "Average loss at step 1157600: 325.042165\n",
      "Average loss at step 1157700: 323.148057\n",
      "Average loss at step 1157800: 325.838802\n",
      "Average loss at step 1157900: 327.356977\n",
      "Average loss at step 1158000: 328.449164\n",
      "Average loss at step 1158100: 327.788091\n",
      "Average loss at step 1158200: 329.116328\n",
      "Average loss at step 1158300: 328.762918\n",
      "Average loss at step 1158400: 326.567668\n",
      "Average loss at step 1158500: 321.455272\n",
      "Average loss at step 1158600: 327.959036\n",
      "Average loss at step 1158700: 327.871784\n",
      "Average loss at step 1158800: 322.940436\n",
      "Average loss at step 1158900: 325.980828\n",
      "Average loss at step 1159000: 326.897919\n",
      "Average loss at step 1159100: 326.991137\n",
      "Average loss at step 1159200: 324.978590\n",
      "Average loss at step 1159300: 325.409375\n",
      "Average loss at step 1159400: 322.192906\n",
      "Average loss at step 1159500: 328.357176\n",
      "Average loss at step 1159600: 327.619209\n",
      "Average loss at step 1159700: 326.966017\n",
      "Average loss at step 1159800: 327.050226\n",
      "Average loss at step 1159900: 325.844517\n",
      "Average loss at step 1160000: 327.116964\n",
      "Graph 232: 21 nodes\n",
      "Average loss at step 1160100: 335.306643\n",
      "Average loss at step 1160200: 330.136564\n",
      "Average loss at step 1160300: 338.136605\n",
      "Average loss at step 1160400: 335.561449\n",
      "Average loss at step 1160500: 332.217893\n",
      "Average loss at step 1160600: 334.224659\n",
      "Average loss at step 1160700: 332.820099\n",
      "Average loss at step 1160800: 332.403878\n",
      "Average loss at step 1160900: 329.095702\n",
      "Average loss at step 1161000: 333.146615\n",
      "Average loss at step 1161100: 329.494664\n",
      "Average loss at step 1161200: 332.935678\n",
      "Average loss at step 1161300: 331.888180\n",
      "Average loss at step 1161400: 327.218488\n",
      "Average loss at step 1161500: 336.033659\n",
      "Average loss at step 1161600: 333.158847\n",
      "Average loss at step 1161700: 335.470728\n",
      "Average loss at step 1161800: 329.666233\n",
      "Average loss at step 1161900: 330.025417\n",
      "Average loss at step 1162000: 333.387339\n",
      "Average loss at step 1162100: 331.373515\n",
      "Average loss at step 1162200: 327.719108\n",
      "Average loss at step 1162300: 331.057617\n",
      "Average loss at step 1162400: 331.618046\n",
      "Average loss at step 1162500: 329.237066\n",
      "Average loss at step 1162600: 330.953130\n",
      "Average loss at step 1162700: 332.653424\n",
      "Average loss at step 1162800: 331.300740\n",
      "Average loss at step 1162900: 327.394722\n",
      "Average loss at step 1163000: 329.573421\n",
      "Average loss at step 1163100: 328.259841\n",
      "Average loss at step 1163200: 329.394543\n",
      "Average loss at step 1163300: 329.328721\n",
      "Average loss at step 1163400: 329.256117\n",
      "Average loss at step 1163500: 331.479943\n",
      "Average loss at step 1163600: 332.151186\n",
      "Average loss at step 1163700: 329.761453\n",
      "Average loss at step 1163800: 330.903626\n",
      "Average loss at step 1163900: 329.565621\n",
      "Average loss at step 1164000: 328.889092\n",
      "Average loss at step 1164100: 330.118889\n",
      "Average loss at step 1164200: 330.059889\n",
      "Average loss at step 1164300: 327.532238\n",
      "Average loss at step 1164400: 329.197836\n",
      "Average loss at step 1164500: 327.575382\n",
      "Average loss at step 1164600: 330.433659\n",
      "Average loss at step 1164700: 330.701796\n",
      "Average loss at step 1164800: 328.080010\n",
      "Average loss at step 1164900: 326.374417\n",
      "Average loss at step 1165000: 329.045710\n",
      "Graph 233: 23 nodes\n",
      "Average loss at step 1165100: 328.103305\n",
      "Average loss at step 1165200: 324.009258\n",
      "Average loss at step 1165300: 325.838296\n",
      "Average loss at step 1165400: 315.203659\n",
      "Average loss at step 1165500: 319.437943\n",
      "Average loss at step 1165600: 320.645791\n",
      "Average loss at step 1165700: 317.981498\n",
      "Average loss at step 1165800: 319.961726\n",
      "Average loss at step 1165900: 316.998737\n",
      "Average loss at step 1166000: 315.757709\n",
      "Average loss at step 1166100: 318.470470\n",
      "Average loss at step 1166200: 313.724116\n",
      "Average loss at step 1166300: 314.248827\n",
      "Average loss at step 1166400: 316.790287\n",
      "Average loss at step 1166500: 318.854718\n",
      "Average loss at step 1166600: 318.702965\n",
      "Average loss at step 1166700: 317.683030\n",
      "Average loss at step 1166800: 317.486407\n",
      "Average loss at step 1166900: 319.258441\n",
      "Average loss at step 1167000: 319.073883\n",
      "Average loss at step 1167100: 317.646779\n",
      "Average loss at step 1167200: 315.459793\n",
      "Average loss at step 1167300: 317.550319\n",
      "Average loss at step 1167400: 314.075416\n",
      "Average loss at step 1167500: 318.940463\n",
      "Average loss at step 1167600: 314.685090\n",
      "Average loss at step 1167700: 311.951442\n",
      "Average loss at step 1167800: 309.392164\n",
      "Average loss at step 1167900: 314.835381\n",
      "Average loss at step 1168000: 310.948709\n",
      "Average loss at step 1168100: 317.989652\n",
      "Average loss at step 1168200: 314.226014\n",
      "Average loss at step 1168300: 317.884604\n",
      "Average loss at step 1168400: 316.800027\n",
      "Average loss at step 1168500: 318.271514\n",
      "Average loss at step 1168600: 316.315979\n",
      "Average loss at step 1168700: 314.656356\n",
      "Average loss at step 1168800: 315.533207\n",
      "Average loss at step 1168900: 311.498749\n",
      "Average loss at step 1169000: 313.766689\n",
      "Average loss at step 1169100: 313.602586\n",
      "Average loss at step 1169200: 313.316542\n",
      "Average loss at step 1169300: 315.567853\n",
      "Average loss at step 1169400: 315.878042\n",
      "Average loss at step 1169500: 316.839230\n",
      "Average loss at step 1169600: 319.785014\n",
      "Average loss at step 1169700: 313.693572\n",
      "Average loss at step 1169800: 318.258101\n",
      "Average loss at step 1169900: 314.488791\n",
      "Average loss at step 1170000: 312.078916\n",
      "Graph 234: 21 nodes\n",
      "Average loss at step 1170100: 327.576503\n",
      "Average loss at step 1170200: 327.250771\n",
      "Average loss at step 1170300: 325.916137\n",
      "Average loss at step 1170400: 326.694491\n",
      "Average loss at step 1170500: 321.545368\n",
      "Average loss at step 1170600: 326.069015\n",
      "Average loss at step 1170700: 323.009056\n",
      "Average loss at step 1170800: 324.913964\n",
      "Average loss at step 1170900: 321.423207\n",
      "Average loss at step 1171000: 324.811379\n",
      "Average loss at step 1171100: 325.292366\n",
      "Average loss at step 1171200: 320.060597\n",
      "Average loss at step 1171300: 326.327258\n",
      "Average loss at step 1171400: 323.982331\n",
      "Average loss at step 1171500: 324.954227\n",
      "Average loss at step 1171600: 322.229420\n",
      "Average loss at step 1171700: 320.825273\n",
      "Average loss at step 1171800: 324.604357\n",
      "Average loss at step 1171900: 322.841393\n",
      "Average loss at step 1172000: 323.410294\n",
      "Average loss at step 1172100: 321.845824\n",
      "Average loss at step 1172200: 319.649604\n",
      "Average loss at step 1172300: 322.728863\n",
      "Average loss at step 1172400: 323.015962\n",
      "Average loss at step 1172500: 322.664195\n",
      "Average loss at step 1172600: 320.013747\n",
      "Average loss at step 1172700: 323.622578\n",
      "Average loss at step 1172800: 322.541567\n",
      "Average loss at step 1172900: 322.838670\n",
      "Average loss at step 1173000: 323.132655\n",
      "Average loss at step 1173100: 327.440180\n",
      "Average loss at step 1173200: 323.296363\n",
      "Average loss at step 1173300: 319.308254\n",
      "Average loss at step 1173400: 320.222701\n",
      "Average loss at step 1173500: 320.723013\n",
      "Average loss at step 1173600: 325.191359\n",
      "Average loss at step 1173700: 325.531350\n",
      "Average loss at step 1173800: 322.064128\n",
      "Average loss at step 1173900: 324.138680\n",
      "Average loss at step 1174000: 323.480336\n",
      "Average loss at step 1174100: 323.993550\n",
      "Average loss at step 1174200: 322.139669\n",
      "Average loss at step 1174300: 325.653983\n",
      "Average loss at step 1174400: 322.485761\n",
      "Average loss at step 1174500: 319.160600\n",
      "Average loss at step 1174600: 323.433142\n",
      "Average loss at step 1174700: 323.849578\n",
      "Average loss at step 1174800: 320.134769\n",
      "Average loss at step 1174900: 321.213425\n",
      "Average loss at step 1175000: 326.595970\n",
      "Graph 235: 49 nodes\n",
      "Average loss at step 1175100: 329.428747\n",
      "Average loss at step 1175200: 327.465390\n",
      "Average loss at step 1175300: 326.661289\n",
      "Average loss at step 1175400: 330.902134\n",
      "Average loss at step 1175500: 328.858609\n",
      "Average loss at step 1175600: 322.929658\n",
      "Average loss at step 1175700: 328.364884\n",
      "Average loss at step 1175800: 328.346296\n",
      "Average loss at step 1175900: 327.758292\n",
      "Average loss at step 1176000: 323.336841\n",
      "Average loss at step 1176100: 324.348457\n",
      "Average loss at step 1176200: 326.321629\n",
      "Average loss at step 1176300: 324.084681\n",
      "Average loss at step 1176400: 325.987032\n",
      "Average loss at step 1176500: 326.208248\n",
      "Average loss at step 1176600: 328.854035\n",
      "Average loss at step 1176700: 321.486167\n",
      "Average loss at step 1176800: 323.213408\n",
      "Average loss at step 1176900: 326.950977\n",
      "Average loss at step 1177000: 325.235569\n",
      "Average loss at step 1177100: 324.813921\n",
      "Average loss at step 1177200: 327.867150\n",
      "Average loss at step 1177300: 331.071751\n",
      "Average loss at step 1177400: 327.017650\n",
      "Average loss at step 1177500: 327.709571\n",
      "Average loss at step 1177600: 324.667378\n",
      "Average loss at step 1177700: 323.700511\n",
      "Average loss at step 1177800: 327.662576\n",
      "Average loss at step 1177900: 328.566806\n",
      "Average loss at step 1178000: 328.981280\n",
      "Average loss at step 1178100: 324.248432\n",
      "Average loss at step 1178200: 327.002485\n",
      "Average loss at step 1178300: 324.541626\n",
      "Average loss at step 1178400: 323.118129\n",
      "Average loss at step 1178500: 328.853854\n",
      "Average loss at step 1178600: 326.252522\n",
      "Average loss at step 1178700: 325.345204\n",
      "Average loss at step 1178800: 325.415274\n",
      "Average loss at step 1178900: 326.767179\n",
      "Average loss at step 1179000: 324.316873\n",
      "Average loss at step 1179100: 322.355605\n",
      "Average loss at step 1179200: 322.568783\n",
      "Average loss at step 1179300: 327.769105\n",
      "Average loss at step 1179400: 326.293033\n",
      "Average loss at step 1179500: 325.257283\n",
      "Average loss at step 1179600: 328.801917\n",
      "Average loss at step 1179700: 328.238991\n",
      "Average loss at step 1179800: 322.607732\n",
      "Average loss at step 1179900: 321.868669\n",
      "Average loss at step 1180000: 321.994838\n",
      "Graph 236: 16 nodes\n",
      "Average loss at step 1180100: 321.942126\n",
      "Average loss at step 1180200: 319.637542\n",
      "Average loss at step 1180300: 317.762547\n",
      "Average loss at step 1180400: 314.198611\n",
      "Average loss at step 1180500: 309.048957\n",
      "Average loss at step 1180600: 314.552436\n",
      "Average loss at step 1180700: 312.552225\n",
      "Average loss at step 1180800: 311.572130\n",
      "Average loss at step 1180900: 312.405674\n",
      "Average loss at step 1181000: 308.819634\n",
      "Average loss at step 1181100: 312.202676\n",
      "Average loss at step 1181200: 314.216731\n",
      "Average loss at step 1181300: 310.428634\n",
      "Average loss at step 1181400: 313.441522\n",
      "Average loss at step 1181500: 312.142633\n",
      "Average loss at step 1181600: 305.845125\n",
      "Average loss at step 1181700: 307.996150\n",
      "Average loss at step 1181800: 305.905959\n",
      "Average loss at step 1181900: 309.879503\n",
      "Average loss at step 1299600: 325.729332\n",
      "Average loss at step 1299700: 321.950158\n",
      "Average loss at step 1299800: 322.554153\n",
      "Average loss at step 1299900: 326.273404\n",
      "Average loss at step 1300000: 324.147754\n",
      "Graph 260: 36 nodes\n",
      "Average loss at step 1300100: 336.406902\n",
      "Average loss at step 1300200: 336.099840\n",
      "Average loss at step 1300300: 335.876909\n",
      "Average loss at step 1300400: 329.968369\n",
      "Average loss at step 1300500: 330.788007\n",
      "Average loss at step 1300600: 331.169929\n",
      "Average loss at step 1300700: 330.343481\n",
      "Average loss at step 1300800: 326.096973\n",
      "Average loss at step 1300900: 331.438069\n",
      "Average loss at step 1301000: 333.069114\n",
      "Average loss at step 1301100: 331.855165\n",
      "Average loss at step 1301200: 330.387710\n",
      "Average loss at step 1301300: 330.211390\n",
      "Average loss at step 1301400: 329.513286\n",
      "Average loss at step 1301500: 329.023146\n",
      "Average loss at step 1301600: 329.842297\n",
      "Average loss at step 1301700: 329.912912\n",
      "Average loss at step 1301800: 329.441246\n",
      "Average loss at step 1301900: 332.581526\n",
      "Average loss at step 1302000: 329.481884\n",
      "Average loss at step 1302100: 331.002638\n",
      "Average loss at step 1302200: 327.384531\n",
      "Average loss at step 1302300: 330.161599\n",
      "Average loss at step 1302400: 330.671572\n",
      "Average loss at step 1302500: 332.351830\n",
      "Average loss at step 1302600: 329.314748\n",
      "Average loss at step 1302700: 327.930442\n",
      "Average loss at step 1302800: 328.656534\n",
      "Average loss at step 1302900: 331.041006\n",
      "Average loss at step 1303000: 328.971902\n",
      "Average loss at step 1303100: 330.755889\n",
      "Average loss at step 1303200: 329.368617\n",
      "Average loss at step 1303300: 330.049380\n",
      "Average loss at step 1303400: 325.857979\n",
      "Average loss at step 1303500: 329.552017\n",
      "Average loss at step 1303600: 325.099014\n",
      "Average loss at step 1303700: 328.903185\n",
      "Average loss at step 1303800: 327.577679\n",
      "Average loss at step 1303900: 329.904771\n",
      "Average loss at step 1304000: 330.583842\n",
      "Average loss at step 1304100: 327.461898\n",
      "Average loss at step 1304200: 330.253994\n",
      "Average loss at step 1304300: 328.278222\n",
      "Average loss at step 1304400: 329.010860\n",
      "Average loss at step 1304500: 329.635294\n",
      "Average loss at step 1304600: 331.157047\n",
      "Average loss at step 1304700: 330.839591\n",
      "Average loss at step 1304800: 327.030828\n",
      "Average loss at step 1304900: 329.923210\n",
      "Average loss at step 1305000: 324.340577\n",
      "Time: 23.8837759495\n",
      "Graph 261: 25 nodes\n",
      "Average loss at step 1305100: 331.743590\n",
      "Average loss at step 1305200: 325.241380\n",
      "Average loss at step 1305300: 324.622364\n",
      "Average loss at step 1305400: 329.942981\n",
      "Average loss at step 1305500: 325.740396\n",
      "Average loss at step 1305600: 325.590986\n",
      "Average loss at step 1305700: 331.708170\n",
      "Average loss at step 1305800: 328.631185\n",
      "Average loss at step 1305900: 324.675771\n",
      "Average loss at step 1306000: 324.026120\n",
      "Average loss at step 1306100: 329.407523\n",
      "Average loss at step 1306200: 329.288366\n",
      "Average loss at step 1306300: 324.859364\n",
      "Average loss at step 1306400: 325.291973\n",
      "Average loss at step 1306500: 325.050747\n",
      "Average loss at step 1306600: 329.184443\n",
      "Average loss at step 1306700: 326.104793\n",
      "Average loss at step 1306800: 325.030178\n",
      "Average loss at step 1306900: 322.790125\n",
      "Average loss at step 1307000: 323.295877\n",
      "Average loss at step 1307100: 325.525003\n",
      "Average loss at step 1307200: 325.347926\n",
      "Average loss at step 1307300: 327.410624\n",
      "Average loss at step 1307400: 325.657965\n",
      "Average loss at step 1307500: 321.577152\n",
      "Average loss at step 1307600: 321.486481\n",
      "Average loss at step 1307700: 326.734745\n",
      "Average loss at step 1307800: 322.763900\n",
      "Average loss at step 1307900: 323.300520\n",
      "Average loss at step 1308000: 323.502341\n",
      "Average loss at step 1308100: 321.209686\n",
      "Average loss at step 1308200: 327.404339\n",
      "Average loss at step 1308300: 325.742037\n",
      "Average loss at step 1308400: 322.949775\n",
      "Average loss at step 1308500: 326.211062\n",
      "Average loss at step 1308600: 323.860096\n",
      "Average loss at step 1308700: 326.453545\n",
      "Average loss at step 1308800: 324.137190\n",
      "Average loss at step 1308900: 327.801959\n",
      "Average loss at step 1309000: 326.882961\n",
      "Average loss at step 1309100: 321.443485\n",
      "Average loss at step 1309200: 328.898453\n",
      "Average loss at step 1309300: 327.611826\n",
      "Average loss at step 1309400: 323.387655\n",
      "Average loss at step 1309500: 325.509259\n",
      "Average loss at step 1309600: 325.397516\n",
      "Average loss at step 1309700: 325.164206\n",
      "Average loss at step 1309800: 325.350661\n",
      "Average loss at step 1309900: 323.959598\n",
      "Average loss at step 1310000: 323.142348\n",
      "Graph 262: 37 nodes\n",
      "Average loss at step 1310100: 335.579899\n",
      "Average loss at step 1310200: 331.959800\n",
      "Average loss at step 1310300: 332.531862\n",
      "Average loss at step 1310400: 329.669739\n",
      "Average loss at step 1310500: 331.983848\n",
      "Average loss at step 1310600: 335.001579\n",
      "Average loss at step 1310700: 332.245101\n",
      "Average loss at step 1310800: 330.152615\n",
      "Average loss at step 1310900: 327.859472\n",
      "Average loss at step 1311000: 327.739100\n",
      "Average loss at step 1311100: 329.723418\n",
      "Average loss at step 1311200: 326.003772\n",
      "Average loss at step 1311300: 325.374833\n",
      "Average loss at step 1311400: 332.792194\n",
      "Average loss at step 1311500: 331.497311\n",
      "Average loss at step 1311600: 333.486497\n",
      "Average loss at step 1311700: 331.381550\n",
      "Average loss at step 1311800: 329.890138\n",
      "Average loss at step 1311900: 325.890110\n",
      "Average loss at step 1312000: 330.724115\n",
      "Average loss at step 1312100: 329.555316\n",
      "Average loss at step 1312200: 331.454029\n",
      "Average loss at step 1312300: 326.991402\n",
      "Average loss at step 1312400: 330.765313\n",
      "Average loss at step 1312500: 328.923515\n",
      "Average loss at step 1312600: 328.420856\n",
      "Average loss at step 1312700: 330.219249\n",
      "Average loss at step 1312800: 331.894871\n",
      "Average loss at step 1312900: 330.136761\n",
      "Average loss at step 1313000: 331.302770\n",
      "Average loss at step 1313100: 326.119038\n",
      "Average loss at step 1313200: 328.950184\n",
      "Average loss at step 1313300: 328.568546\n",
      "Average loss at step 1313400: 328.653041\n",
      "Average loss at step 1313500: 332.726791\n",
      "Average loss at step 1313600: 329.991503\n",
      "Average loss at step 1313700: 331.248219\n",
      "Average loss at step 1313800: 327.399077\n",
      "Average loss at step 1313900: 326.132424\n",
      "Average loss at step 1314000: 330.027987\n",
      "Average loss at step 1314100: 331.327047\n",
      "Average loss at step 1314200: 332.064316\n",
      "Average loss at step 1314300: 331.471881\n",
      "Average loss at step 1314400: 326.560764\n",
      "Average loss at step 1314500: 331.525163\n",
      "Average loss at step 1314600: 328.600318\n",
      "Average loss at step 1314700: 331.092399\n",
      "Average loss at step 1314800: 331.101693\n",
      "Average loss at step 1314900: 329.474734\n",
      "Average loss at step 1315000: 330.263282\n",
      "Graph 263: 31 nodes\n",
      "Average loss at step 1315100: 331.275774\n",
      "Average loss at step 1315200: 334.380888\n",
      "Average loss at step 1315300: 331.700748\n",
      "Average loss at step 1315400: 331.060138\n",
      "Average loss at step 1315500: 330.835447\n",
      "Average loss at step 1315600: 332.266648\n",
      "Average loss at step 1315700: 330.859058\n",
      "Average loss at step 1315800: 326.985695\n",
      "Average loss at step 1315900: 331.101982\n",
      "Average loss at step 1316000: 330.332752\n",
      "Average loss at step 1316100: 330.784490\n",
      "Average loss at step 1316200: 332.602635\n",
      "Average loss at step 1316300: 331.195470\n",
      "Average loss at step 1316400: 328.557626\n",
      "Average loss at step 1316500: 327.361866\n",
      "Average loss at step 1316600: 327.349505\n",
      "Average loss at step 1316700: 329.697833\n",
      "Average loss at step 1316800: 327.389258\n",
      "Average loss at step 1316900: 329.124923\n",
      "Average loss at step 1317000: 325.822721\n",
      "Average loss at step 1317100: 331.073734\n",
      "Average loss at step 1317200: 326.888677\n",
      "Average loss at step 1317300: 328.527481\n",
      "Average loss at step 1317400: 331.073191\n",
      "Average loss at step 1317500: 332.059377\n",
      "Average loss at step 1317600: 329.831343\n",
      "Average loss at step 1317700: 334.855100\n",
      "Average loss at step 1317800: 327.506227\n",
      "Average loss at step 1317900: 329.152663\n",
      "Average loss at step 1318000: 326.341622\n",
      "Average loss at step 1318100: 327.282259\n",
      "Average loss at step 1318200: 329.128534\n",
      "Average loss at step 1318300: 331.471067\n",
      "Average loss at step 1318400: 326.551497\n",
      "Average loss at step 1318500: 329.579551\n",
      "Average loss at step 1318600: 330.189626\n",
      "Average loss at step 1318700: 330.355962\n",
      "Average loss at step 1318800: 327.320723\n",
      "Average loss at step 1318900: 327.529263\n",
      "Average loss at step 1319000: 323.336696\n",
      "Average loss at step 1319100: 328.076651\n",
      "Average loss at step 1319200: 323.751807\n",
      "Average loss at step 1319300: 328.491289\n",
      "Average loss at step 1319400: 325.660900\n",
      "Average loss at step 1319500: 328.202295\n",
      "Average loss at step 1319600: 331.061863\n",
      "Average loss at step 1319700: 326.218984\n",
      "Average loss at step 1319800: 325.577928\n",
      "Average loss at step 1319900: 330.239418\n",
      "Average loss at step 1320000: 325.136348\n",
      "Graph 264: 23 nodes\n",
      "Average loss at step 1320100: 340.434918\n",
      "Average loss at step 1320200: 335.445488\n",
      "Average loss at step 1320300: 335.541318\n",
      "Average loss at step 1320400: 333.634073\n",
      "Average loss at step 1320500: 333.174066\n",
      "Average loss at step 1320600: 333.315629\n",
      "Average loss at step 1320700: 337.900143\n",
      "Average loss at step 1320800: 336.897586\n",
      "Average loss at step 1320900: 336.364361\n",
      "Average loss at step 1321000: 332.684975\n",
      "Average loss at step 1321100: 331.730950\n",
      "Average loss at step 1321200: 336.273276\n",
      "Average loss at step 1321300: 337.396621\n",
      "Average loss at step 1321400: 336.150533\n",
      "Average loss at step 1321500: 336.694587\n",
      "Average loss at step 1321600: 333.379528\n",
      "Average loss at step 1321700: 336.055067\n",
      "Average loss at step 1321800: 336.110052\n",
      "Average loss at step 1321900: 332.380663\n",
      "Average loss at step 1322000: 334.076902\n",
      "Average loss at step 1322100: 335.502453\n",
      "Average loss at step 1322200: 335.308827\n",
      "Average loss at step 1322300: 337.500377\n",
      "Average loss at step 1322400: 334.272211\n",
      "Average loss at step 1322500: 333.575011\n",
      "Average loss at step 1322600: 331.088466\n",
      "Average loss at step 1322700: 333.285803\n",
      "Average loss at step 1322800: 335.971735\n",
      "Average loss at step 1322900: 332.940482\n",
      "Average loss at step 1323000: 333.321638\n",
      "Average loss at step 1323100: 332.303888\n",
      "Average loss at step 1323200: 332.046903\n",
      "Average loss at step 1323300: 334.110251\n",
      "Average loss at step 1323400: 335.828262\n",
      "Average loss at step 1323500: 333.137069\n",
      "Average loss at step 1323600: 333.415015\n",
      "Average loss at step 1323700: 334.793242\n",
      "Average loss at step 1323800: 335.788043\n",
      "Average loss at step 1323900: 329.740239\n",
      "Average loss at step 1324000: 329.061682\n",
      "Average loss at step 1324100: 334.334452\n",
      "Average loss at step 1324200: 331.362200\n",
      "Average loss at step 1324300: 328.505875\n",
      "Average loss at step 1324400: 335.529412\n",
      "Average loss at step 1324500: 332.138608\n",
      "Average loss at step 1324600: 333.426661\n",
      "Average loss at step 1324700: 330.962954\n",
      "Average loss at step 1324800: 336.241290\n",
      "Average loss at step 1324900: 336.101941\n",
      "Average loss at step 1325000: 329.904658\n",
      "Graph 265: 19 nodes\n",
      "Average loss at step 1325100: 339.591872\n",
      "Average loss at step 1325200: 332.887193\n",
      "Average loss at step 1325300: 335.324951\n",
      "Average loss at step 1325400: 341.598908\n",
      "Average loss at step 1325500: 335.387894\n",
      "Average loss at step 1325600: 333.116670\n",
      "Average loss at step 1325700: 333.084523\n",
      "Average loss at step 1325800: 334.194011\n",
      "Average loss at step 1325900: 337.517843\n",
      "Average loss at step 1326000: 332.066169\n",
      "Average loss at step 1326100: 328.917482\n",
      "Average loss at step 1326200: 332.018700\n",
      "Average loss at step 1326300: 333.796808\n",
      "Average loss at step 1326400: 331.439883\n",
      "Average loss at step 1326500: 334.044955\n",
      "Average loss at step 1326600: 331.880559\n",
      "Average loss at step 1326700: 333.252087\n",
      "Average loss at step 1326800: 336.854344\n",
      "Average loss at step 1326900: 334.833217\n",
      "Average loss at step 1327000: 335.002426\n",
      "Average loss at step 1327100: 333.438770\n",
      "Average loss at step 1327200: 335.076016\n",
      "Average loss at step 1327300: 333.956587\n",
      "Average loss at step 1327400: 333.770525\n",
      "Average loss at step 1327500: 332.004930\n",
      "Average loss at step 1327600: 333.328718\n",
      "Average loss at step 1327700: 337.126493\n",
      "Average loss at step 1327800: 338.293703\n",
      "Average loss at step 1327900: 331.880934\n",
      "Average loss at step 1328000: 333.113719\n",
      "Average loss at step 1328100: 332.308064\n",
      "Average loss at step 1328200: 335.313869\n",
      "Average loss at step 1328300: 335.388243\n",
      "Average loss at step 1328400: 333.620391\n",
      "Average loss at step 1328500: 334.056089\n",
      "Average loss at step 1328600: 332.205269\n",
      "Average loss at step 1328700: 334.750058\n",
      "Average loss at step 1328800: 333.965938\n",
      "Average loss at step 1328900: 335.229691\n",
      "Average loss at step 1329000: 334.370028\n",
      "Average loss at step 1329100: 333.390021\n",
      "Average loss at step 1329200: 331.535666\n",
      "Average loss at step 1329300: 333.825404\n",
      "Average loss at step 1329400: 329.382366\n",
      "Average loss at step 1329500: 332.489359\n",
      "Average loss at step 1329600: 330.694830\n",
      "Average loss at step 1329700: 332.434885\n",
      "Average loss at step 1329800: 333.234964\n",
      "Average loss at step 1329900: 328.220454\n",
      "Average loss at step 1330000: 332.851757\n",
      "Graph 266: 25 nodes\n",
      "Average loss at step 1330100: 337.950246\n",
      "Average loss at step 1330200: 334.167882\n",
      "Average loss at step 1330300: 335.229452\n",
      "Average loss at step 1330400: 337.017707\n",
      "Average loss at step 1330500: 338.153407\n",
      "Average loss at step 1448600: 322.420274\n",
      "Average loss at step 1448700: 325.537622\n",
      "Average loss at step 1448800: 323.823663\n",
      "Average loss at step 1448900: 321.490563\n",
      "Average loss at step 1449000: 324.836725\n",
      "Average loss at step 1449100: 326.714058\n",
      "Average loss at step 1449200: 323.209144\n",
      "Average loss at step 1449300: 322.975590\n",
      "Average loss at step 1449400: 325.186826\n",
      "Average loss at step 1449500: 323.923443\n",
      "Average loss at step 1449600: 321.365188\n",
      "Average loss at step 1449700: 322.749065\n",
      "Average loss at step 1449800: 326.775018\n",
      "Average loss at step 1449900: 324.483591\n",
      "Average loss at step 1450000: 323.960974\n",
      "Graph 290: 47 nodes\n",
      "Average loss at step 1450100: 336.563762\n",
      "Average loss at step 1450200: 337.995985\n",
      "Average loss at step 1450300: 334.627685\n",
      "Average loss at step 1450400: 333.631290\n",
      "Average loss at step 1450500: 334.451803\n",
      "Average loss at step 1450600: 330.654009\n",
      "Average loss at step 1450700: 328.598840\n",
      "Average loss at step 1450800: 330.747141\n",
      "Average loss at step 1450900: 329.141544\n",
      "Average loss at step 1451000: 332.419402\n",
      "Average loss at step 1451100: 329.130684\n",
      "Average loss at step 1451200: 327.074184\n",
      "Average loss at step 1451300: 331.319017\n",
      "Average loss at step 1451400: 329.461799\n",
      "Average loss at step 1451500: 326.273193\n",
      "Average loss at step 1451600: 325.153389\n",
      "Average loss at step 1451700: 326.456855\n",
      "Average loss at step 1451800: 330.569697\n",
      "Average loss at step 1451900: 328.572839\n",
      "Average loss at step 1452000: 328.076014\n",
      "Average loss at step 1452100: 330.389647\n",
      "Average loss at step 1452200: 327.887309\n",
      "Average loss at step 1452300: 327.529799\n",
      "Average loss at step 1452400: 331.092794\n",
      "Average loss at step 1452500: 327.929888\n",
      "Average loss at step 1452600: 324.850621\n",
      "Average loss at step 1452700: 329.514771\n",
      "Average loss at step 1452800: 327.162102\n",
      "Average loss at step 1452900: 325.006404\n",
      "Average loss at step 1453000: 325.710560\n",
      "Average loss at step 1453100: 330.683555\n",
      "Average loss at step 1453200: 328.254470\n",
      "Average loss at step 1453300: 330.992551\n",
      "Average loss at step 1453400: 328.718991\n",
      "Average loss at step 1453500: 332.733969\n",
      "Average loss at step 1453600: 326.615000\n",
      "Average loss at step 1453700: 327.977259\n",
      "Average loss at step 1453800: 331.249553\n",
      "Average loss at step 1453900: 330.918278\n",
      "Average loss at step 1454000: 328.848819\n",
      "Average loss at step 1454100: 325.301437\n",
      "Average loss at step 1454200: 325.819609\n",
      "Average loss at step 1454300: 331.115685\n",
      "Average loss at step 1454400: 330.694663\n",
      "Average loss at step 1454500: 325.341331\n",
      "Average loss at step 1454600: 326.198653\n",
      "Average loss at step 1454700: 328.668476\n",
      "Average loss at step 1454800: 326.507845\n",
      "Average loss at step 1454900: 330.592064\n",
      "Average loss at step 1455000: 326.954481\n",
      "Time: 24.5750100613\n",
      "Graph 291: 21 nodes\n",
      "Average loss at step 1455100: 332.150313\n",
      "Average loss at step 1455200: 324.509241\n",
      "Average loss at step 1455300: 327.680748\n",
      "Average loss at step 1455400: 329.616035\n",
      "Average loss at step 1455500: 327.083865\n",
      "Average loss at step 1455600: 326.249161\n",
      "Average loss at step 1455700: 327.193824\n",
      "Average loss at step 1455800: 328.844916\n",
      "Average loss at step 1455900: 328.291094\n",
      "Average loss at step 1456000: 332.824600\n",
      "Average loss at step 1456100: 324.300145\n",
      "Average loss at step 1456200: 328.417334\n",
      "Average loss at step 1456300: 330.041075\n",
      "Average loss at step 1456400: 327.618443\n",
      "Average loss at step 1456500: 327.126610\n",
      "Average loss at step 1456600: 326.582660\n",
      "Average loss at step 1456700: 326.552491\n",
      "Average loss at step 1456800: 324.086159\n",
      "Average loss at step 1456900: 328.779727\n",
      "Average loss at step 1457000: 325.173270\n",
      "Average loss at step 1457100: 326.656018\n",
      "Average loss at step 1457200: 325.218624\n",
      "Average loss at step 1457300: 327.228845\n",
      "Average loss at step 1457400: 325.832866\n",
      "Average loss at step 1457500: 324.070986\n",
      "Average loss at step 1457600: 329.330386\n",
      "Average loss at step 1457700: 325.182366\n",
      "Average loss at step 1457800: 325.606489\n",
      "Average loss at step 1457900: 325.773308\n",
      "Average loss at step 1458000: 327.921164\n",
      "Average loss at step 1458100: 326.041969\n",
      "Average loss at step 1458200: 331.001440\n",
      "Average loss at step 1458300: 328.258471\n",
      "Average loss at step 1458400: 324.263842\n",
      "Average loss at step 1458500: 325.269915\n",
      "Average loss at step 1458600: 325.241657\n",
      "Average loss at step 1458700: 325.826883\n",
      "Average loss at step 1458800: 327.854996\n",
      "Average loss at step 1458900: 324.571480\n",
      "Average loss at step 1459000: 325.620456\n",
      "Average loss at step 1459100: 327.164382\n",
      "Average loss at step 1459200: 325.524043\n",
      "Average loss at step 1459300: 323.536291\n",
      "Average loss at step 1459400: 326.604081\n",
      "Average loss at step 1459500: 329.792063\n",
      "Average loss at step 1459600: 325.696051\n",
      "Average loss at step 1459700: 325.486409\n",
      "Average loss at step 1459800: 323.884566\n",
      "Average loss at step 1459900: 329.514866\n",
      "Average loss at step 1460000: 326.538144\n",
      "Graph 292: 19 nodes\n",
      "Average loss at step 1460100: 338.454645\n",
      "Average loss at step 1460200: 333.264834\n",
      "Average loss at step 1460300: 337.779141\n",
      "Average loss at step 1460400: 335.426609\n",
      "Average loss at step 1460500: 328.988608\n",
      "Average loss at step 1460600: 333.862257\n",
      "Average loss at step 1460700: 334.790878\n",
      "Average loss at step 1460800: 333.516543\n",
      "Average loss at step 1460900: 336.259042\n",
      "Average loss at step 1461000: 331.936700\n",
      "Average loss at step 1461100: 330.328201\n",
      "Average loss at step 1461200: 331.388456\n",
      "Average loss at step 1461300: 330.026859\n",
      "Average loss at step 1461400: 330.576127\n",
      "Average loss at step 1461500: 331.504007\n",
      "Average loss at step 1461600: 333.458131\n",
      "Average loss at step 1461700: 329.640751\n",
      "Average loss at step 1461800: 331.071692\n",
      "Average loss at step 1461900: 330.437084\n",
      "Average loss at step 1462000: 331.477901\n",
      "Average loss at step 1462100: 328.730503\n",
      "Average loss at step 1462200: 333.389677\n",
      "Average loss at step 1462300: 329.963817\n",
      "Average loss at step 1462400: 330.480513\n",
      "Average loss at step 1462500: 332.815063\n",
      "Average loss at step 1462600: 332.197463\n",
      "Average loss at step 1462700: 330.491088\n",
      "Average loss at step 1462800: 330.109384\n",
      "Average loss at step 1462900: 331.236513\n",
      "Average loss at step 1463000: 325.570191\n",
      "Average loss at step 1463100: 326.501917\n",
      "Average loss at step 1463200: 327.323460\n",
      "Average loss at step 1463300: 329.032671\n",
      "Average loss at step 1463400: 329.092116\n",
      "Average loss at step 1463500: 331.409577\n",
      "Average loss at step 1463600: 327.743480\n",
      "Average loss at step 1463700: 329.054690\n",
      "Average loss at step 1463800: 331.383662\n",
      "Average loss at step 1463900: 329.806709\n",
      "Average loss at step 1464000: 329.315488\n",
      "Average loss at step 1464100: 333.316952\n",
      "Average loss at step 1464200: 332.317106\n",
      "Average loss at step 1464300: 326.959383\n",
      "Average loss at step 1464400: 329.218983\n",
      "Average loss at step 1464500: 333.492300\n",
      "Average loss at step 1464600: 334.382251\n",
      "Average loss at step 1464700: 331.936427\n",
      "Average loss at step 1464800: 330.647226\n",
      "Average loss at step 1464900: 328.259277\n",
      "Average loss at step 1465000: 330.287555\n",
      "Graph 293: 22 nodes\n",
      "Average loss at step 1465100: 335.353540\n",
      "Average loss at step 1465200: 330.995647\n",
      "Average loss at step 1465300: 328.802382\n",
      "Average loss at step 1465400: 328.541486\n",
      "Average loss at step 1465500: 329.663791\n",
      "Average loss at step 1465600: 329.397947\n",
      "Average loss at step 1465700: 329.560758\n",
      "Average loss at step 1465800: 329.111756\n",
      "Average loss at step 1465900: 326.673775\n",
      "Average loss at step 1466000: 329.472890\n",
      "Average loss at step 1466100: 327.608374\n",
      "Average loss at step 1466200: 330.814585\n",
      "Average loss at step 1466300: 328.499991\n",
      "Average loss at step 1466400: 327.884924\n",
      "Average loss at step 1466500: 327.531893\n",
      "Average loss at step 1466600: 327.330318\n",
      "Average loss at step 1466700: 328.192236\n",
      "Average loss at step 1466800: 328.113997\n",
      "Average loss at step 1466900: 325.886680\n",
      "Average loss at step 1467000: 327.107354\n",
      "Average loss at step 1467100: 327.132357\n",
      "Average loss at step 1467200: 325.408168\n",
      "Average loss at step 1467300: 326.132622\n",
      "Average loss at step 1467400: 328.242528\n",
      "Average loss at step 1467500: 327.009319\n",
      "Average loss at step 1467600: 320.973408\n",
      "Average loss at step 1467700: 327.373476\n",
      "Average loss at step 1467800: 329.486520\n",
      "Average loss at step 1467900: 326.257753\n",
      "Average loss at step 1468000: 327.644261\n",
      "Average loss at step 1468100: 326.571692\n",
      "Average loss at step 1468200: 324.518718\n",
      "Average loss at step 1468300: 327.434273\n",
      "Average loss at step 1468400: 324.565281\n",
      "Average loss at step 1468500: 326.877035\n",
      "Average loss at step 1468600: 328.020134\n",
      "Average loss at step 1468700: 329.799017\n",
      "Average loss at step 1468800: 326.812604\n",
      "Average loss at step 1468900: 328.489964\n",
      "Average loss at step 1469000: 328.663227\n",
      "Average loss at step 1469100: 327.652670\n",
      "Average loss at step 1469200: 326.706922\n",
      "Average loss at step 1469300: 324.056366\n",
      "Average loss at step 1469400: 325.608377\n",
      "Average loss at step 1469500: 329.284102\n",
      "Average loss at step 1469600: 327.150133\n",
      "Average loss at step 1469700: 323.670591\n",
      "Average loss at step 1469800: 324.448483\n",
      "Average loss at step 1469900: 327.395801\n",
      "Average loss at step 1470000: 324.875143\n",
      "Graph 294: 36 nodes\n",
      "Average loss at step 1470100: 327.843492\n",
      "Average loss at step 1470200: 331.185174\n",
      "Average loss at step 1470300: 327.987152\n",
      "Average loss at step 1470400: 325.202323\n",
      "Average loss at step 1470500: 329.220734\n",
      "Average loss at step 1470600: 329.270462\n",
      "Average loss at step 1470700: 328.173524\n",
      "Average loss at step 1470800: 327.599113\n",
      "Average loss at step 1470900: 329.204751\n",
      "Average loss at step 1471000: 327.500047\n",
      "Average loss at step 1471100: 324.498586\n",
      "Average loss at step 1471200: 328.667771\n",
      "Average loss at step 1471300: 329.373497\n",
      "Average loss at step 1471400: 322.432633\n",
      "Average loss at step 1471500: 326.007858\n",
      "Average loss at step 1471600: 325.011307\n",
      "Average loss at step 1471700: 328.942192\n",
      "Average loss at step 1471800: 323.899791\n",
      "Average loss at step 1471900: 325.959934\n",
      "Average loss at step 1472000: 324.137292\n",
      "Average loss at step 1472100: 325.948151\n",
      "Average loss at step 1472200: 326.659501\n",
      "Average loss at step 1472300: 325.668681\n",
      "Average loss at step 1472400: 326.984488\n",
      "Average loss at step 1472500: 323.148802\n",
      "Average loss at step 1472600: 323.874503\n",
      "Average loss at step 1472700: 321.110340\n",
      "Average loss at step 1472800: 328.162743\n",
      "Average loss at step 1472900: 322.291606\n",
      "Average loss at step 1473000: 326.564318\n",
      "Average loss at step 1473100: 326.263857\n",
      "Average loss at step 1473200: 326.605126\n",
      "Average loss at step 1473300: 324.105013\n",
      "Average loss at step 1473400: 325.295297\n",
      "Average loss at step 1473500: 324.717094\n",
      "Average loss at step 1473600: 322.738253\n",
      "Average loss at step 1473700: 324.796016\n",
      "Average loss at step 1473800: 322.362966\n",
      "Average loss at step 1473900: 325.995474\n",
      "Average loss at step 1474000: 322.214042\n",
      "Average loss at step 1474100: 321.682805\n",
      "Average loss at step 1474200: 323.921505\n",
      "Average loss at step 1474300: 324.753711\n",
      "Average loss at step 1474400: 320.455845\n",
      "Average loss at step 1474500: 326.271755\n",
      "Average loss at step 1474600: 325.519148\n",
      "Average loss at step 1474700: 324.506030\n",
      "Average loss at step 1474800: 327.869801\n",
      "Average loss at step 1474900: 328.477414\n",
      "Average loss at step 1475000: 322.789423\n",
      "Graph 295: 25 nodes\n",
      "Average loss at step 1475100: 334.854188\n",
      "Average loss at step 1475200: 333.843756\n",
      "Average loss at step 1475300: 332.249766\n",
      "Average loss at step 1475400: 336.033149\n",
      "Average loss at step 1475500: 329.504771\n",
      "Average loss at step 1475600: 331.767576\n",
      "Average loss at step 1475700: 331.481866\n",
      "Average loss at step 1475800: 331.868735\n",
      "Average loss at step 1475900: 331.524597\n",
      "Average loss at step 1476000: 331.207177\n",
      "Average loss at step 1476100: 330.313040\n",
      "Average loss at step 1476200: 330.124769\n",
      "Average loss at step 1476300: 327.593200\n",
      "Average loss at step 1476400: 326.760610\n",
      "Average loss at step 1476500: 329.819297\n",
      "Average loss at step 1476600: 327.777812\n",
      "Average loss at step 1476700: 335.374609\n",
      "Average loss at step 1476800: 331.130229\n",
      "Average loss at step 1476900: 331.104352\n",
      "Average loss at step 1477000: 329.394464\n",
      "Average loss at step 1477100: 333.519245\n",
      "Average loss at step 1477200: 327.388680\n",
      "Average loss at step 1477300: 329.805653\n",
      "Average loss at step 1477400: 333.636967\n",
      "Average loss at step 1477500: 329.731258\n",
      "Average loss at step 1477600: 327.413803\n",
      "Average loss at step 1477700: 328.536630\n",
      "Average loss at step 1477800: 333.180193\n",
      "Average loss at step 1477900: 330.062282\n",
      "Average loss at step 1478000: 332.565445\n",
      "Average loss at step 1478100: 325.426693\n",
      "Average loss at step 1478200: 331.081682\n",
      "Average loss at step 1478300: 329.818639\n",
      "Average loss at step 1478400: 328.464097\n",
      "Average loss at step 1478500: 328.296824\n",
      "Average loss at step 1478600: 329.159890\n",
      "Average loss at step 1478700: 329.280117\n",
      "Average loss at step 1478800: 329.348725\n",
      "Average loss at step 1478900: 331.403874\n",
      "Average loss at step 1479000: 331.393735\n",
      "Average loss at step 1479100: 326.680328\n",
      "Average loss at step 1479200: 332.584193\n",
      "Average loss at step 1479300: 325.407612\n",
      "Average loss at step 1479400: 332.660151\n",
      "Average loss at step 1479500: 331.347883\n",
      "Average loss at step 1604300: 322.454188\n",
      "Average loss at step 1604400: 322.692610\n",
      "Average loss at step 1604500: 323.909767\n",
      "Average loss at step 1604600: 322.985683\n",
      "Average loss at step 1604700: 324.201753\n",
      "Average loss at step 1604800: 323.136442\n",
      "Average loss at step 1604900: 324.875057\n",
      "Average loss at step 1605000: 324.997315\n",
      "Time: 23.7086071968\n",
      "Graph 321: 16 nodes\n",
      "Average loss at step 1605100: 332.631083\n",
      "Average loss at step 1605200: 326.003790\n",
      "Average loss at step 1605300: 326.172468\n",
      "Average loss at step 1605400: 329.141422\n",
      "Average loss at step 1605500: 328.232358\n",
      "Average loss at step 1605600: 327.306667\n",
      "Average loss at step 1605700: 323.044088\n",
      "Average loss at step 1605800: 329.236431\n",
      "Average loss at step 1605900: 324.991723\n",
      "Average loss at step 1606000: 326.123776\n",
      "Average loss at step 1606100: 324.492347\n",
      "Average loss at step 1606200: 324.399384\n",
      "Average loss at step 1606300: 326.254937\n",
      "Average loss at step 1606400: 328.247760\n",
      "Average loss at step 1606500: 325.398139\n",
      "Average loss at step 1606600: 321.411324\n",
      "Average loss at step 1606700: 320.176577\n",
      "Average loss at step 1606800: 323.415723\n",
      "Average loss at step 1606900: 322.854141\n",
      "Average loss at step 1607000: 321.271992\n",
      "Average loss at step 1607100: 322.548591\n",
      "Average loss at step 1607200: 325.866980\n",
      "Average loss at step 1607300: 324.460337\n",
      "Average loss at step 1607400: 322.150116\n",
      "Average loss at step 1607500: 323.522451\n",
      "Average loss at step 1607600: 321.636835\n",
      "Average loss at step 1607700: 320.594448\n",
      "Average loss at step 1607800: 325.533246\n",
      "Average loss at step 1607900: 322.194879\n",
      "Average loss at step 1608000: 323.237047\n",
      "Average loss at step 1608100: 323.673997\n",
      "Average loss at step 1608200: 323.765422\n",
      "Average loss at step 1608300: 323.014008\n",
      "Average loss at step 1608400: 322.259563\n",
      "Average loss at step 1608500: 326.431708\n",
      "Average loss at step 1608600: 324.739771\n",
      "Average loss at step 1608700: 321.198019\n",
      "Average loss at step 1608800: 322.059131\n",
      "Average loss at step 1608900: 322.656039\n",
      "Average loss at step 1609000: 319.636367\n",
      "Average loss at step 1609100: 325.787086\n",
      "Average loss at step 1609200: 320.519767\n",
      "Average loss at step 1609300: 321.343262\n",
      "Average loss at step 1609400: 323.708031\n",
      "Average loss at step 1609500: 321.712327\n",
      "Average loss at step 1609600: 323.797557\n",
      "Average loss at step 1609700: 323.675077\n",
      "Average loss at step 1609800: 323.627888\n",
      "Average loss at step 1609900: 321.283502\n",
      "Average loss at step 1610000: 320.389592\n",
      "Graph 322: 25 nodes\n",
      "Average loss at step 1610100: 337.998969\n",
      "Average loss at step 1610200: 332.308973\n",
      "Average loss at step 1610300: 329.950718\n",
      "Average loss at step 1610400: 329.810943\n",
      "Average loss at step 1610500: 330.201849\n",
      "Average loss at step 1610600: 329.577561\n",
      "Average loss at step 1610700: 325.785989\n",
      "Average loss at step 1610800: 325.041038\n",
      "Average loss at step 1610900: 321.873612\n",
      "Average loss at step 1611000: 325.148819\n",
      "Average loss at step 1611100: 329.098899\n",
      "Average loss at step 1611200: 321.647316\n",
      "Average loss at step 1611300: 327.000178\n",
      "Average loss at step 1611400: 320.730792\n",
      "Average loss at step 1611500: 323.235737\n",
      "Average loss at step 1611600: 325.782096\n",
      "Average loss at step 1611700: 324.552349\n",
      "Average loss at step 1611800: 319.482297\n",
      "Average loss at step 1611900: 322.122920\n",
      "Average loss at step 1612000: 324.892789\n",
      "Average loss at step 1612100: 321.223721\n",
      "Average loss at step 1612200: 321.618408\n",
      "Average loss at step 1612300: 322.630116\n",
      "Average loss at step 1612400: 322.958853\n",
      "Average loss at step 1612500: 324.091275\n",
      "Average loss at step 1612600: 322.035630\n",
      "Average loss at step 1612700: 324.035188\n",
      "Average loss at step 1612800: 320.507388\n",
      "Average loss at step 1612900: 324.909678\n",
      "Average loss at step 1613000: 318.620333\n",
      "Average loss at step 1613100: 322.243825\n",
      "Average loss at step 1613200: 321.223128\n",
      "Average loss at step 1613300: 322.163451\n",
      "Average loss at step 1613400: 323.575912\n",
      "Average loss at step 1613500: 325.007515\n",
      "Average loss at step 1613600: 325.340710\n",
      "Average loss at step 1613700: 322.168784\n",
      "Average loss at step 1613800: 321.784747\n",
      "Average loss at step 1613900: 324.593282\n",
      "Average loss at step 1614000: 321.885394\n",
      "Average loss at step 1614100: 323.700660\n",
      "Average loss at step 1614200: 322.793932\n",
      "Average loss at step 1614300: 327.594608\n",
      "Average loss at step 1614400: 321.166526\n",
      "Average loss at step 1614500: 325.384297\n",
      "Average loss at step 1614600: 321.729735\n",
      "Average loss at step 1614700: 321.714956\n",
      "Average loss at step 1614800: 328.744837\n",
      "Average loss at step 1614900: 326.404274\n",
      "Average loss at step 1615000: 324.354007\n",
      "Graph 323: 23 nodes\n",
      "Average loss at step 1615100: 345.475280\n",
      "Average loss at step 1615200: 340.225356\n",
      "Average loss at step 1615300: 340.147007\n",
      "Average loss at step 1615400: 342.700616\n",
      "Average loss at step 1615500: 336.934266\n",
      "Average loss at step 1615600: 340.958265\n",
      "Average loss at step 1615700: 335.066951\n",
      "Average loss at step 1615800: 339.378870\n",
      "Average loss at step 1615900: 336.834675\n",
      "Average loss at step 1616000: 336.420541\n",
      "Average loss at step 1616100: 335.278028\n",
      "Average loss at step 1616200: 336.580366\n",
      "Average loss at step 1616300: 336.708019\n",
      "Average loss at step 1616400: 336.507225\n",
      "Average loss at step 1616500: 335.619050\n",
      "Average loss at step 1616600: 335.612941\n",
      "Average loss at step 1616700: 337.183121\n",
      "Average loss at step 1616800: 336.224792\n",
      "Average loss at step 1616900: 328.731722\n",
      "Average loss at step 1617000: 337.213068\n",
      "Average loss at step 1617100: 333.693301\n",
      "Average loss at step 1617200: 332.813698\n",
      "Average loss at step 1617300: 332.861689\n",
      "Average loss at step 1617400: 335.596725\n",
      "Average loss at step 1617500: 336.602715\n",
      "Average loss at step 1617600: 331.435534\n",
      "Average loss at step 1617700: 335.474288\n",
      "Average loss at step 1617800: 330.104358\n",
      "Average loss at step 1617900: 338.273027\n",
      "Average loss at step 1618000: 333.562273\n",
      "Average loss at step 1618100: 336.064818\n",
      "Average loss at step 1618200: 335.374632\n",
      "Average loss at step 1618300: 335.661587\n",
      "Average loss at step 1618400: 338.157751\n",
      "Average loss at step 1618500: 337.859270\n",
      "Average loss at step 1618600: 332.727886\n",
      "Average loss at step 1618700: 339.176875\n",
      "Average loss at step 1618800: 334.680184\n",
      "Average loss at step 1618900: 329.270088\n",
      "Average loss at step 1619000: 337.398480\n",
      "Average loss at step 1619100: 332.962946\n",
      "Average loss at step 1619200: 334.378275\n",
      "Average loss at step 1619300: 335.698183\n",
      "Average loss at step 1619400: 334.836119\n",
      "Average loss at step 1619500: 332.539837\n",
      "Average loss at step 1619600: 333.858943\n",
      "Average loss at step 1619700: 332.768059\n",
      "Average loss at step 1619800: 333.859207\n",
      "Average loss at step 1619900: 334.635442\n",
      "Average loss at step 1620000: 333.780321\n",
      "Graph 324: 24 nodes\n",
      "Average loss at step 1620100: 328.034994\n",
      "Average loss at step 1620200: 322.730255\n",
      "Average loss at step 1620300: 323.750683\n",
      "Average loss at step 1620400: 321.384980\n",
      "Average loss at step 1620500: 320.404646\n",
      "Average loss at step 1620600: 321.081773\n",
      "Average loss at step 1620700: 317.053511\n",
      "Average loss at step 1620800: 320.180020\n",
      "Average loss at step 1620900: 322.748796\n",
      "Average loss at step 1621000: 318.358757\n",
      "Average loss at step 1621100: 320.753799\n",
      "Average loss at step 1621200: 319.290927\n",
      "Average loss at step 1621300: 320.600237\n",
      "Average loss at step 1621400: 321.542206\n",
      "Average loss at step 1621500: 318.608140\n",
      "Average loss at step 1621600: 317.127643\n",
      "Average loss at step 1621700: 316.047265\n",
      "Average loss at step 1621800: 316.300828\n",
      "Average loss at step 1621900: 320.645019\n",
      "Average loss at step 1622000: 319.417553\n",
      "Average loss at step 1622100: 318.710221\n",
      "Average loss at step 1622200: 317.333184\n",
      "Average loss at step 1622300: 316.734502\n",
      "Average loss at step 1622400: 317.147884\n",
      "Average loss at step 1622500: 317.412596\n",
      "Average loss at step 1622600: 314.721168\n",
      "Average loss at step 1622700: 316.020031\n",
      "Average loss at step 1622800: 316.383832\n",
      "Average loss at step 1622900: 316.539112\n",
      "Average loss at step 1623000: 316.114662\n",
      "Average loss at step 1623100: 321.256790\n",
      "Average loss at step 1623200: 315.684942\n",
      "Average loss at step 1623300: 316.615481\n",
      "Average loss at step 1623400: 321.189867\n",
      "Average loss at step 1623500: 316.220447\n",
      "Average loss at step 1623600: 316.447343\n",
      "Average loss at step 1623700: 315.125639\n",
      "Average loss at step 1623800: 315.982616\n",
      "Average loss at step 1623900: 314.376660\n",
      "Average loss at step 1624000: 316.310676\n",
      "Average loss at step 1624100: 314.546777\n",
      "Average loss at step 1624200: 315.686313\n",
      "Average loss at step 1624300: 319.158132\n",
      "Average loss at step 1624400: 322.472499\n",
      "Average loss at step 1624500: 317.233912\n",
      "Average loss at step 1624600: 321.496265\n",
      "Average loss at step 1624700: 314.186823\n",
      "Average loss at step 1624800: 314.865761\n",
      "Average loss at step 1624900: 319.630253\n",
      "Average loss at step 1625000: 314.976157\n",
      "Graph 325: 20 nodes\n",
      "Average loss at step 1625100: 344.732805\n",
      "Average loss at step 1625200: 335.926432\n",
      "Average loss at step 1625300: 335.251749\n",
      "Average loss at step 1625400: 331.181361\n",
      "Average loss at step 1625500: 335.928902\n",
      "Average loss at step 1625600: 332.036376\n",
      "Average loss at step 1625700: 332.942662\n",
      "Average loss at step 1625800: 332.436367\n",
      "Average loss at step 1625900: 332.558319\n",
      "Average loss at step 1626000: 327.407923\n",
      "Average loss at step 1626100: 331.830287\n",
      "Average loss at step 1626200: 329.514435\n",
      "Average loss at step 1626300: 328.836253\n",
      "Average loss at step 1626400: 329.065635\n",
      "Average loss at step 1626500: 332.600576\n",
      "Average loss at step 1626600: 330.220281\n",
      "Average loss at step 1626700: 332.180981\n",
      "Average loss at step 1626800: 330.807536\n",
      "Average loss at step 1626900: 328.832927\n",
      "Average loss at step 1627000: 326.834554\n",
      "Average loss at step 1627100: 332.413762\n",
      "Average loss at step 1627200: 328.198879\n",
      "Average loss at step 1627300: 330.973643\n",
      "Average loss at step 1627400: 330.097724\n",
      "Average loss at step 1627500: 330.844330\n",
      "Average loss at step 1627600: 330.095815\n",
      "Average loss at step 1627700: 328.246659\n",
      "Average loss at step 1627800: 332.207518\n",
      "Average loss at step 1627900: 330.344859\n",
      "Average loss at step 1628000: 330.228035\n",
      "Average loss at step 1628100: 330.332627\n",
      "Average loss at step 1628200: 327.432995\n",
      "Average loss at step 1628300: 331.845523\n",
      "Average loss at step 1628400: 329.038789\n",
      "Average loss at step 1628500: 334.336628\n",
      "Average loss at step 1628600: 328.441990\n",
      "Average loss at step 1628700: 330.474834\n",
      "Average loss at step 1628800: 326.868428\n",
      "Average loss at step 1628900: 332.774560\n",
      "Average loss at step 1629000: 328.089901\n",
      "Average loss at step 1629100: 332.112454\n",
      "Average loss at step 1629200: 330.681116\n",
      "Average loss at step 1629300: 328.363009\n",
      "Average loss at step 1629400: 330.729339\n",
      "Average loss at step 1629500: 329.521171\n",
      "Average loss at step 1629600: 329.129365\n",
      "Average loss at step 1629700: 327.351426\n",
      "Average loss at step 1629800: 326.935564\n",
      "Average loss at step 1629900: 328.233368\n",
      "Average loss at step 1630000: 332.115451\n",
      "Graph 326: 24 nodes\n",
      "Average loss at step 1630100: 335.031227\n",
      "Average loss at step 1630200: 332.325380\n",
      "Average loss at step 1630300: 332.695179\n",
      "Average loss at step 1630400: 333.545338\n",
      "Average loss at step 1630500: 329.289000\n",
      "Average loss at step 1630600: 330.345816\n",
      "Average loss at step 1630700: 332.664243\n",
      "Average loss at step 1630800: 331.495101\n",
      "Average loss at step 1630900: 332.962906\n",
      "Average loss at step 1631000: 328.557346\n",
      "Average loss at step 1631100: 329.771147\n",
      "Average loss at step 1631200: 328.142870\n",
      "Average loss at step 1631300: 332.616015\n",
      "Average loss at step 1631400: 330.272312\n",
      "Average loss at step 1631500: 332.498827\n",
      "Average loss at step 1631600: 331.313078\n",
      "Average loss at step 1631700: 328.209046\n",
      "Average loss at step 1631800: 335.596514\n",
      "Average loss at step 1631900: 333.465221\n",
      "Average loss at step 1632000: 331.711701\n",
      "Average loss at step 1632100: 330.371483\n",
      "Average loss at step 1632200: 332.950436\n",
      "Average loss at step 1632300: 333.507699\n",
      "Average loss at step 1632400: 327.696631\n",
      "Average loss at step 1632500: 325.419593\n",
      "Average loss at step 1632600: 329.820595\n",
      "Average loss at step 1632700: 328.622657\n",
      "Average loss at step 1632800: 331.549292\n",
      "Average loss at step 1632900: 328.410899\n",
      "Average loss at step 1633000: 332.702961\n",
      "Average loss at step 1633100: 332.094553\n",
      "Average loss at step 1633200: 331.383853\n",
      "Average loss at step 1633300: 329.444916\n",
      "Average loss at step 1633400: 330.979044\n",
      "Average loss at step 1633500: 331.973233\n",
      "Average loss at step 1633600: 330.667972\n",
      "Average loss at step 1633700: 328.222052\n",
      "Average loss at step 1633800: 332.508473\n",
      "Average loss at step 1633900: 332.433141\n",
      "Average loss at step 1634000: 330.231616\n",
      "Average loss at step 1634100: 332.888177\n",
      "Average loss at step 1634200: 329.300533\n",
      "Average loss at step 1634300: 329.608683\n",
      "Average loss at step 1634400: 333.840244\n",
      "Average loss at step 1634500: 331.718414\n",
      "Average loss at step 1634600: 331.788684\n",
      "Average loss at step 1634700: 331.290990\n",
      "Average loss at step 1634800: 329.917352\n",
      "Average loss at step 1634900: 331.826852\n",
      "Average loss at step 1635000: 329.345021\n",
      "Graph 327: 34 nodes\n",
      "Average loss at step 1635100: 342.922801\n",
      "Average loss at step 1635200: 335.353450\n",
      "Average loss at step 1769600: 316.825783\n",
      "Average loss at step 1769700: 311.578023\n",
      "Average loss at step 1769800: 312.695915\n",
      "Average loss at step 1769900: 316.335264\n",
      "Average loss at step 1770000: 312.452860\n",
      "Graph 354: 11 nodes\n",
      "Average loss at step 1770100: 324.581975\n",
      "Average loss at step 1770200: 324.141659\n",
      "Average loss at step 1770300: 323.843374\n",
      "Average loss at step 1770400: 321.264047\n",
      "Average loss at step 1770500: 323.040328\n",
      "Average loss at step 1770600: 324.491422\n",
      "Average loss at step 1770700: 321.788282\n",
      "Average loss at step 1770800: 322.929531\n",
      "Average loss at step 1770900: 323.841063\n",
      "Average loss at step 1771000: 322.741256\n",
      "Average loss at step 1771100: 320.924245\n",
      "Average loss at step 1771200: 321.684468\n",
      "Average loss at step 1771300: 324.110590\n",
      "Average loss at step 1771400: 320.176531\n",
      "Average loss at step 1771500: 321.342149\n",
      "Average loss at step 1771600: 319.146237\n",
      "Average loss at step 1771700: 322.141695\n",
      "Average loss at step 1771800: 321.966831\n",
      "Average loss at step 1771900: 318.951634\n",
      "Average loss at step 1772000: 322.061233\n",
      "Average loss at step 1772100: 319.450801\n",
      "Average loss at step 1772200: 318.558529\n",
      "Average loss at step 1772300: 315.919398\n",
      "Average loss at step 1772400: 318.126818\n",
      "Average loss at step 1772500: 321.782057\n",
      "Average loss at step 1772600: 321.625935\n",
      "Average loss at step 1772700: 318.785853\n",
      "Average loss at step 1772800: 320.319417\n",
      "Average loss at step 1772900: 319.870509\n",
      "Average loss at step 1773000: 317.947536\n",
      "Average loss at step 1773100: 317.082977\n",
      "Average loss at step 1773200: 320.565691\n",
      "Average loss at step 1773300: 323.444166\n",
      "Average loss at step 1773400: 321.785062\n",
      "Average loss at step 1773500: 315.628659\n",
      "Average loss at step 1773600: 320.973289\n",
      "Average loss at step 1773700: 321.835837\n",
      "Average loss at step 1773800: 318.237047\n",
      "Average loss at step 1773900: 317.363207\n",
      "Average loss at step 1774000: 319.891539\n",
      "Average loss at step 1774100: 318.561747\n",
      "Average loss at step 1774200: 322.944952\n",
      "Average loss at step 1774300: 323.137971\n",
      "Average loss at step 1774400: 318.709437\n",
      "Average loss at step 1774500: 320.205656\n",
      "Average loss at step 1774600: 320.262570\n",
      "Average loss at step 1774700: 320.215715\n",
      "Average loss at step 1774800: 315.828633\n",
      "Average loss at step 1774900: 320.760836\n",
      "Average loss at step 1775000: 316.408077\n",
      "Graph 355: 14 nodes\n",
      "Average loss at step 1775100: 344.595608\n",
      "Average loss at step 1775200: 340.150402\n",
      "Average loss at step 1775300: 337.801293\n",
      "Average loss at step 1775400: 341.803334\n",
      "Average loss at step 1775500: 335.854590\n",
      "Average loss at step 1775600: 337.824221\n",
      "Average loss at step 1775700: 336.242887\n",
      "Average loss at step 1775800: 335.026567\n",
      "Average loss at step 1775900: 334.406128\n",
      "Average loss at step 1776000: 338.107524\n",
      "Average loss at step 1776100: 337.195466\n",
      "Average loss at step 1776200: 335.834715\n",
      "Average loss at step 1776300: 336.655528\n",
      "Average loss at step 1776400: 333.549482\n",
      "Average loss at step 1776500: 336.017337\n",
      "Average loss at step 1776600: 334.107438\n",
      "Average loss at step 1776700: 333.523188\n",
      "Average loss at step 1776800: 335.665896\n",
      "Average loss at step 1776900: 336.103840\n",
      "Average loss at step 1777000: 330.897479\n",
      "Average loss at step 1777100: 333.568829\n",
      "Average loss at step 1777200: 330.465377\n",
      "Average loss at step 1777300: 334.958328\n",
      "Average loss at step 1777400: 332.430515\n",
      "Average loss at step 1777500: 335.271510\n",
      "Average loss at step 1777600: 332.361056\n",
      "Average loss at step 1777700: 334.596190\n",
      "Average loss at step 1777800: 337.701723\n",
      "Average loss at step 1777900: 337.851959\n",
      "Average loss at step 1778000: 334.743693\n",
      "Average loss at step 1778100: 334.102657\n",
      "Average loss at step 1778200: 332.432760\n",
      "Average loss at step 1778300: 334.172273\n",
      "Average loss at step 1778400: 335.952083\n",
      "Average loss at step 1778500: 330.447342\n",
      "Average loss at step 1778600: 336.222550\n",
      "Average loss at step 1778700: 335.657999\n",
      "Average loss at step 1778800: 336.140538\n",
      "Average loss at step 1778900: 334.490014\n",
      "Average loss at step 1779000: 335.818535\n",
      "Average loss at step 1779100: 336.322182\n",
      "Average loss at step 1779200: 336.745243\n",
      "Average loss at step 1779300: 334.366131\n",
      "Average loss at step 1779400: 332.247406\n",
      "Average loss at step 1779500: 332.415622\n",
      "Average loss at step 1779600: 335.208418\n",
      "Average loss at step 1779700: 332.275530\n",
      "Average loss at step 1779800: 334.146903\n",
      "Average loss at step 1779900: 332.218613\n",
      "Average loss at step 1780000: 332.629561\n",
      "Graph 356: 21 nodes\n",
      "Average loss at step 1780100: 335.618658\n",
      "Average loss at step 1780200: 338.495770\n",
      "Average loss at step 1780300: 331.915065\n",
      "Average loss at step 1780400: 331.737387\n",
      "Average loss at step 1780500: 331.090656\n",
      "Average loss at step 1780600: 330.162448\n",
      "Average loss at step 1780700: 332.370868\n",
      "Average loss at step 1780800: 333.992744\n",
      "Average loss at step 1780900: 328.271773\n",
      "Average loss at step 1781000: 329.554442\n",
      "Average loss at step 1781100: 330.012231\n",
      "Average loss at step 1781200: 329.250442\n",
      "Average loss at step 1781300: 330.811602\n",
      "Average loss at step 1781400: 331.186520\n",
      "Average loss at step 1781500: 332.007868\n",
      "Average loss at step 1781600: 332.887816\n",
      "Average loss at step 1781700: 328.426577\n",
      "Average loss at step 1781800: 331.944729\n",
      "Average loss at step 1781900: 333.825483\n",
      "Average loss at step 1782000: 332.431932\n",
      "Average loss at step 1782100: 329.962369\n",
      "Average loss at step 1782200: 331.461658\n",
      "Average loss at step 1782300: 331.954823\n",
      "Average loss at step 1782400: 332.136656\n",
      "Average loss at step 1782500: 330.249245\n",
      "Average loss at step 1782600: 332.702040\n",
      "Average loss at step 1782700: 330.054745\n",
      "Average loss at step 1782800: 330.257073\n",
      "Average loss at step 1782900: 328.404159\n",
      "Average loss at step 1783000: 327.240344\n",
      "Average loss at step 1783100: 332.277205\n",
      "Average loss at step 1783200: 328.797320\n",
      "Average loss at step 1783300: 333.372816\n",
      "Average loss at step 1783400: 330.259331\n",
      "Average loss at step 1783500: 328.927627\n",
      "Average loss at step 1783600: 330.170297\n",
      "Average loss at step 1783700: 327.250413\n",
      "Average loss at step 1783800: 329.895587\n",
      "Average loss at step 1783900: 329.766199\n",
      "Average loss at step 1784000: 330.284057\n",
      "Average loss at step 1784100: 332.800260\n",
      "Average loss at step 1784200: 328.270098\n",
      "Average loss at step 1784300: 330.630545\n",
      "Average loss at step 1784400: 326.558502\n",
      "Average loss at step 1784500: 329.542353\n",
      "Average loss at step 1784600: 331.113655\n",
      "Average loss at step 1784700: 330.169191\n",
      "Average loss at step 1784800: 329.317886\n",
      "Average loss at step 1784900: 327.346355\n",
      "Average loss at step 1852200: 325.323565\n",
      "Average loss at step 1852300: 326.582671\n",
      "Average loss at step 1852400: 326.462810\n",
      "Average loss at step 1852500: 327.725477\n",
      "Average loss at step 1852600: 329.309897\n",
      "Average loss at step 1852700: 331.508482\n",
      "Average loss at step 1852800: 325.131167\n",
      "Average loss at step 1852900: 326.900311\n",
      "Average loss at step 1853000: 330.294144\n",
      "Average loss at step 1853100: 327.504469\n",
      "Average loss at step 1853200: 327.528602\n",
      "Average loss at step 1853300: 327.716594\n",
      "Average loss at step 1853400: 330.174395\n",
      "Average loss at step 1853500: 328.691359\n",
      "Average loss at step 1853600: 326.889602\n",
      "Average loss at step 1853700: 331.280271\n",
      "Average loss at step 1853800: 327.581288\n",
      "Average loss at step 1853900: 332.607685\n",
      "Average loss at step 1854000: 329.143648\n",
      "Average loss at step 1854100: 327.673758\n",
      "Average loss at step 1854200: 329.807395\n",
      "Average loss at step 1854300: 330.728820\n",
      "Average loss at step 1854400: 329.372813\n",
      "Average loss at step 1854500: 328.108237\n",
      "Average loss at step 1854600: 331.089956\n",
      "Average loss at step 1854700: 325.826100\n",
      "Average loss at step 1854800: 330.341508\n",
      "Average loss at step 1854900: 327.563877\n",
      "Average loss at step 1855000: 327.710068\n",
      "Time: 24.151594162\n",
      "Graph 371: 23 nodes\n",
      "Average loss at step 1855100: 338.311079\n",
      "Average loss at step 1855200: 330.103079\n",
      "Average loss at step 1855300: 331.844637\n",
      "Average loss at step 1855400: 331.913397\n",
      "Average loss at step 1855500: 334.273014\n",
      "Average loss at step 1855600: 331.779805\n",
      "Average loss at step 1855700: 329.951023\n",
      "Average loss at step 1855800: 330.504230\n",
      "Average loss at step 1855900: 332.866477\n",
      "Average loss at step 1856000: 330.925815\n",
      "Average loss at step 1856100: 332.339126\n",
      "Average loss at step 1856200: 330.386630\n",
      "Average loss at step 1856300: 334.615581\n",
      "Average loss at step 1856400: 332.437247\n",
      "Average loss at step 1856500: 331.878957\n",
      "Average loss at step 1856600: 333.038380\n",
      "Average loss at step 1856700: 329.571797\n",
      "Average loss at step 1856800: 330.880558\n",
      "Average loss at step 1856900: 333.909009\n",
      "Average loss at step 1857000: 332.890006\n",
      "Average loss at step 1857100: 331.167213\n",
      "Average loss at step 1857200: 331.166395\n",
      "Average loss at step 1857300: 325.980118\n",
      "Average loss at step 1857400: 330.956013\n",
      "Average loss at step 1857500: 328.893153\n",
      "Average loss at step 1857600: 330.530054\n",
      "Average loss at step 1857700: 332.651485\n",
      "Average loss at step 1857800: 331.013035\n",
      "Average loss at step 1857900: 330.375474\n",
      "Average loss at step 1858000: 331.905969\n",
      "Average loss at step 1858100: 328.913078\n",
      "Average loss at step 1858200: 329.999532\n",
      "Average loss at step 1858300: 327.654368\n",
      "Average loss at step 1858400: 329.057678\n",
      "Average loss at step 1858500: 330.896065\n",
      "Average loss at step 1858600: 327.599983\n",
      "Average loss at step 1858700: 328.224564\n",
      "Average loss at step 1858800: 327.968391\n",
      "Average loss at step 1858900: 327.348307\n",
      "Average loss at step 1859000: 330.738789\n",
      "Average loss at step 1859100: 328.554305\n",
      "Average loss at step 1859200: 330.759300\n",
      "Average loss at step 1859300: 328.263578\n",
      "Average loss at step 1859400: 330.872178\n",
      "Average loss at step 1859500: 329.308465\n",
      "Average loss at step 1859600: 328.596576\n",
      "Average loss at step 1859700: 330.723461\n",
      "Average loss at step 1859800: 327.833771\n",
      "Average loss at step 1859900: 331.560582\n",
      "Average loss at step 1860000: 329.008914\n",
      "Graph 372: 23 nodes\n",
      "Average loss at step 1860100: 325.293264\n",
      "Average loss at step 1860200: 318.403928\n",
      "Average loss at step 1860300: 318.041467\n",
      "Average loss at step 1860400: 319.594728\n",
      "Average loss at step 1860500: 319.992885\n",
      "Average loss at step 1860600: 315.955928\n",
      "Average loss at step 1860700: 315.368680\n",
      "Average loss at step 1860800: 319.236732\n",
      "Average loss at step 1860900: 318.943270\n",
      "Average loss at step 1861000: 314.043324\n",
      "Average loss at step 1861100: 317.440324\n",
      "Average loss at step 1861200: 315.006063\n",
      "Average loss at step 1861300: 315.820971\n",
      "Average loss at step 1861400: 317.583681\n",
      "Average loss at step 1861500: 318.417908\n",
      "Average loss at step 1861600: 314.503294\n",
      "Average loss at step 1861700: 318.469194\n",
      "Average loss at step 1861800: 319.651101\n",
      "Average loss at step 1861900: 319.393619\n",
      "Average loss at step 1862000: 317.957012\n",
      "Average loss at step 1862100: 318.886816\n",
      "Average loss at step 1862200: 315.953087\n",
      "Average loss at step 1862300: 321.213794\n",
      "Average loss at step 1862400: 316.457688\n",
      "Average loss at step 1862500: 318.350513\n",
      "Average loss at step 1862600: 319.312656\n",
      "Average loss at step 1862700: 315.608334\n",
      "Average loss at step 1862800: 315.617799\n",
      "Average loss at step 1862900: 316.837240\n",
      "Average loss at step 1863000: 316.952755\n",
      "Average loss at step 1863100: 315.871131\n",
      "Average loss at step 1863200: 316.834256\n",
      "Average loss at step 1863300: 313.125465\n",
      "Average loss at step 1863400: 319.598862\n",
      "Average loss at step 1863500: 315.201139\n",
      "Average loss at step 1863600: 313.691376\n",
      "Average loss at step 1863700: 317.164397\n",
      "Average loss at step 1863800: 313.703206\n",
      "Average loss at step 1863900: 317.271281\n",
      "Average loss at step 1864000: 316.353008\n",
      "Average loss at step 1864100: 316.002881\n",
      "Average loss at step 1864200: 317.897459\n",
      "Average loss at step 1864300: 314.861101\n",
      "Average loss at step 1864400: 315.515857\n",
      "Average loss at step 1864500: 317.083478\n",
      "Average loss at step 1864600: 320.531575\n",
      "Average loss at step 1864700: 317.431424\n",
      "Average loss at step 1864800: 311.718254\n",
      "Average loss at step 1864900: 316.517569\n",
      "Average loss at step 1865000: 316.764258\n",
      "Graph 373: 32 nodes\n",
      "Average loss at step 1865100: 339.715139\n",
      "Average loss at step 1865200: 340.663487\n",
      "Average loss at step 1865300: 335.340155\n",
      "Average loss at step 1865400: 334.853333\n",
      "Average loss at step 1865500: 339.272160\n",
      "Average loss at step 1865600: 338.360857\n",
      "Average loss at step 1865700: 336.822029\n",
      "Average loss at step 1865800: 334.771015\n",
      "Average loss at step 1865900: 337.498280\n",
      "Average loss at step 1866000: 334.181194\n",
      "Average loss at step 1866100: 334.999550\n",
      "Average loss at step 1866200: 332.922279\n",
      "Average loss at step 1866300: 336.593106\n",
      "Average loss at step 1866400: 334.683922\n",
      "Average loss at step 1866500: 333.887190\n",
      "Average loss at step 1866600: 337.474739\n",
      "Average loss at step 1866700: 336.469193\n",
      "Average loss at step 1866800: 336.012022\n",
      "Average loss at step 1866900: 337.305995\n",
      "Average loss at step 1867000: 335.442611\n",
      "Average loss at step 1867100: 333.811760\n",
      "Average loss at step 1867200: 332.078180\n",
      "Average loss at step 1867300: 334.339309\n",
      "Average loss at step 1867400: 333.690878\n",
      "Average loss at step 1867500: 332.857870\n",
      "Average loss at step 1867600: 332.248097\n",
      "Average loss at step 1867700: 337.089780\n",
      "Average loss at step 1867800: 335.294167\n",
      "Average loss at step 1867900: 335.281291\n",
      "Average loss at step 1868000: 331.436968\n",
      "Average loss at step 1868100: 334.419945\n",
      "Average loss at step 1868200: 334.778364\n",
      "Average loss at step 1868300: 336.793203\n",
      "Average loss at step 1868400: 334.750895\n",
      "Average loss at step 1868500: 336.951537\n",
      "Average loss at step 1868600: 335.014486\n",
      "Average loss at step 1868700: 334.568556\n",
      "Average loss at step 1868800: 331.430486\n",
      "Average loss at step 1868900: 334.887923\n",
      "Average loss at step 1869000: 335.905984\n",
      "Average loss at step 1869100: 334.355417\n",
      "Average loss at step 1869200: 334.037555\n",
      "Average loss at step 1869300: 336.543449\n",
      "Average loss at step 1869400: 331.457018\n",
      "Average loss at step 1869500: 334.311229\n",
      "Average loss at step 1869600: 332.972621\n",
      "Average loss at step 1869700: 331.073748\n",
      "Average loss at step 1869800: 335.066747\n",
      "Average loss at step 1869900: 334.841565\n",
      "Average loss at step 1870000: 331.850174\n",
      "Graph 374: 24 nodes\n",
      "Average loss at step 1870100: 330.976519\n",
      "Average loss at step 1870200: 332.479659\n",
      "Average loss at step 1870300: 332.438926\n",
      "Average loss at step 1870400: 332.467358\n",
      "Average loss at step 1870500: 328.041653\n",
      "Average loss at step 1870600: 332.220401\n",
      "Average loss at step 1870700: 329.174142\n",
      "Average loss at step 1870800: 329.698491\n",
      "Average loss at step 1870900: 335.065989\n",
      "Average loss at step 1871000: 331.483365\n",
      "Average loss at step 1871100: 329.026432\n",
      "Average loss at step 1871200: 331.532903\n",
      "Average loss at step 1871300: 329.914654\n",
      "Average loss at step 1871400: 331.609935\n",
      "Average loss at step 1871500: 331.550014\n",
      "Average loss at step 1871600: 326.142305\n",
      "Average loss at step 1871700: 329.794646\n",
      "Average loss at step 1871800: 330.664817\n",
      "Average loss at step 1871900: 329.274618\n",
      "Average loss at step 1872000: 330.892891\n",
      "Average loss at step 1872100: 328.809498\n",
      "Average loss at step 1872200: 331.033126\n",
      "Average loss at step 1872300: 327.905942\n",
      "Average loss at step 1872400: 329.010102\n",
      "Average loss at step 1872500: 328.263137\n",
      "Average loss at step 1872600: 330.811563\n",
      "Average loss at step 1872700: 330.602226\n",
      "Average loss at step 1872800: 330.481796\n",
      "Average loss at step 1872900: 331.336050\n",
      "Average loss at step 1873000: 332.156712\n",
      "Average loss at step 1873100: 331.380256\n",
      "Average loss at step 1873200: 329.529428\n",
      "Average loss at step 1873300: 328.973681\n",
      "Average loss at step 1873400: 326.393235\n",
      "Average loss at step 1873500: 326.116674\n",
      "Average loss at step 1873600: 328.159610\n",
      "Average loss at step 1873700: 332.778965\n",
      "Average loss at step 1873800: 327.171059\n",
      "Average loss at step 1873900: 330.907693\n",
      "Average loss at step 1874000: 327.332980\n",
      "Average loss at step 1874100: 331.552604\n",
      "Average loss at step 1874200: 332.784055\n",
      "Average loss at step 1874300: 331.688514\n",
      "Average loss at step 1874400: 328.470203\n",
      "Average loss at step 1874500: 331.421974\n",
      "Average loss at step 1874600: 329.676839\n",
      "Average loss at step 1874700: 329.234873\n",
      "Average loss at step 1874800: 330.910966\n",
      "Average loss at step 1874900: 330.664959\n",
      "Average loss at step 1875000: 332.309214\n",
      "Graph 375: 31 nodes\n",
      "Average loss at step 1875100: 333.738809\n",
      "Average loss at step 1875200: 331.613819\n",
      "Average loss at step 1875300: 333.882600\n",
      "Average loss at step 1875400: 332.636382\n",
      "Average loss at step 1875500: 331.860565\n",
      "Average loss at step 1875600: 332.310779\n",
      "Average loss at step 1875700: 330.951524\n",
      "Average loss at step 1875800: 329.454648\n",
      "Average loss at step 1875900: 331.985202\n",
      "Average loss at step 1876000: 333.414479\n",
      "Average loss at step 1876100: 334.184926\n",
      "Average loss at step 1876200: 332.664837\n",
      "Average loss at step 1876300: 331.211655\n",
      "Average loss at step 1876400: 331.061907\n",
      "Average loss at step 1876500: 328.049002\n",
      "Average loss at step 1876600: 332.214635\n",
      "Average loss at step 1876700: 331.884582\n",
      "Average loss at step 1876800: 327.791681\n",
      "Average loss at step 1876900: 334.528406\n",
      "Average loss at step 1877000: 329.400196\n",
      "Average loss at step 1877100: 334.388217\n",
      "Average loss at step 1877200: 331.557997\n",
      "Average loss at step 1877300: 334.025189\n",
      "Average loss at step 1877400: 332.624441\n",
      "Average loss at step 1877500: 330.671002\n",
      "Average loss at step 1877600: 328.888127\n",
      "Average loss at step 1877700: 333.306646\n",
      "Average loss at step 1877800: 328.093061\n",
      "Average loss at step 1877900: 331.238798\n",
      "Average loss at step 1878000: 330.191659\n",
      "Average loss at step 1878100: 328.116324\n",
      "Average loss at step 1878200: 330.046161\n",
      "Average loss at step 1878300: 328.042472\n",
      "Average loss at step 1878400: 328.656934\n",
      "Average loss at step 1878500: 329.478823\n",
      "Average loss at step 1878600: 332.142919\n",
      "Average loss at step 1878700: 330.933187\n",
      "Average loss at step 1878800: 332.630497\n",
      "Average loss at step 1878900: 327.855339\n",
      "Average loss at step 1879000: 327.734190\n",
      "Average loss at step 1879100: 332.129235\n",
      "Average loss at step 1879200: 335.971707\n",
      "Average loss at step 1879300: 329.779031\n",
      "Average loss at step 1879400: 327.696020\n",
      "Average loss at step 1879500: 332.927568\n",
      "Average loss at step 1879600: 333.168000\n",
      "Average loss at step 1879700: 330.598056\n",
      "Average loss at step 1879800: 329.095059\n",
      "Average loss at step 1879900: 329.672008\n",
      "Average loss at step 1880000: 330.382151\n",
      "Graph 376: 27 nodes\n",
      "Average loss at step 1880100: 335.956345\n",
      "Average loss at step 1880200: 337.735326\n",
      "Average loss at step 1880300: 333.333467\n",
      "Average loss at step 1880400: 337.916985\n",
      "Average loss at step 1880500: 334.917619\n",
      "Average loss at step 1880600: 332.965884\n",
      "Average loss at step 1880700: 332.819009\n",
      "Average loss at step 1880800: 335.208378\n",
      "Average loss at step 1880900: 335.408803\n",
      "Average loss at step 1881000: 334.754337\n",
      "Average loss at step 1881100: 333.992968\n",
      "Average loss at step 1881200: 333.218040\n",
      "Average loss at step 1881300: 334.723475\n",
      "Average loss at step 1881400: 332.844348\n",
      "Average loss at step 1881500: 333.471486\n",
      "Average loss at step 1881600: 333.346236\n",
      "Average loss at step 1881700: 333.348849\n",
      "Average loss at step 1881800: 327.881969\n",
      "Average loss at step 1881900: 332.607251\n",
      "Average loss at step 1882000: 333.470941\n",
      "Average loss at step 1882100: 334.155215\n",
      "Average loss at step 1882200: 331.622544\n",
      "Average loss at step 1882300: 332.158400\n",
      "Average loss at step 1882400: 329.619558\n",
      "Average loss at step 1882500: 332.176034\n",
      "Average loss at step 1882600: 328.800409\n",
      "Average loss at step 1882700: 331.717871\n",
      "Average loss at step 1882800: 335.925233\n",
      "Average loss at step 1882900: 328.554389\n",
      "Average loss at step 1883000: 333.018096\n",
      "Average loss at step 1883100: 330.991623\n",
      "Average loss at step 2023800: 328.156595\n",
      "Average loss at step 2023900: 326.604962\n",
      "Average loss at step 2024000: 331.818510\n",
      "Average loss at step 2024100: 328.722085\n",
      "Average loss at step 2024200: 333.557466\n",
      "Average loss at step 2024300: 331.658612\n",
      "Average loss at step 2024400: 326.052690\n",
      "Average loss at step 2024500: 328.390620\n",
      "Average loss at step 2024600: 326.414488\n",
      "Average loss at step 2024700: 326.895787\n",
      "Average loss at step 2024800: 330.645825\n",
      "Average loss at step 2024900: 329.281744\n",
      "Average loss at step 2025000: 331.632196\n",
      "Graph 405: 17 nodes\n",
      "Average loss at step 2025100: 339.359565\n",
      "Average loss at step 2025200: 331.077987\n",
      "Average loss at step 2025300: 331.073290\n",
      "Average loss at step 2025400: 333.611496\n",
      "Average loss at step 2025500: 331.709338\n",
      "Average loss at step 2025600: 329.524025\n",
      "Average loss at step 2025700: 328.959986\n",
      "Average loss at step 2025800: 331.687240\n",
      "Average loss at step 2025900: 333.160560\n",
      "Average loss at step 2026000: 330.519852\n",
      "Average loss at step 2026100: 331.646382\n",
      "Average loss at step 2026200: 327.162650\n",
      "Average loss at step 2026300: 334.691457\n",
      "Average loss at step 2026400: 330.389067\n",
      "Average loss at step 2026500: 329.950655\n",
      "Average loss at step 2026600: 332.277049\n",
      "Average loss at step 2026700: 331.325878\n",
      "Average loss at step 2026800: 328.152979\n",
      "Average loss at step 2026900: 328.984375\n",
      "Average loss at step 2027000: 332.814215\n",
      "Average loss at step 2027100: 328.512273\n",
      "Average loss at step 2027200: 331.510495\n",
      "Average loss at step 2027300: 332.808702\n",
      "Average loss at step 2027400: 329.554470\n",
      "Average loss at step 2027500: 327.839561\n",
      "Average loss at step 2027600: 332.392450\n",
      "Average loss at step 2027700: 332.201228\n",
      "Average loss at step 2027800: 331.415538\n",
      "Average loss at step 2027900: 330.645197\n",
      "Average loss at step 2028000: 327.880069\n",
      "Average loss at step 2028100: 331.295685\n",
      "Average loss at step 2028200: 326.300079\n",
      "Average loss at step 2028300: 330.017736\n",
      "Average loss at step 2028400: 330.096503\n",
      "Average loss at step 2028500: 330.496186\n",
      "Average loss at step 2028600: 330.489215\n",
      "Average loss at step 2028700: 331.392439\n",
      "Average loss at step 2028800: 330.958874\n",
      "Average loss at step 2028900: 328.726634\n",
      "Average loss at step 2029000: 332.147182\n",
      "Average loss at step 2029100: 330.108540\n",
      "Average loss at step 2029200: 330.529030\n",
      "Average loss at step 2029300: 329.747652\n",
      "Average loss at step 2029400: 331.995152\n",
      "Average loss at step 2029500: 330.949335\n",
      "Average loss at step 2029600: 326.685785\n",
      "Average loss at step 2029700: 326.622089\n",
      "Average loss at step 2029800: 330.146253\n",
      "Average loss at step 2029900: 332.567778\n",
      "Average loss at step 2030000: 331.419322\n",
      "Graph 406: 10 nodes\n",
      "Average loss at step 2030100: 341.506089\n",
      "Average loss at step 2030200: 337.641444\n",
      "Average loss at step 2030300: 338.823874\n",
      "Average loss at step 2030400: 337.385566\n",
      "Average loss at step 2030500: 336.194123\n",
      "Average loss at step 2030600: 337.549539\n",
      "Average loss at step 2030700: 336.406819\n",
      "Average loss at step 2030800: 339.462530\n",
      "Average loss at step 2030900: 338.394572\n",
      "Average loss at step 2031000: 333.407358\n",
      "Average loss at step 2031100: 335.358345\n",
      "Average loss at step 2031200: 339.779332\n",
      "Average loss at step 2031300: 334.542800\n",
      "Average loss at step 2031400: 337.723550\n",
      "Average loss at step 2031500: 335.611941\n",
      "Average loss at step 2031600: 334.450947\n",
      "Average loss at step 2031700: 334.963224\n",
      "Average loss at step 2031800: 335.776043\n",
      "Average loss at step 2031900: 337.589850\n",
      "Average loss at step 2032000: 336.734044\n",
      "Average loss at step 2032100: 338.298291\n",
      "Average loss at step 2032200: 335.527767\n",
      "Average loss at step 2032300: 336.901599\n",
      "Average loss at step 2032400: 334.319890\n",
      "Average loss at step 2032500: 337.492610\n",
      "Average loss at step 2032600: 337.938195\n",
      "Average loss at step 2032700: 336.334895\n",
      "Average loss at step 2032800: 338.922864\n",
      "Average loss at step 2032900: 337.285044\n",
      "Average loss at step 2033000: 337.382244\n",
      "Average loss at step 2033100: 335.272093\n",
      "Average loss at step 2033200: 338.409503\n",
      "Average loss at step 2033300: 337.420637\n",
      "Average loss at step 2033400: 331.462740\n",
      "Average loss at step 2033500: 333.096063\n",
      "Average loss at step 2033600: 336.418728\n",
      "Average loss at step 2033700: 336.518729\n",
      "Average loss at step 2033800: 336.366968\n",
      "Average loss at step 2033900: 336.043582\n",
      "Average loss at step 2034000: 335.101985\n",
      "Average loss at step 2034100: 341.323739\n",
      "Average loss at step 2034200: 334.610292\n",
      "Average loss at step 2034300: 337.045231\n",
      "Average loss at step 2034400: 335.402713\n",
      "Average loss at step 2034500: 336.282951\n",
      "Average loss at step 2034600: 337.391737\n",
      "Average loss at step 2034700: 335.452086\n",
      "Average loss at step 2034800: 337.146236\n",
      "Average loss at step 2034900: 336.966849\n",
      "Average loss at step 2035000: 334.942411\n",
      "Graph 407: 12 nodes\n",
      "Average loss at step 2035100: 341.918368\n",
      "Average loss at step 2035200: 336.667007\n",
      "Average loss at step 2035300: 337.483325\n",
      "Average loss at step 2035400: 334.975701\n",
      "Average loss at step 2035500: 331.591019\n",
      "Average loss at step 2035600: 338.064740\n",
      "Average loss at step 2035700: 335.614920\n",
      "Average loss at step 2035800: 334.426610\n",
      "Average loss at step 2035900: 335.369138\n",
      "Average loss at step 2036000: 337.355407\n",
      "Average loss at step 2036100: 334.842190\n",
      "Average loss at step 2036200: 334.580175\n",
      "Average loss at step 2036300: 329.519253\n",
      "Average loss at step 2036400: 336.673299\n",
      "Average loss at step 2036500: 335.031256\n",
      "Average loss at step 2036600: 334.030787\n",
      "Average loss at step 2036700: 336.349653\n",
      "Average loss at step 2036800: 337.014354\n",
      "Average loss at step 2036900: 333.580043\n",
      "Average loss at step 2037000: 329.836260\n",
      "Average loss at step 2037100: 331.920376\n",
      "Average loss at step 2037200: 335.999591\n",
      "Average loss at step 2037300: 328.652417\n",
      "Average loss at step 2037400: 332.437328\n",
      "Average loss at step 2037500: 335.455061\n",
      "Average loss at step 2037600: 332.126982\n",
      "Average loss at step 2037700: 334.457670\n",
      "Average loss at step 2037800: 334.429303\n",
      "Average loss at step 2037900: 334.582238\n",
      "Average loss at step 2038000: 333.820334\n",
      "Average loss at step 2038100: 336.599250\n",
      "Average loss at step 2038200: 335.307727\n",
      "Average loss at step 2038300: 336.505018\n",
      "Average loss at step 2038400: 334.012454\n",
      "Average loss at step 2038500: 332.103827\n",
      "Average loss at step 2038600: 336.796027\n",
      "Average loss at step 2038700: 332.146881\n",
      "Average loss at step 2038800: 328.743693\n",
      "Average loss at step 2038900: 335.074499\n",
      "Average loss at step 2039000: 336.815890\n",
      "Average loss at step 2039100: 334.610160\n",
      "Average loss at step 2039200: 332.987634\n",
      "Average loss at step 2039300: 331.733243\n",
      "Average loss at step 2039400: 332.146037\n",
      "Average loss at step 2039500: 331.745234\n",
      "Average loss at step 2039600: 332.572865\n",
      "Average loss at step 2039700: 330.452114\n",
      "Average loss at step 2039800: 336.380656\n",
      "Average loss at step 2039900: 328.855264\n",
      "Average loss at step 2040000: 335.848083\n",
      "Graph 408: 30 nodes\n",
      "Average loss at step 2040100: 333.930251\n",
      "Average loss at step 2040200: 329.204359\n",
      "Average loss at step 2040300: 333.430171\n",
      "Average loss at step 2040400: 331.573341\n",
      "Average loss at step 2040500: 328.502808\n",
      "Average loss at step 2040600: 328.288864\n",
      "Average loss at step 2040700: 330.527354\n",
      "Average loss at step 2040800: 330.566247\n",
      "Average loss at step 2040900: 328.943057\n",
      "Average loss at step 2041000: 327.648468\n",
      "Average loss at step 2041100: 328.353387\n",
      "Average loss at step 2041200: 328.923250\n",
      "Average loss at step 2041300: 332.289789\n",
      "Average loss at step 2041400: 330.450969\n",
      "Average loss at step 2041500: 330.618631\n",
      "Average loss at step 2041600: 330.482919\n",
      "Average loss at step 2041700: 327.871254\n",
      "Average loss at step 2041800: 327.950001\n",
      "Average loss at step 2041900: 328.574360\n",
      "Average loss at step 2042000: 329.285114\n",
      "Average loss at step 2042100: 325.762788\n",
      "Average loss at step 2042200: 328.460442\n",
      "Average loss at step 2042300: 330.873915\n",
      "Average loss at step 2042400: 327.825938\n",
      "Average loss at step 2042500: 330.753983\n",
      "Average loss at step 2042600: 330.230127\n",
      "Average loss at step 2042700: 327.800788\n",
      "Average loss at step 2042800: 329.112679\n",
      "Average loss at step 2042900: 325.107167\n",
      "Average loss at step 2043000: 325.598101\n",
      "Average loss at step 2043100: 328.317563\n",
      "Average loss at step 2043200: 330.172872\n",
      "Average loss at step 2043300: 328.758228\n",
      "Average loss at step 2043400: 327.062781\n",
      "Average loss at step 2043500: 329.732657\n",
      "Average loss at step 2043600: 326.132439\n",
      "Average loss at step 2043700: 330.830823\n",
      "Average loss at step 2043800: 325.778252\n",
      "Average loss at step 2043900: 323.597677\n",
      "Average loss at step 2044000: 329.775310\n",
      "Average loss at step 2044100: 329.263000\n",
      "Average loss at step 2044200: 329.041696\n",
      "Average loss at step 2044300: 329.291424\n",
      "Average loss at step 2044400: 329.999285\n",
      "Average loss at step 2044500: 331.156008\n",
      "Average loss at step 2044600: 328.103004\n",
      "Average loss at step 2044700: 329.494291\n",
      "Average loss at step 2044800: 334.649346\n",
      "Average loss at step 2044900: 331.046521\n",
      "Average loss at step 2045000: 325.982577\n",
      "Graph 409: 21 nodes\n",
      "Average loss at step 2045100: 336.718182\n",
      "Average loss at step 2045200: 332.968642\n",
      "Average loss at step 2045300: 332.590305\n",
      "Average loss at step 2045400: 335.158462\n",
      "Average loss at step 2045500: 330.654776\n",
      "Average loss at step 2045600: 331.769699\n",
      "Average loss at step 2045700: 335.835856\n",
      "Average loss at step 2045800: 338.961919\n",
      "Average loss at step 2045900: 335.428233\n",
      "Average loss at step 2046000: 331.690217\n",
      "Average loss at step 2046100: 331.197161\n",
      "Average loss at step 2046200: 335.315774\n",
      "Average loss at step 2046300: 336.652550\n",
      "Average loss at step 2046400: 333.011395\n",
      "Average loss at step 2046500: 332.895444\n",
      "Average loss at step 2046600: 332.986289\n",
      "Average loss at step 2046700: 333.315467\n",
      "Average loss at step 2046800: 331.972594\n",
      "Average loss at step 2046900: 334.290793\n",
      "Average loss at step 2047000: 334.999804\n",
      "Average loss at step 2047100: 334.037158\n",
      "Average loss at step 2047200: 330.670483\n",
      "Average loss at step 2047300: 335.277881\n",
      "Average loss at step 2047400: 334.834994\n",
      "Average loss at step 2047500: 331.720405\n",
      "Average loss at step 2047600: 329.047482\n",
      "Average loss at step 2047700: 336.134461\n",
      "Average loss at step 2047800: 334.304317\n",
      "Average loss at step 2047900: 334.443783\n",
      "Average loss at step 2048000: 335.213080\n",
      "Average loss at step 2048100: 331.308713\n",
      "Average loss at step 2048200: 333.568530\n",
      "Average loss at step 2048300: 332.295441\n",
      "Average loss at step 2048400: 333.720705\n",
      "Average loss at step 2048500: 330.175999\n",
      "Average loss at step 2048600: 331.929149\n",
      "Average loss at step 2048700: 335.774014\n",
      "Average loss at step 2048800: 332.155825\n",
      "Average loss at step 2048900: 333.409506\n",
      "Average loss at step 2049000: 336.040616\n",
      "Average loss at step 2049100: 330.276363\n",
      "Average loss at step 2049200: 332.473058\n",
      "Average loss at step 2049300: 332.596258\n",
      "Average loss at step 2049400: 333.732318\n",
      "Average loss at step 2049500: 332.794146\n",
      "Average loss at step 2049600: 331.095571\n",
      "Average loss at step 2049700: 334.908435\n",
      "Average loss at step 2049800: 335.920533\n",
      "Average loss at step 2049900: 333.318401\n",
      "Average loss at step 2050000: 334.183140\n",
      "Graph 410: 18 nodes\n",
      "Average loss at step 2050100: 332.805010\n",
      "Average loss at step 2050200: 329.067600\n",
      "Average loss at step 2050300: 325.083939\n",
      "Average loss at step 2050400: 328.804468\n",
      "Average loss at step 2050500: 327.601887\n",
      "Average loss at step 2050600: 326.272602\n",
      "Average loss at step 2050700: 327.121698\n",
      "Average loss at step 2050800: 325.259833\n",
      "Average loss at step 2050900: 323.519614\n",
      "Average loss at step 2051000: 326.869889\n",
      "Average loss at step 2051100: 324.942026\n",
      "Average loss at step 2051200: 325.955450\n",
      "Average loss at step 2051300: 325.615957\n",
      "Average loss at step 2051400: 328.140656\n",
      "Average loss at step 2051500: 325.450743\n",
      "Average loss at step 2051600: 326.914599\n",
      "Average loss at step 2051700: 323.169822\n",
      "Average loss at step 2051800: 328.270358\n",
      "Average loss at step 2051900: 326.801184\n",
      "Average loss at step 2052000: 323.123091\n",
      "Average loss at step 2052100: 323.235779\n",
      "Average loss at step 2052200: 324.892826\n",
      "Average loss at step 2052300: 321.157128\n",
      "Average loss at step 2052400: 323.968697\n",
      "Average loss at step 2052500: 320.485788\n",
      "Average loss at step 2052600: 324.197397\n",
      "Average loss at step 2052700: 324.743491\n",
      "Average loss at step 2052800: 324.686080\n",
      "Average loss at step 2052900: 322.054767\n",
      "Average loss at step 2053000: 323.522119\n",
      "Average loss at step 2053100: 322.113111\n",
      "Average loss at step 2053200: 323.775794\n",
      "Average loss at step 2053300: 322.518574\n",
      "Average loss at step 2053400: 326.819253\n",
      "Average loss at step 2053500: 324.013663\n",
      "Average loss at step 2053600: 323.758582\n",
      "Average loss at step 2053700: 326.470713\n",
      "Average loss at step 2053800: 324.011177\n",
      "Average loss at step 2053900: 326.506560\n",
      "Average loss at step 2054000: 324.813879\n",
      "Average loss at step 2054100: 321.285581\n",
      "Average loss at step 2054200: 326.613983\n",
      "Average loss at step 2054300: 324.698227\n",
      "Average loss at step 2054400: 324.030162\n",
      "Average loss at step 2054500: 320.282606\n",
      "Average loss at step 2054600: 322.210548\n",
      "Average loss at step 2054700: 327.762521\n",
      "Average loss at step 2205400: 330.825576\n",
      "Average loss at step 2205500: 330.699071\n",
      "Average loss at step 2205600: 331.978836\n",
      "Average loss at step 2205700: 334.023845\n",
      "Average loss at step 2205800: 333.770005\n",
      "Average loss at step 2205900: 333.487555\n",
      "Average loss at step 2206000: 332.123006\n",
      "Average loss at step 2206100: 329.884861\n",
      "Average loss at step 2206200: 335.451222\n",
      "Average loss at step 2206300: 334.290807\n",
      "Average loss at step 2206400: 331.505319\n",
      "Average loss at step 2206500: 333.502867\n",
      "Average loss at step 2206600: 330.879072\n",
      "Average loss at step 2206700: 333.282851\n",
      "Average loss at step 2206800: 330.510134\n",
      "Average loss at step 2206900: 333.724133\n",
      "Average loss at step 2207000: 329.540596\n",
      "Average loss at step 2207100: 329.555281\n",
      "Average loss at step 2207200: 336.240317\n",
      "Average loss at step 2207300: 329.003021\n",
      "Average loss at step 2207400: 332.578313\n",
      "Average loss at step 2207500: 331.518573\n",
      "Average loss at step 2207600: 331.671004\n",
      "Average loss at step 2207700: 332.724471\n",
      "Average loss at step 2207800: 335.167588\n",
      "Average loss at step 2207900: 330.942117\n",
      "Average loss at step 2208000: 332.093711\n",
      "Average loss at step 2208100: 330.098903\n",
      "Average loss at step 2208200: 332.333767\n",
      "Average loss at step 2208300: 329.554420\n",
      "Average loss at step 2208400: 333.824899\n",
      "Average loss at step 2208500: 329.850193\n",
      "Average loss at step 2208600: 334.541822\n",
      "Average loss at step 2208700: 329.938078\n",
      "Average loss at step 2208800: 326.332486\n",
      "Average loss at step 2208900: 331.280344\n",
      "Average loss at step 2209000: 331.044293\n",
      "Average loss at step 2209100: 327.304705\n",
      "Average loss at step 2209200: 330.324218\n",
      "Average loss at step 2209300: 328.435427\n",
      "Average loss at step 2209400: 331.181912\n",
      "Average loss at step 2209500: 333.992238\n",
      "Average loss at step 2209600: 332.951060\n",
      "Average loss at step 2209700: 333.153174\n",
      "Average loss at step 2209800: 331.473653\n",
      "Average loss at step 2209900: 335.612561\n",
      "Average loss at step 2210000: 334.213707\n",
      "Graph 442: 13 nodes\n",
      "Average loss at step 2210100: 336.178018\n",
      "Average loss at step 2210200: 335.809042\n",
      "Average loss at step 2210300: 334.541826\n",
      "Average loss at step 2210400: 333.672275\n",
      "Average loss at step 2210500: 330.768073\n",
      "Average loss at step 2210600: 335.587593\n",
      "Average loss at step 2210700: 328.692328\n",
      "Average loss at step 2210800: 331.454436\n",
      "Average loss at step 2210900: 329.000482\n",
      "Average loss at step 2211000: 328.817461\n",
      "Average loss at step 2211100: 332.047711\n",
      "Average loss at step 2211200: 333.298681\n",
      "Average loss at step 2211300: 329.693859\n",
      "Average loss at step 2211400: 332.186997\n",
      "Average loss at step 2211500: 330.413207\n",
      "Average loss at step 2211600: 329.278878\n",
      "Average loss at step 2211700: 329.011676\n",
      "Average loss at step 2211800: 328.934823\n",
      "Average loss at step 2211900: 331.292375\n",
      "Average loss at step 2212000: 333.163121\n",
      "Average loss at step 2212100: 329.309638\n",
      "Average loss at step 2212200: 327.372747\n",
      "Average loss at step 2212300: 329.908581\n",
      "Average loss at step 2212400: 331.613009\n",
      "Average loss at step 2212500: 331.123981\n",
      "Average loss at step 2212600: 333.291565\n",
      "Average loss at step 2212700: 330.678940\n",
      "Average loss at step 2212800: 330.662168\n",
      "Average loss at step 2212900: 333.704464\n",
      "Average loss at step 2213000: 330.534742\n",
      "Average loss at step 2213100: 332.660289\n",
      "Average loss at step 2213200: 329.109267\n",
      "Average loss at step 2213300: 328.213198\n",
      "Average loss at step 2213400: 332.492646\n",
      "Average loss at step 2213500: 328.405755\n",
      "Average loss at step 2213600: 326.537867\n",
      "Average loss at step 2213700: 329.218588\n",
      "Average loss at step 2213800: 329.561084\n",
      "Average loss at step 2213900: 329.851448\n",
      "Average loss at step 2214000: 328.694367\n",
      "Average loss at step 2214100: 330.590934\n",
      "Average loss at step 2214200: 331.077710\n",
      "Average loss at step 2214300: 330.852804\n",
      "Average loss at step 2214400: 331.426958\n",
      "Average loss at step 2214500: 329.141488\n",
      "Average loss at step 2214600: 329.547372\n",
      "Average loss at step 2214700: 329.496626\n",
      "Average loss at step 2214800: 328.963491\n",
      "Average loss at step 2214900: 327.876037\n",
      "Average loss at step 2215000: 332.768815\n",
      "Graph 443: 13 nodes\n",
      "Average loss at step 2215100: 332.890815\n",
      "Average loss at step 2215200: 333.732562\n",
      "Average loss at step 2215300: 329.798433\n",
      "Average loss at step 2215400: 331.168057\n",
      "Average loss at step 2215500: 328.799442\n",
      "Average loss at step 2215600: 328.721763\n",
      "Average loss at step 2215700: 324.686403\n",
      "Average loss at step 2215800: 330.682317\n",
      "Average loss at step 2215900: 326.349090\n",
      "Average loss at step 2216000: 329.781469\n",
      "Average loss at step 2216100: 328.059080\n",
      "Average loss at step 2216200: 327.350656\n",
      "Average loss at step 2216300: 330.855550\n",
      "Average loss at step 2216400: 330.429972\n",
      "Average loss at step 2216500: 326.874045\n",
      "Average loss at step 2216600: 325.861249\n",
      "Average loss at step 2216700: 326.937994\n",
      "Average loss at step 2216800: 326.113856\n",
      "Average loss at step 2216900: 327.477051\n",
      "Average loss at step 2217000: 331.938298\n",
      "Average loss at step 2217100: 328.646847\n",
      "Average loss at step 2217200: 328.017287\n",
      "Average loss at step 2217300: 325.447190\n",
      "Average loss at step 2217400: 329.785863\n",
      "Average loss at step 2217500: 326.417352\n",
      "Average loss at step 2217600: 328.070376\n",
      "Average loss at step 2217700: 330.130463\n",
      "Average loss at step 2217800: 329.412557\n",
      "Average loss at step 2217900: 323.696067\n",
      "Average loss at step 2218000: 327.758548\n",
      "Average loss at step 2218100: 326.231705\n",
      "Average loss at step 2218200: 326.099193\n",
      "Average loss at step 2218300: 324.243026\n",
      "Average loss at step 2218400: 327.897770\n",
      "Average loss at step 2218500: 328.852274\n",
      "Average loss at step 2218600: 324.356743\n",
      "Average loss at step 2218700: 325.139187\n",
      "Average loss at step 2218800: 327.960095\n",
      "Average loss at step 2218900: 330.477428\n",
      "Average loss at step 2219000: 325.515461\n",
      "Average loss at step 2219100: 327.994786\n",
      "Average loss at step 2219200: 328.128641\n",
      "Average loss at step 2219300: 326.229413\n",
      "Average loss at step 2219400: 326.159651\n",
      "Average loss at step 2219500: 324.586158\n",
      "Average loss at step 2219600: 324.249582\n",
      "Average loss at step 2219700: 324.289199\n",
      "Average loss at step 2219800: 323.886433\n",
      "Average loss at step 2219900: 322.092860\n",
      "Average loss at step 2220000: 325.471002\n",
      "Graph 444: 27 nodes\n",
      "Average loss at step 2220100: 336.571824\n",
      "Average loss at step 2220200: 332.316621\n",
      "Average loss at step 2220300: 331.023750\n",
      "Average loss at step 2220400: 329.555801\n",
      "Average loss at step 2220500: 325.624013\n",
      "Average loss at step 2220600: 326.415215\n",
      "Average loss at step 2220700: 324.140735\n",
      "Average loss at step 2220800: 322.013394\n",
      "Average loss at step 2220900: 324.172439\n",
      "Average loss at step 2221000: 327.858173\n",
      "Average loss at step 2221100: 325.217788\n",
      "Average loss at step 2221200: 322.322898\n",
      "Average loss at step 2221300: 320.636228\n",
      "Average loss at step 2221400: 322.546318\n",
      "Average loss at step 2221500: 325.180222\n",
      "Average loss at step 2221600: 323.555837\n",
      "Average loss at step 2221700: 322.051916\n",
      "Average loss at step 2221800: 324.336948\n",
      "Average loss at step 2221900: 320.897959\n",
      "Average loss at step 2222000: 322.683834\n",
      "Average loss at step 2222100: 321.709086\n",
      "Average loss at step 2222200: 323.023124\n",
      "Average loss at step 2222300: 323.904252\n",
      "Average loss at step 2222400: 319.341396\n",
      "Average loss at step 2222500: 322.985405\n",
      "Average loss at step 2222600: 323.620567\n",
      "Average loss at step 2222700: 321.579352\n",
      "Average loss at step 2222800: 325.590778\n",
      "Average loss at step 2222900: 323.418054\n",
      "Average loss at step 2223000: 320.268406\n",
      "Average loss at step 2223100: 318.569128\n",
      "Average loss at step 2223200: 321.678948\n",
      "Average loss at step 2223300: 323.468623\n",
      "Average loss at step 2223400: 324.399090\n",
      "Average loss at step 2223500: 323.872333\n",
      "Average loss at step 2223600: 321.198944\n",
      "Average loss at step 2223700: 320.651389\n",
      "Average loss at step 2223800: 325.225688\n",
      "Average loss at step 2223900: 323.693814\n",
      "Average loss at step 2224000: 322.084774\n",
      "Average loss at step 2224100: 321.608941\n",
      "Average loss at step 2224200: 323.801399\n",
      "Average loss at step 2224300: 321.474936\n",
      "Average loss at step 2224400: 321.778412\n",
      "Average loss at step 2224500: 326.761241\n",
      "Average loss at step 2224600: 321.527813\n",
      "Average loss at step 2224700: 322.677854\n",
      "Average loss at step 2224800: 321.112081\n",
      "Average loss at step 2224900: 321.620232\n",
      "Average loss at step 2225000: 322.577614\n",
      "Graph 445: 20 nodes\n",
      "Average loss at step 2225100: 334.443280\n",
      "Average loss at step 2225200: 328.227756\n",
      "Average loss at step 2225300: 326.084561\n",
      "Average loss at step 2225400: 327.015346\n",
      "Average loss at step 2225500: 327.394054\n",
      "Average loss at step 2225600: 326.523770\n",
      "Average loss at step 2225700: 323.143360\n",
      "Average loss at step 2225800: 322.744949\n",
      "Average loss at step 2225900: 321.801497\n",
      "Average loss at step 2226000: 325.081441\n",
      "Average loss at step 2226100: 323.736996\n",
      "Average loss at step 2226200: 323.183356\n",
      "Average loss at step 2226300: 325.413805\n",
      "Average loss at step 2226400: 325.550985\n",
      "Average loss at step 2226500: 326.909047\n",
      "Average loss at step 2226600: 322.883443\n",
      "Average loss at step 2226700: 328.749421\n",
      "Average loss at step 2226800: 327.630917\n",
      "Average loss at step 2226900: 325.100371\n",
      "Average loss at step 2227000: 330.438964\n",
      "Average loss at step 2227100: 324.731899\n",
      "Average loss at step 2227200: 323.819196\n",
      "Average loss at step 2227300: 321.555882\n",
      "Average loss at step 2227400: 328.710738\n",
      "Average loss at step 2227500: 327.132830\n",
      "Average loss at step 2227600: 321.985363\n",
      "Average loss at step 2227700: 323.988978\n",
      "Average loss at step 2227800: 323.081705\n",
      "Average loss at step 2227900: 322.895525\n",
      "Average loss at step 2228000: 328.129547\n",
      "Average loss at step 2228100: 324.038656\n",
      "Average loss at step 2228200: 326.757653\n",
      "Average loss at step 2228300: 325.141340\n",
      "Average loss at step 2228400: 320.961758\n",
      "Average loss at step 2228500: 323.159109\n",
      "Average loss at step 2228600: 327.383635\n",
      "Average loss at step 2228700: 326.256935\n",
      "Average loss at step 2228800: 321.554107\n",
      "Average loss at step 2228900: 324.142367\n",
      "Average loss at step 2229000: 324.402925\n",
      "Average loss at step 2229100: 327.414428\n",
      "Average loss at step 2229200: 325.837855\n",
      "Average loss at step 2229300: 324.760256\n",
      "Average loss at step 2229400: 325.830223\n",
      "Average loss at step 2229500: 320.752603\n",
      "Average loss at step 2229600: 324.520091\n",
      "Average loss at step 2229700: 322.307967\n",
      "Average loss at step 2229800: 324.092662\n",
      "Average loss at step 2229900: 326.158137\n",
      "Average loss at step 2230000: 326.081904\n",
      "Graph 446: 28 nodes\n",
      "Average loss at step 2230100: 339.261301\n",
      "Average loss at step 2230200: 338.741038\n",
      "Average loss at step 2230300: 337.619219\n",
      "Average loss at step 2230400: 336.437179\n",
      "Average loss at step 2230500: 336.958930\n",
      "Average loss at step 2230600: 337.341775\n",
      "Average loss at step 2230700: 331.735651\n",
      "Average loss at step 2230800: 338.510280\n",
      "Average loss at step 2230900: 334.546404\n",
      "Average loss at step 2231000: 336.477487\n",
      "Average loss at step 2231100: 329.970111\n",
      "Average loss at step 2231200: 337.773221\n",
      "Average loss at step 2231300: 332.223084\n",
      "Average loss at step 2231400: 336.323077\n",
      "Average loss at step 2231500: 335.784287\n",
      "Average loss at step 2231600: 333.709003\n",
      "Average loss at step 2231700: 331.995369\n",
      "Average loss at step 2231800: 334.632646\n",
      "Average loss at step 2231900: 333.641590\n",
      "Average loss at step 2232000: 329.324929\n",
      "Average loss at step 2232100: 334.452372\n",
      "Average loss at step 2232200: 333.970424\n",
      "Average loss at step 2232300: 332.628874\n",
      "Average loss at step 2232400: 334.620901\n",
      "Average loss at step 2232500: 336.714937\n",
      "Average loss at step 2232600: 334.528069\n",
      "Average loss at step 2232700: 330.669802\n",
      "Average loss at step 2232800: 333.143187\n",
      "Average loss at step 2232900: 332.333651\n",
      "Average loss at step 2233000: 335.087121\n",
      "Average loss at step 2233100: 336.848972\n",
      "Average loss at step 2233200: 336.374238\n",
      "Average loss at step 2233300: 333.524987\n",
      "Average loss at step 2233400: 333.638429\n",
      "Average loss at step 2233500: 336.789806\n",
      "Average loss at step 2233600: 333.691094\n",
      "Average loss at step 2233700: 335.205720\n",
      "Average loss at step 2233800: 334.564148\n",
      "Average loss at step 2233900: 333.846676\n",
      "Average loss at step 2234000: 334.078120\n",
      "Average loss at step 2234100: 333.321069\n",
      "Average loss at step 2234200: 334.656041\n",
      "Average loss at step 2234300: 330.172672\n",
      "Average loss at step 2234400: 334.226754\n",
      "Average loss at step 2234500: 334.111835\n",
      "Average loss at step 2234600: 331.815740\n",
      "Average loss at step 2234700: 335.108134\n",
      "Average loss at step 2234800: 333.331204\n",
      "Average loss at step 2234900: 330.928120\n",
      "Average loss at step 2235000: 334.666010\n",
      "Graph 447: 26 nodes\n",
      "Average loss at step 2235100: 327.634425\n",
      "Average loss at step 2235200: 320.047572\n",
      "Average loss at step 2235300: 323.025805\n",
      "Average loss at step 2235400: 320.698078\n",
      "Average loss at step 2235500: 317.787264\n",
      "Average loss at step 2235600: 318.447619\n",
      "Average loss at step 2235700: 319.959852\n",
      "Average loss at step 2235800: 321.077981\n",
      "Average loss at step 2235900: 317.964881\n",
      "Average loss at step 2236000: 316.038126\n",
      "Average loss at step 2236100: 322.270388\n",
      "Average loss at step 2236200: 318.624564\n",
      "Average loss at step 2236300: 322.025922\n",
      "Average loss at step 2389100: 307.637527\n",
      "Average loss at step 2389200: 303.085352\n",
      "Average loss at step 2389300: 300.430470\n",
      "Average loss at step 2389400: 302.209272\n",
      "Average loss at step 2389500: 302.694163\n",
      "Average loss at step 2389600: 303.390474\n",
      "Average loss at step 2389700: 301.021639\n",
      "Average loss at step 2389800: 307.893208\n",
      "Average loss at step 2389900: 304.791335\n",
      "Average loss at step 2390000: 306.142173\n",
      "Graph 478: 23 nodes\n",
      "Average loss at step 2390100: 341.461343\n",
      "Average loss at step 2390200: 333.989060\n",
      "Average loss at step 2390300: 335.896801\n",
      "Average loss at step 2390400: 334.048653\n",
      "Average loss at step 2390500: 329.118761\n",
      "Average loss at step 2390600: 333.554109\n",
      "Average loss at step 2390700: 331.793416\n",
      "Average loss at step 2390800: 331.078712\n",
      "Average loss at step 2390900: 333.670563\n",
      "Average loss at step 2391000: 327.589348\n",
      "Average loss at step 2391100: 331.238447\n",
      "Average loss at step 2391200: 331.614784\n",
      "Average loss at step 2391300: 329.730876\n",
      "Average loss at step 2391400: 327.681560\n",
      "Average loss at step 2391500: 326.345932\n",
      "Average loss at step 2391600: 332.544268\n",
      "Average loss at step 2391700: 330.133445\n",
      "Average loss at step 2391800: 332.419291\n",
      "Average loss at step 2391900: 328.989735\n",
      "Average loss at step 2392000: 327.603611\n",
      "Average loss at step 2392100: 326.333604\n",
      "Average loss at step 2392200: 326.973845\n",
      "Average loss at step 2392300: 327.807417\n",
      "Average loss at step 2392400: 328.780610\n",
      "Average loss at step 2392500: 329.223212\n",
      "Average loss at step 2392600: 329.839162\n",
      "Average loss at step 2392700: 330.006042\n",
      "Average loss at step 2392800: 328.639710\n",
      "Average loss at step 2392900: 328.756530\n",
      "Average loss at step 2393000: 328.329222\n",
      "Average loss at step 2393100: 330.972764\n",
      "Average loss at step 2393200: 328.138235\n",
      "Average loss at step 2393300: 328.571817\n",
      "Average loss at step 2393400: 327.747486\n",
      "Average loss at step 2393500: 331.485531\n",
      "Average loss at step 2393600: 328.147408\n",
      "Average loss at step 2393700: 328.022264\n",
      "Average loss at step 2393800: 329.390583\n",
      "Average loss at step 2393900: 326.942295\n",
      "Average loss at step 2394000: 328.933333\n",
      "Average loss at step 2394100: 330.615620\n",
      "Average loss at step 2394200: 328.184795\n",
      "Average loss at step 2394300: 326.317904\n",
      "Average loss at step 2394400: 327.767826\n",
      "Average loss at step 2394500: 328.689451\n",
      "Average loss at step 2394600: 326.935222\n",
      "Average loss at step 2394700: 328.323936\n",
      "Average loss at step 2394800: 331.193094\n",
      "Average loss at step 2394900: 331.170690\n",
      "Average loss at step 2395000: 330.998425\n",
      "Graph 479: 36 nodes\n",
      "Average loss at step 2395100: 328.477198\n",
      "Average loss at step 2395200: 322.688574\n",
      "Average loss at step 2395300: 327.292722\n",
      "Average loss at step 2395400: 324.400076\n",
      "Average loss at step 2395500: 325.560027\n",
      "Average loss at step 2395600: 321.469472\n",
      "Average loss at step 2395700: 322.682575\n",
      "Average loss at step 2395800: 319.889403\n",
      "Average loss at step 2395900: 321.877686\n",
      "Average loss at step 2396000: 324.247495\n",
      "Average loss at step 2396100: 317.121562\n",
      "Average loss at step 2396200: 320.799038\n",
      "Average loss at step 2396300: 321.177417\n",
      "Average loss at step 2396400: 322.231333\n",
      "Average loss at step 2396500: 322.332385\n",
      "Average loss at step 2396600: 317.107982\n",
      "Average loss at step 2396700: 321.328418\n",
      "Average loss at step 2396800: 321.235717\n",
      "Average loss at step 2396900: 322.705708\n",
      "Average loss at step 2397000: 314.780355\n",
      "Average loss at step 2397100: 321.048234\n",
      "Average loss at step 2397200: 320.635925\n",
      "Average loss at step 2397300: 319.193928\n",
      "Average loss at step 2397400: 319.779257\n",
      "Average loss at step 2397500: 322.882456\n",
      "Average loss at step 2397600: 317.019751\n",
      "Average loss at step 2397700: 322.413680\n",
      "Average loss at step 2397800: 316.845635\n",
      "Average loss at step 2397900: 319.645634\n",
      "Average loss at step 2398000: 321.474025\n",
      "Average loss at step 2398100: 320.377176\n",
      "Average loss at step 2398200: 323.905174\n",
      "Average loss at step 2398300: 319.413665\n",
      "Average loss at step 2398400: 320.787325\n",
      "Average loss at step 2398500: 324.246037\n",
      "Average loss at step 2398600: 319.455050\n",
      "Average loss at step 2398700: 319.840774\n",
      "Average loss at step 2398800: 320.866099\n",
      "Average loss at step 2398900: 319.763600\n",
      "Average loss at step 2399000: 317.247480\n",
      "Average loss at step 2399100: 318.943125\n",
      "Average loss at step 2399200: 320.027271\n",
      "Average loss at step 2399300: 323.733595\n",
      "Average loss at step 2399400: 321.318436\n",
      "Average loss at step 2399500: 316.615822\n",
      "Average loss at step 2399600: 323.205244\n",
      "Average loss at step 2399700: 318.265585\n",
      "Average loss at step 2399800: 320.230279\n",
      "Average loss at step 2399900: 321.976021\n",
      "Average loss at step 2400000: 320.520225\n",
      "Graph 480: 23 nodes\n",
      "Average loss at step 2400100: 339.975544\n",
      "Average loss at step 2400200: 337.698997\n",
      "Average loss at step 2400300: 334.834502\n",
      "Average loss at step 2400400: 335.865459\n",
      "Average loss at step 2400500: 333.086120\n",
      "Average loss at step 2400600: 335.595163\n",
      "Average loss at step 2400700: 337.624330\n",
      "Average loss at step 2400800: 336.122136\n",
      "Average loss at step 2400900: 334.898216\n",
      "Average loss at step 2401000: 334.432685\n",
      "Average loss at step 2401100: 328.645690\n",
      "Average loss at step 2401200: 334.049671\n",
      "Average loss at step 2401300: 334.005444\n",
      "Average loss at step 2401400: 333.334038\n",
      "Average loss at step 2401500: 329.189570\n",
      "Average loss at step 2401600: 332.050354\n",
      "Average loss at step 2401700: 330.252186\n",
      "Average loss at step 2401800: 334.337135\n",
      "Average loss at step 2401900: 331.455075\n",
      "Average loss at step 2402000: 327.438632\n",
      "Average loss at step 2402100: 337.259808\n",
      "Average loss at step 2402200: 331.288452\n",
      "Average loss at step 2402300: 333.340553\n",
      "Average loss at step 2402400: 330.269765\n",
      "Average loss at step 2402500: 330.862595\n",
      "Average loss at step 2402600: 330.265123\n",
      "Average loss at step 2402700: 331.345610\n",
      "Average loss at step 2402800: 328.614759\n",
      "Average loss at step 2402900: 332.232079\n",
      "Average loss at step 2403000: 328.300652\n",
      "Average loss at step 2403100: 332.518420\n",
      "Average loss at step 2403200: 330.595403\n",
      "Average loss at step 2403300: 331.824369\n",
      "Average loss at step 2403400: 328.440901\n",
      "Average loss at step 2403500: 329.287817\n",
      "Average loss at step 2403600: 333.431091\n",
      "Average loss at step 2403700: 331.557436\n",
      "Average loss at step 2403800: 335.225100\n",
      "Average loss at step 2403900: 330.416320\n",
      "Average loss at step 2404000: 330.479016\n",
      "Average loss at step 2404100: 333.268149\n",
      "Average loss at step 2404200: 329.523331\n",
      "Average loss at step 2404300: 330.273855\n",
      "Average loss at step 2404400: 335.974855\n",
      "Average loss at step 2404500: 333.087343\n",
      "Average loss at step 2404600: 328.141307\n",
      "Average loss at step 2404700: 331.547846\n",
      "Average loss at step 2404800: 333.389845\n",
      "Average loss at step 2404900: 330.844290\n",
      "Average loss at step 2405000: 332.017884\n",
      "Time: 23.7387800217\n",
      "Graph 481: 22 nodes\n",
      "Average loss at step 2405100: 335.375524\n",
      "Average loss at step 2405200: 334.238528\n",
      "Average loss at step 2405300: 335.626920\n",
      "Average loss at step 2405400: 333.971138\n",
      "Average loss at step 2405500: 335.252313\n",
      "Average loss at step 2405600: 333.415852\n",
      "Average loss at step 2405700: 330.223723\n",
      "Average loss at step 2405800: 336.699680\n",
      "Average loss at step 2405900: 335.962033\n",
      "Average loss at step 2406000: 336.760370\n",
      "Average loss at step 2406100: 334.556054\n",
      "Average loss at step 2406200: 333.134451\n",
      "Average loss at step 2406300: 334.046822\n",
      "Average loss at step 2406400: 334.153984\n",
      "Average loss at step 2406500: 332.895694\n",
      "Average loss at step 2406600: 335.247483\n",
      "Average loss at step 2406700: 332.541671\n",
      "Average loss at step 2406800: 330.705745\n",
      "Average loss at step 2406900: 330.165613\n",
      "Average loss at step 2407000: 333.086541\n",
      "Average loss at step 2407100: 335.256046\n",
      "Average loss at step 2407200: 333.246636\n",
      "Average loss at step 2407300: 328.769035\n",
      "Average loss at step 2407400: 332.371267\n",
      "Average loss at step 2407500: 333.133466\n",
      "Average loss at step 2407600: 334.862993\n",
      "Average loss at step 2407700: 331.626132\n",
      "Average loss at step 2407800: 333.420258\n",
      "Average loss at step 2407900: 334.240628\n",
      "Average loss at step 2408000: 332.738345\n",
      "Average loss at step 2408100: 333.849276\n",
      "Average loss at step 2408200: 332.459928\n",
      "Average loss at step 2408300: 331.832478\n",
      "Average loss at step 2408400: 335.530078\n",
      "Average loss at step 2408500: 330.766240\n",
      "Average loss at step 2408600: 332.973291\n",
      "Average loss at step 2408700: 334.009709\n",
      "Average loss at step 2408800: 331.352438\n",
      "Average loss at step 2408900: 336.932299\n",
      "Average loss at step 2409000: 336.879890\n",
      "Average loss at step 2409100: 333.425212\n",
      "Average loss at step 2409200: 333.655443\n",
      "Average loss at step 2409300: 333.769547\n",
      "Average loss at step 2409400: 334.359733\n",
      "Average loss at step 2409500: 333.188817\n",
      "Average loss at step 2409600: 335.299643\n",
      "Average loss at step 2409700: 334.190992\n",
      "Average loss at step 2409800: 333.624459\n",
      "Average loss at step 2409900: 332.244580\n",
      "Average loss at step 2410000: 333.344151\n",
      "Graph 482: 14 nodes\n",
      "Average loss at step 2410100: 334.965407\n",
      "Average loss at step 2410200: 330.722237\n",
      "Average loss at step 2410300: 333.973252\n",
      "Average loss at step 2410400: 329.765476\n",
      "Average loss at step 2410500: 329.013986\n",
      "Average loss at step 2410600: 335.226452\n",
      "Average loss at step 2410700: 332.506346\n",
      "Average loss at step 2410800: 331.538075\n",
      "Average loss at step 2410900: 334.339196\n",
      "Average loss at step 2411000: 329.091206\n",
      "Average loss at step 2411100: 330.379963\n",
      "Average loss at step 2411200: 328.633283\n",
      "Average loss at step 2411300: 328.920693\n",
      "Average loss at step 2411400: 333.981684\n",
      "Average loss at step 2411500: 328.141368\n",
      "Average loss at step 2411600: 331.801044\n",
      "Average loss at step 2411700: 327.026923\n",
      "Average loss at step 2411800: 333.725021\n",
      "Average loss at step 2411900: 333.167977\n",
      "Average loss at step 2412000: 326.471105\n",
      "Average loss at step 2412100: 329.898083\n",
      "Average loss at step 2412200: 331.060118\n",
      "Average loss at step 2412300: 327.452759\n",
      "Average loss at step 2412400: 333.569547\n",
      "Average loss at step 2412500: 334.834229\n",
      "Average loss at step 2412600: 328.589480\n",
      "Average loss at step 2412700: 327.569531\n",
      "Average loss at step 2412800: 328.304620\n",
      "Average loss at step 2412900: 331.977759\n",
      "Average loss at step 2413000: 329.347919\n",
      "Average loss at step 2413100: 326.431984\n",
      "Average loss at step 2413200: 332.073737\n",
      "Average loss at step 2413300: 330.699184\n",
      "Average loss at step 2413400: 327.516054\n",
      "Average loss at step 2413500: 331.009046\n",
      "Average loss at step 2413600: 330.307538\n",
      "Average loss at step 2413700: 330.222368\n",
      "Average loss at step 2413800: 329.932623\n",
      "Average loss at step 2413900: 329.468582\n",
      "Average loss at step 2414000: 332.010299\n",
      "Average loss at step 2414100: 328.060242\n",
      "Average loss at step 2414200: 330.010460\n",
      "Average loss at step 2414300: 328.925131\n",
      "Average loss at step 2414400: 332.945153\n",
      "Average loss at step 2414500: 327.679802\n",
      "Average loss at step 2414600: 328.495990\n",
      "Average loss at step 2414700: 328.069551\n",
      "Average loss at step 2414800: 328.246815\n",
      "Average loss at step 2414900: 329.595950\n",
      "Average loss at step 2415000: 329.254556\n",
      "Graph 483: 22 nodes\n",
      "Average loss at step 2415100: 330.911433\n",
      "Average loss at step 2415200: 333.982352\n",
      "Average loss at step 2415300: 329.928084\n",
      "Average loss at step 2415400: 332.482490\n",
      "Average loss at step 2415500: 328.874117\n",
      "Average loss at step 2415600: 328.963965\n",
      "Average loss at step 2415700: 328.189395\n",
      "Average loss at step 2415800: 328.301705\n",
      "Average loss at step 2415900: 328.323529\n",
      "Average loss at step 2416000: 328.327811\n",
      "Average loss at step 2416100: 330.027292\n",
      "Average loss at step 2416200: 326.319562\n",
      "Average loss at step 2416300: 326.458272\n",
      "Average loss at step 2416400: 326.161901\n",
      "Average loss at step 2416500: 327.337059\n",
      "Average loss at step 2416600: 328.863238\n",
      "Average loss at step 2416700: 324.285156\n",
      "Average loss at step 2416800: 328.662520\n",
      "Average loss at step 2416900: 323.303859\n",
      "Average loss at step 2417000: 327.331601\n",
      "Average loss at step 2417100: 326.769014\n",
      "Average loss at step 2417200: 325.055464\n",
      "Average loss at step 2417300: 323.005235\n",
      "Average loss at step 2417400: 325.391593\n",
      "Average loss at step 2417500: 327.939046\n",
      "Average loss at step 2417600: 326.281918\n",
      "Average loss at step 2417700: 326.036943\n",
      "Average loss at step 2417800: 323.004823\n",
      "Average loss at step 2417900: 325.610962\n",
      "Average loss at step 2418000: 320.467138\n",
      "Average loss at step 2418100: 324.454952\n",
      "Average loss at step 2418200: 324.676404\n",
      "Average loss at step 2418300: 326.410526\n",
      "Average loss at step 2418400: 324.821823\n",
      "Average loss at step 2418500: 326.938145\n",
      "Average loss at step 2418600: 329.335911\n",
      "Average loss at step 2418700: 330.220131\n",
      "Average loss at step 2418800: 323.137268\n",
      "Average loss at step 2418900: 329.053979\n",
      "Average loss at step 2419000: 325.524243\n",
      "Average loss at step 2419100: 325.945025\n",
      "Average loss at step 2419200: 321.545316\n",
      "Average loss at step 2419300: 325.058872\n",
      "Average loss at step 2419400: 326.652234\n",
      "Average loss at step 2419500: 325.582361\n",
      "Average loss at step 2419600: 325.175655\n",
      "Average loss at step 2419700: 326.732276\n",
      "Average loss at step 2419800: 324.363179\n",
      "Average loss at step 2419900: 327.602943\n",
      "Average loss at step 2420000: 330.420713\n",
      "Graph 484: 34 nodes\n",
      "Average loss at step 2583900: 332.724759\n",
      "Average loss at step 2584000: 327.769245\n",
      "Average loss at step 2584100: 330.259521\n",
      "Average loss at step 2584200: 329.281221\n",
      "Average loss at step 2584300: 332.153003\n",
      "Average loss at step 2584400: 330.620602\n",
      "Average loss at step 2584500: 329.541425\n",
      "Average loss at step 2584600: 329.856579\n",
      "Average loss at step 2584700: 329.857134\n",
      "Average loss at step 2584800: 329.697035\n",
      "Average loss at step 2584900: 326.735677\n",
      "Average loss at step 2585000: 331.892529\n",
      "Graph 517: 20 nodes\n",
      "Average loss at step 2585100: 335.903750\n",
      "Average loss at step 2585200: 329.463426\n",
      "Average loss at step 2585300: 332.688884\n",
      "Average loss at step 2585400: 330.418907\n",
      "Average loss at step 2585500: 338.308769\n",
      "Average loss at step 2585600: 332.229132\n",
      "Average loss at step 2585700: 331.492544\n",
      "Average loss at step 2585800: 333.884364\n",
      "Average loss at step 2585900: 332.582506\n",
      "Average loss at step 2586000: 330.047548\n",
      "Average loss at step 2586100: 328.518797\n",
      "Average loss at step 2586200: 330.480165\n",
      "Average loss at step 2586300: 325.618817\n",
      "Average loss at step 2586400: 327.325091\n",
      "Average loss at step 2586500: 328.252302\n",
      "Average loss at step 2586600: 329.925270\n",
      "Average loss at step 2586700: 327.975946\n",
      "Average loss at step 2586800: 328.902952\n",
      "Average loss at step 2586900: 330.824928\n",
      "Average loss at step 2587000: 328.188296\n",
      "Average loss at step 2587100: 326.999066\n",
      "Average loss at step 2587200: 329.335917\n",
      "Average loss at step 2587300: 323.952934\n",
      "Average loss at step 2587400: 329.408438\n",
      "Average loss at step 2587500: 333.447339\n",
      "Average loss at step 2587600: 327.262752\n",
      "Average loss at step 2587700: 324.913852\n",
      "Average loss at step 2587800: 327.677171\n",
      "Average loss at step 2587900: 326.827843\n",
      "Average loss at step 2588000: 327.394591\n",
      "Average loss at step 2588100: 329.289599\n",
      "Average loss at step 2588200: 327.113359\n",
      "Average loss at step 2588300: 325.457093\n",
      "Average loss at step 2588400: 325.660729\n",
      "Average loss at step 2588500: 325.936307\n",
      "Average loss at step 2588600: 326.133232\n",
      "Average loss at step 2588700: 329.548407\n",
      "Average loss at step 2588800: 325.149709\n",
      "Average loss at step 2588900: 331.809025\n",
      "Average loss at step 2589000: 326.701059\n",
      "Average loss at step 2589100: 326.028397\n",
      "Average loss at step 2589200: 330.389565\n",
      "Average loss at step 2589300: 325.564694\n",
      "Average loss at step 2589400: 328.014903\n",
      "Average loss at step 2589500: 328.500485\n",
      "Average loss at step 2589600: 328.196697\n",
      "Average loss at step 2589700: 326.766728\n",
      "Average loss at step 2589800: 327.261816\n",
      "Average loss at step 2589900: 329.039567\n",
      "Average loss at step 2590000: 328.389059\n",
      "Graph 518: 20 nodes\n",
      "Average loss at step 2590100: 327.180423\n",
      "Average loss at step 2590200: 329.536434\n",
      "Average loss at step 2590300: 325.380081\n",
      "Average loss at step 2590400: 326.234113\n",
      "Average loss at step 2590500: 326.022433\n",
      "Average loss at step 2590600: 327.361218\n",
      "Average loss at step 2590700: 325.783187\n",
      "Average loss at step 2590800: 327.621778\n",
      "Average loss at step 2590900: 322.776535\n",
      "Average loss at step 2591000: 326.805061\n",
      "Average loss at step 2591100: 325.153140\n",
      "Average loss at step 2591200: 328.176838\n",
      "Average loss at step 2591300: 327.298762\n",
      "Average loss at step 2591400: 323.348215\n",
      "Average loss at step 2591500: 325.723793\n",
      "Average loss at step 2591600: 326.920984\n",
      "Average loss at step 2591700: 322.186988\n",
      "Average loss at step 2591800: 322.722877\n",
      "Average loss at step 2591900: 327.105566\n",
      "Average loss at step 2592000: 322.334548\n",
      "Average loss at step 2592100: 323.589302\n",
      "Average loss at step 2592200: 324.357719\n",
      "Average loss at step 2592300: 326.217603\n",
      "Average loss at step 2592400: 327.505080\n",
      "Average loss at step 2592500: 326.396098\n",
      "Average loss at step 2592600: 322.771573\n",
      "Average loss at step 2592700: 326.022771\n",
      "Average loss at step 2592800: 322.004885\n",
      "Average loss at step 2592900: 324.645977\n",
      "Average loss at step 2593000: 328.455021\n",
      "Average loss at step 2593100: 324.132800\n",
      "Average loss at step 2593200: 324.640405\n",
      "Average loss at step 2593300: 322.412622\n",
      "Average loss at step 2593400: 325.136171\n",
      "Average loss at step 2593500: 325.807741\n",
      "Average loss at step 2593600: 328.359172\n",
      "Average loss at step 2593700: 324.133146\n",
      "Average loss at step 2593800: 325.318546\n",
      "Average loss at step 2593900: 327.531023\n",
      "Average loss at step 2594000: 326.356674\n",
      "Average loss at step 2594100: 324.424347\n",
      "Average loss at step 2594200: 326.305471\n",
      "Average loss at step 2594300: 327.714035\n",
      "Average loss at step 2594400: 321.590323\n",
      "Average loss at step 2594500: 327.907870\n",
      "Average loss at step 2594600: 324.930144\n",
      "Average loss at step 2594700: 325.374764\n",
      "Average loss at step 2594800: 324.093044\n",
      "Average loss at step 2594900: 324.968099\n",
      "Average loss at step 2595000: 321.436832\n",
      "Graph 519: 17 nodes\n",
      "Average loss at step 2595100: 333.126438\n",
      "Average loss at step 2595200: 329.895998\n",
      "Average loss at step 2595300: 327.782306\n",
      "Average loss at step 2595400: 327.999516\n",
      "Average loss at step 2595500: 328.074594\n",
      "Average loss at step 2595600: 325.262162\n",
      "Average loss at step 2595700: 330.029791\n",
      "Average loss at step 2595800: 327.215826\n",
      "Average loss at step 2595900: 321.976275\n",
      "Average loss at step 2596000: 327.709733\n",
      "Average loss at step 2596100: 325.330343\n",
      "Average loss at step 2596200: 325.072988\n",
      "Average loss at step 2596300: 323.058284\n",
      "Average loss at step 2596400: 324.988297\n",
      "Average loss at step 2596500: 322.273618\n",
      "Average loss at step 2596600: 320.796040\n",
      "Average loss at step 2596700: 324.709531\n",
      "Average loss at step 2596800: 323.440014\n",
      "Average loss at step 2596900: 322.173888\n",
      "Average loss at step 2597000: 325.606961\n",
      "Average loss at step 2597100: 322.581443\n",
      "Average loss at step 2597200: 323.494187\n",
      "Average loss at step 2597300: 321.619960\n",
      "Average loss at step 2597400: 324.710007\n",
      "Average loss at step 2597500: 320.628578\n",
      "Average loss at step 2597600: 319.405634\n",
      "Average loss at step 2597700: 323.066379\n",
      "Average loss at step 2597800: 325.378519\n",
      "Average loss at step 2597900: 320.987742\n",
      "Average loss at step 2598000: 320.617907\n",
      "Average loss at step 2598100: 319.765880\n",
      "Average loss at step 2598200: 324.848035\n",
      "Average loss at step 2598300: 323.154279\n",
      "Average loss at step 2598400: 321.187795\n",
      "Average loss at step 2598500: 321.803836\n",
      "Average loss at step 2598600: 323.722275\n",
      "Average loss at step 2598700: 324.921545\n",
      "Average loss at step 2598800: 318.635043\n",
      "Average loss at step 2598900: 319.451204\n",
      "Average loss at step 2599000: 314.772756\n",
      "Average loss at step 2599100: 320.365058\n",
      "Average loss at step 2599200: 325.499837\n",
      "Average loss at step 2683000: 325.108078\n",
      "Average loss at step 2683100: 320.483404\n",
      "Average loss at step 2683200: 320.295847\n",
      "Average loss at step 2683300: 320.357422\n",
      "Average loss at step 2683400: 317.295330\n",
      "Average loss at step 2683500: 321.451843\n",
      "Average loss at step 2683600: 321.696967\n",
      "Average loss at step 2683700: 317.392812\n",
      "Average loss at step 2683800: 317.095535\n",
      "Average loss at step 2683900: 313.186188\n",
      "Average loss at step 2684000: 323.583008\n",
      "Average loss at step 2684100: 321.927437\n",
      "Average loss at step 2684200: 318.573613\n",
      "Average loss at step 2684300: 321.210769\n",
      "Average loss at step 2684400: 323.049534\n",
      "Average loss at step 2684500: 314.937262\n",
      "Average loss at step 2684600: 321.359576\n",
      "Average loss at step 2684700: 321.440289\n",
      "Average loss at step 2684800: 322.088802\n",
      "Average loss at step 2684900: 320.260393\n",
      "Average loss at step 2685000: 321.448074\n",
      "Graph 537: 23 nodes\n",
      "Average loss at step 2685100: 332.165705\n",
      "Average loss at step 2685200: 330.204666\n",
      "Average loss at step 2685300: 326.917735\n",
      "Average loss at step 2685400: 326.219598\n",
      "Average loss at step 2685500: 326.821210\n",
      "Average loss at step 2685600: 324.463166\n",
      "Average loss at step 2685700: 327.896989\n",
      "Average loss at step 2685800: 329.524204\n",
      "Average loss at step 2685900: 327.684183\n",
      "Average loss at step 2686000: 323.282674\n",
      "Average loss at step 2686100: 326.011078\n",
      "Average loss at step 2686200: 328.873692\n",
      "Average loss at step 2686300: 327.185237\n",
      "Average loss at step 2686400: 324.976465\n",
      "Average loss at step 2686500: 325.165872\n",
      "Average loss at step 2686600: 330.590984\n",
      "Average loss at step 2686700: 325.732890\n",
      "Average loss at step 2686800: 325.457097\n",
      "Average loss at step 2686900: 330.902149\n",
      "Average loss at step 2687000: 324.629540\n",
      "Average loss at step 2687100: 325.620394\n",
      "Average loss at step 2687200: 329.378603\n",
      "Average loss at step 2687300: 325.624498\n",
      "Average loss at step 2687400: 328.492608\n",
      "Average loss at step 2687500: 325.627518\n",
      "Average loss at step 2687600: 320.171971\n",
      "Average loss at step 2687700: 322.581206\n",
      "Average loss at step 2687800: 325.889124\n",
      "Average loss at step 2687900: 325.624594\n",
      "Average loss at step 2688000: 325.426166\n",
      "Average loss at step 2688100: 325.864233\n",
      "Average loss at step 2688200: 327.039388\n",
      "Average loss at step 2688300: 327.581866\n",
      "Average loss at step 2688400: 326.858651\n",
      "Average loss at step 2688500: 329.926823\n",
      "Average loss at step 2688600: 326.237698\n",
      "Average loss at step 2688700: 324.802545\n",
      "Average loss at step 2688800: 327.183663\n",
      "Average loss at step 2688900: 327.634217\n",
      "Average loss at step 2689000: 325.771534\n",
      "Average loss at step 2689100: 326.182626\n",
      "Average loss at step 2689200: 327.152312\n",
      "Average loss at step 2689300: 323.841965\n",
      "Average loss at step 2689400: 323.796164\n",
      "Average loss at step 2689500: 326.334335\n",
      "Average loss at step 2689600: 326.141222\n",
      "Average loss at step 2689700: 327.135582\n",
      "Average loss at step 2689800: 328.043302\n",
      "Average loss at step 2689900: 325.581460\n",
      "Average loss at step 2690000: 325.143594\n",
      "Graph 538: 17 nodes\n",
      "Average loss at step 2690100: 334.943739\n",
      "Average loss at step 2690200: 333.782482\n",
      "Average loss at step 2690300: 335.517823\n",
      "Average loss at step 2690400: 333.330085\n",
      "Average loss at step 2690500: 330.866881\n",
      "Average loss at step 2690600: 330.576053\n",
      "Average loss at step 2690700: 328.733613\n",
      "Average loss at step 2690800: 334.120522\n",
      "Average loss at step 2690900: 329.420148\n",
      "Average loss at step 2691000: 330.298142\n",
      "Average loss at step 2691100: 335.648434\n",
      "Average loss at step 2691200: 330.102957\n",
      "Average loss at step 2691300: 329.465562\n",
      "Average loss at step 2691400: 333.791431\n",
      "Average loss at step 2691500: 331.192883\n",
      "Average loss at step 2691600: 329.223092\n",
      "Average loss at step 2691700: 327.371984\n",
      "Average loss at step 2691800: 330.111408\n",
      "Average loss at step 2691900: 333.210584\n",
      "Average loss at step 2692000: 333.009666\n",
      "Average loss at step 2692100: 332.225599\n",
      "Average loss at step 2692200: 329.526173\n",
      "Average loss at step 2692300: 329.209694\n",
      "Average loss at step 2692400: 329.688440\n",
      "Average loss at step 2692500: 325.860868\n",
      "Average loss at step 2692600: 329.876093\n",
      "Average loss at step 2692700: 331.032013\n",
      "Average loss at step 2692800: 329.344383\n",
      "Average loss at step 2692900: 331.644577\n",
      "Average loss at step 2693000: 330.870804\n",
      "Average loss at step 2693100: 329.658911\n",
      "Average loss at step 2693200: 329.081516\n",
      "Average loss at step 2693300: 326.709439\n",
      "Average loss at step 2693400: 331.466268\n",
      "Average loss at step 2693500: 331.536031\n",
      "Average loss at step 2693600: 329.893506\n",
      "Average loss at step 2693700: 333.421223\n",
      "Average loss at step 2693800: 329.133099\n",
      "Average loss at step 2693900: 331.219586\n",
      "Average loss at step 2694000: 334.409445\n",
      "Average loss at step 2694100: 326.680701\n",
      "Average loss at step 2694200: 328.097407\n",
      "Average loss at step 2694300: 328.978690\n",
      "Average loss at step 2694400: 330.726385\n",
      "Average loss at step 2694500: 330.812284\n",
      "Average loss at step 2694600: 331.491322\n",
      "Average loss at step 2694700: 327.910010\n",
      "Average loss at step 2694800: 330.874262\n",
      "Average loss at step 2694900: 330.690418\n",
      "Average loss at step 2695000: 326.297827\n",
      "Graph 539: 21 nodes\n",
      "Average loss at step 2695100: 437.434898\n",
      "Average loss at step 2695200: 373.218801\n",
      "Average loss at step 2695300: 379.512467\n",
      "Average loss at step 2695400: 373.955291\n",
      "Average loss at step 2695500: 371.006316\n",
      "Average loss at step 2695600: 368.598283\n",
      "Average loss at step 2695700: 368.653257\n",
      "Average loss at step 2695800: 375.395340\n",
      "Average loss at step 2695900: 362.739259\n",
      "Average loss at step 2696000: 367.000791\n",
      "Average loss at step 2696100: 367.739362\n",
      "Average loss at step 2696200: 369.453180\n",
      "Average loss at step 2696300: 363.963966\n",
      "Average loss at step 2696400: 362.471603\n",
      "Average loss at step 2696500: 370.316714\n",
      "Average loss at step 2696600: 362.646913\n",
      "Average loss at step 2696700: 371.695905\n",
      "Average loss at step 2696800: 367.855984\n",
      "Average loss at step 2696900: 358.466721\n",
      "Average loss at step 2697000: 364.063082\n",
      "Average loss at step 2697100: 367.410731\n",
      "Average loss at step 2697200: 364.743446\n",
      "Average loss at step 2697300: 368.102273\n",
      "Average loss at step 2697400: 367.002321\n",
      "Average loss at step 2697500: 366.915507\n",
      "Average loss at step 2697600: 363.356501\n",
      "Average loss at step 2697700: 363.313601\n",
      "Average loss at step 2697800: 364.616891\n",
      "Average loss at step 2697900: 365.342655\n",
      "Average loss at step 2698000: 362.158205\n",
      "Average loss at step 2698100: 367.074239\n",
      "Average loss at step 2698200: 362.883680\n",
      "Average loss at step 2698300: 365.794150\n",
      "Average loss at step 2782300: 332.188032\n",
      "Average loss at step 2782400: 330.332460\n",
      "Average loss at step 2782500: 324.930945\n",
      "Average loss at step 2782600: 328.845618\n",
      "Average loss at step 2782700: 330.469314\n",
      "Average loss at step 2782800: 328.493296\n",
      "Average loss at step 2782900: 330.372397\n",
      "Average loss at step 2783000: 329.744015\n",
      "Average loss at step 2783100: 330.285536\n",
      "Average loss at step 2783200: 331.625736\n",
      "Average loss at step 2783300: 330.233849\n",
      "Average loss at step 2783400: 327.804712\n",
      "Average loss at step 2783500: 330.535596\n",
      "Average loss at step 2783600: 332.104516\n",
      "Average loss at step 2783700: 330.843039\n",
      "Average loss at step 2783800: 331.465884\n",
      "Average loss at step 2783900: 332.846486\n",
      "Average loss at step 2784000: 330.618848\n",
      "Average loss at step 2784100: 325.684356\n",
      "Average loss at step 2784200: 329.401737\n",
      "Average loss at step 2784300: 329.691970\n",
      "Average loss at step 2784400: 329.306831\n",
      "Average loss at step 2784500: 329.022913\n",
      "Average loss at step 2784600: 333.068869\n",
      "Average loss at step 2784700: 329.021246\n",
      "Average loss at step 2784800: 329.087778\n",
      "Average loss at step 2784900: 329.168751\n",
      "Average loss at step 2785000: 329.908080\n",
      "Graph 557: 28 nodes\n",
      "Average loss at step 2785100: 334.801821\n",
      "Average loss at step 2785200: 337.039368\n",
      "Average loss at step 2785300: 333.542062\n",
      "Average loss at step 2785400: 335.332185\n",
      "Average loss at step 2785500: 334.517579\n",
      "Average loss at step 2785600: 334.674932\n",
      "Average loss at step 2785700: 336.120653\n",
      "Average loss at step 2785800: 335.647786\n",
      "Average loss at step 2785900: 331.958913\n",
      "Average loss at step 2786000: 333.163442\n",
      "Average loss at step 2786100: 333.563870\n",
      "Average loss at step 2786200: 331.103838\n",
      "Average loss at step 2786300: 334.217371\n",
      "Average loss at step 2786400: 332.181036\n",
      "Average loss at step 2786500: 334.356600\n",
      "Average loss at step 2786600: 333.529558\n",
      "Average loss at step 2786700: 331.054359\n",
      "Average loss at step 2786800: 333.607535\n",
      "Average loss at step 2786900: 327.891048\n",
      "Average loss at step 2787000: 334.088793\n",
      "Average loss at step 2787100: 336.336528\n",
      "Average loss at step 2787200: 330.069750\n",
      "Average loss at step 2787300: 329.879969\n",
      "Average loss at step 2787400: 334.232812\n",
      "Average loss at step 2787500: 337.368034\n",
      "Average loss at step 2787600: 333.003371\n",
      "Average loss at step 2787700: 332.561140\n",
      "Average loss at step 2787800: 332.454019\n",
      "Average loss at step 2787900: 331.687877\n",
      "Average loss at step 2788000: 335.399277\n",
      "Average loss at step 2788100: 333.581426\n",
      "Average loss at step 2788200: 331.333345\n",
      "Average loss at step 2788300: 330.907167\n",
      "Average loss at step 2788400: 332.314614\n",
      "Average loss at step 2788500: 331.174122\n",
      "Average loss at step 2788600: 330.455895\n",
      "Average loss at step 2788700: 330.786073\n",
      "Average loss at step 2788800: 334.758375\n",
      "Average loss at step 2788900: 336.132685\n",
      "Average loss at step 2789000: 330.737069\n",
      "Average loss at step 2789100: 329.072628\n",
      "Average loss at step 2789200: 332.264569\n",
      "Average loss at step 2789300: 330.025888\n",
      "Average loss at step 2789400: 328.855077\n",
      "Average loss at step 2789500: 330.493990\n",
      "Average loss at step 2789600: 333.025835\n",
      "Average loss at step 2789700: 332.439420\n",
      "Average loss at step 2789800: 332.022726\n",
      "Average loss at step 2789900: 336.375380\n",
      "Average loss at step 2790000: 331.600180\n",
      "Graph 558: 12 nodes\n",
      "Average loss at step 2790100: 333.105319\n",
      "Average loss at step 2790200: 333.262801\n",
      "Average loss at step 2790300: 329.185572\n",
      "Average loss at step 2790400: 331.672496\n",
      "Average loss at step 2790500: 330.146985\n",
      "Average loss at step 2790600: 332.648378\n",
      "Average loss at step 2790700: 330.966593\n",
      "Average loss at step 2790800: 332.832829\n",
      "Average loss at step 2790900: 328.099953\n",
      "Average loss at step 2791000: 329.175619\n",
      "Average loss at step 2791100: 326.314499\n",
      "Average loss at step 2791200: 331.927207\n",
      "Average loss at step 2791300: 329.242778\n",
      "Average loss at step 2791400: 328.247919\n",
      "Average loss at step 2791500: 329.652054\n",
      "Average loss at step 2791600: 327.638927\n",
      "Average loss at step 2791700: 329.194089\n",
      "Average loss at step 2791800: 330.085079\n",
      "Average loss at step 2791900: 330.636626\n",
      "Average loss at step 2792000: 329.285121\n",
      "Average loss at step 2792100: 328.396115\n",
      "Average loss at step 2792200: 328.603296\n",
      "Average loss at step 2792300: 329.459632\n",
      "Average loss at step 2792400: 328.871764\n",
      "Average loss at step 2792500: 329.555399\n",
      "Average loss at step 2792600: 327.785621\n",
      "Average loss at step 2792700: 328.383484\n",
      "Average loss at step 2792800: 327.235214\n",
      "Average loss at step 2792900: 329.112496\n",
      "Average loss at step 2793000: 328.099429\n",
      "Average loss at step 2793100: 328.668167\n",
      "Average loss at step 2793200: 330.527046\n",
      "Average loss at step 2793300: 326.681159\n",
      "Average loss at step 2793400: 328.542061\n",
      "Average loss at step 2793500: 326.758404\n",
      "Average loss at step 2793600: 327.852373\n",
      "Average loss at step 2793700: 327.691372\n",
      "Average loss at step 2793800: 328.937577\n",
      "Average loss at step 2793900: 329.012980\n",
      "Average loss at step 2794000: 327.617727\n",
      "Average loss at step 2794100: 327.055770\n",
      "Average loss at step 2794200: 327.454489\n",
      "Average loss at step 2794300: 329.977637\n",
      "Average loss at step 2794400: 330.338507\n",
      "Average loss at step 2794500: 325.221928\n",
      "Average loss at step 2794600: 328.951092\n",
      "Average loss at step 2794700: 329.823152\n",
      "Average loss at step 2794800: 326.183904\n",
      "Average loss at step 2794900: 331.072132\n",
      "Average loss at step 2795000: 327.449970\n",
      "Graph 559: 34 nodes\n",
      "Average loss at step 2795100: 338.567651\n",
      "Average loss at step 2795200: 339.115474\n",
      "Average loss at step 2795300: 334.937088\n",
      "Average loss at step 2795400: 334.311621\n",
      "Average loss at step 2795500: 335.247468\n",
      "Average loss at step 2795600: 335.988111\n",
      "Average loss at step 2795700: 332.461741\n",
      "Average loss at step 2795800: 334.941560\n",
      "Average loss at step 2795900: 337.577445\n",
      "Average loss at step 2796000: 332.459639\n",
      "Average loss at step 2796100: 332.407676\n",
      "Average loss at step 2796200: 335.661972\n",
      "Average loss at step 2796300: 332.268581\n",
      "Average loss at step 2796400: 330.595922\n",
      "Average loss at step 2796500: 333.120914\n",
      "Average loss at step 2796600: 335.941968\n",
      "Average loss at step 2796700: 335.242219\n",
      "Average loss at step 2796800: 333.023493\n",
      "Average loss at step 2796900: 338.868956\n",
      "Average loss at step 2797000: 334.274472\n",
      "Average loss at step 2797100: 333.984107\n",
      "Average loss at step 2797200: 329.781478\n",
      "Average loss at step 2797300: 333.693701\n",
      "Average loss at step 2797400: 335.652769\n",
      "Average loss at step 2797500: 334.728359\n",
      "Average loss at step 2797600: 335.469830\n",
      "Average loss at step 2881800: 321.815615\n",
      "Average loss at step 2881900: 323.934514\n",
      "Average loss at step 2882000: 324.745460\n",
      "Average loss at step 2882100: 327.147153\n",
      "Average loss at step 2882200: 321.861547\n",
      "Average loss at step 2882300: 327.035279\n",
      "Average loss at step 2882400: 327.728686\n",
      "Average loss at step 2882500: 321.686092\n",
      "Average loss at step 2882600: 328.009843\n",
      "Average loss at step 2882700: 324.608749\n",
      "Average loss at step 2882800: 328.214612\n",
      "Average loss at step 2882900: 324.051072\n",
      "Average loss at step 2883000: 323.525225\n",
      "Average loss at step 2883100: 322.789459\n",
      "Average loss at step 2883200: 327.172123\n",
      "Average loss at step 2883300: 328.893272\n",
      "Average loss at step 2883400: 322.850748\n",
      "Average loss at step 2883500: 323.958579\n",
      "Average loss at step 2883600: 326.278297\n",
      "Average loss at step 2883700: 326.047916\n",
      "Average loss at step 2883800: 325.054118\n",
      "Average loss at step 2883900: 328.599613\n",
      "Average loss at step 2884000: 324.199894\n",
      "Average loss at step 2884100: 325.895702\n",
      "Average loss at step 2884200: 322.898662\n",
      "Average loss at step 2884300: 321.009931\n",
      "Average loss at step 2884400: 325.061027\n",
      "Average loss at step 2884500: 327.564183\n",
      "Average loss at step 2884600: 326.411065\n",
      "Average loss at step 2884700: 325.652161\n",
      "Average loss at step 2884800: 323.929157\n",
      "Average loss at step 2884900: 322.809637\n",
      "Average loss at step 2885000: 326.442959\n",
      "Graph 577: 24 nodes\n",
      "Average loss at step 2885100: 334.232280\n",
      "Average loss at step 2885200: 334.987366\n",
      "Average loss at step 2885300: 330.063707\n",
      "Average loss at step 2885400: 330.299316\n",
      "Average loss at step 2885500: 332.010418\n",
      "Average loss at step 2885600: 331.424839\n",
      "Average loss at step 2885700: 333.187051\n",
      "Average loss at step 2885800: 333.527844\n",
      "Average loss at step 2885900: 331.100570\n",
      "Average loss at step 2886000: 333.936731\n",
      "Average loss at step 2886100: 330.209442\n",
      "Average loss at step 2886200: 334.566483\n",
      "Average loss at step 2886300: 328.512661\n",
      "Average loss at step 2886400: 332.390714\n",
      "Average loss at step 2886500: 331.230175\n",
      "Average loss at step 2886600: 334.991985\n",
      "Average loss at step 2886700: 328.454978\n",
      "Average loss at step 2886800: 333.276452\n",
      "Average loss at step 2886900: 331.999945\n",
      "Average loss at step 2887000: 329.987504\n",
      "Average loss at step 2887100: 328.563998\n",
      "Average loss at step 2887200: 331.810181\n",
      "Average loss at step 2887300: 330.836164\n",
      "Average loss at step 2887400: 329.342863\n",
      "Average loss at step 2887500: 329.220253\n",
      "Average loss at step 2887600: 331.494837\n",
      "Average loss at step 2887700: 328.494607\n",
      "Average loss at step 2887800: 329.852675\n",
      "Average loss at step 2887900: 331.057952\n",
      "Average loss at step 2888000: 330.437175\n",
      "Average loss at step 2888100: 331.560902\n",
      "Average loss at step 2888200: 331.205652\n",
      "Average loss at step 2888300: 326.503593\n",
      "Average loss at step 2888400: 330.557073\n",
      "Average loss at step 2888500: 332.757406\n",
      "Average loss at step 2888600: 330.538473\n",
      "Average loss at step 2888700: 329.897970\n",
      "Average loss at step 2888800: 328.772467\n",
      "Average loss at step 2888900: 332.615235\n",
      "Average loss at step 2889000: 328.758938\n",
      "Average loss at step 2889100: 330.017456\n",
      "Average loss at step 2889200: 332.500081\n",
      "Average loss at step 2889300: 331.007421\n",
      "Average loss at step 2889400: 329.820211\n",
      "Average loss at step 2889500: 330.518507\n",
      "Average loss at step 2889600: 332.269436\n",
      "Average loss at step 2889700: 325.830550\n",
      "Average loss at step 2889800: 331.136675\n",
      "Average loss at step 2889900: 327.590787\n",
      "Average loss at step 2890000: 328.588533\n",
      "Graph 578: 20 nodes\n",
      "Average loss at step 2890100: 332.462080\n",
      "Average loss at step 2890200: 329.651253\n",
      "Average loss at step 2890300: 326.827608\n",
      "Average loss at step 2890400: 328.218011\n",
      "Average loss at step 2890500: 329.674585\n",
      "Average loss at step 2890600: 331.817312\n",
      "Average loss at step 2890700: 330.688673\n",
      "Average loss at step 2890800: 324.720391\n",
      "Average loss at step 2890900: 329.485065\n",
      "Average loss at step 2891000: 330.896134\n",
      "Average loss at step 2891100: 327.340823\n",
      "Average loss at step 2891200: 326.734708\n",
      "Average loss at step 2891300: 329.166038\n",
      "Average loss at step 2891400: 330.212448\n",
      "Average loss at step 2891500: 329.521931\n",
      "Average loss at step 2891600: 328.356415\n",
      "Average loss at step 2891700: 329.631487\n",
      "Average loss at step 2891800: 330.514801\n",
      "Average loss at step 2891900: 327.717850\n",
      "Average loss at step 2892000: 328.836344\n",
      "Average loss at step 2892100: 327.624425\n",
      "Average loss at step 2892200: 328.434240\n",
      "Average loss at step 2892300: 329.539538\n",
      "Average loss at step 2892400: 328.278536\n",
      "Average loss at step 2892500: 323.224207\n",
      "Average loss at step 2892600: 328.774609\n",
      "Average loss at step 2892700: 326.336089\n",
      "Average loss at step 2892800: 328.526247\n",
      "Average loss at step 2892900: 330.047287\n",
      "Average loss at step 2893000: 325.793466\n",
      "Average loss at step 2893100: 329.476614\n",
      "Average loss at step 2893200: 325.165403\n",
      "Average loss at step 2893300: 330.020352\n",
      "Average loss at step 2893400: 329.071194\n",
      "Average loss at step 2893500: 326.938745\n",
      "Average loss at step 2893600: 325.165107\n",
      "Average loss at step 2893700: 325.110536\n",
      "Average loss at step 2893800: 328.529686\n",
      "Average loss at step 2893900: 325.064850\n",
      "Average loss at step 2894000: 327.683414\n",
      "Average loss at step 2894100: 327.166962\n",
      "Average loss at step 2894200: 330.226733\n",
      "Average loss at step 2894300: 330.811920\n",
      "Average loss at step 2894400: 329.402970\n",
      "Average loss at step 2894500: 329.319614\n",
      "Average loss at step 2894600: 325.532512\n",
      "Average loss at step 2894700: 328.029951\n",
      "Average loss at step 2894800: 327.123963\n",
      "Average loss at step 2894900: 327.502970\n",
      "Average loss at step 2895000: 328.017944\n",
      "Graph 579: 21 nodes\n",
      "Average loss at step 2895100: 343.984642\n",
      "Average loss at step 2895200: 336.421344\n",
      "Average loss at step 2895300: 336.994370\n",
      "Average loss at step 2895400: 333.832054\n",
      "Average loss at step 2895500: 336.204992\n",
      "Average loss at step 2895600: 334.551130\n",
      "Average loss at step 2895700: 334.244489\n",
      "Average loss at step 2895800: 335.205316\n",
      "Average loss at step 2895900: 336.226062\n",
      "Average loss at step 2896000: 334.734091\n",
      "Average loss at step 2896100: 332.154369\n",
      "Average loss at step 2896200: 333.746287\n",
      "Average loss at step 2896300: 330.832657\n",
      "Average loss at step 2896400: 337.054151\n",
      "Average loss at step 2896500: 334.184260\n",
      "Average loss at step 2896600: 334.385824\n",
      "Average loss at step 2896700: 333.451162\n",
      "Average loss at step 2896800: 335.881729\n",
      "Average loss at step 2896900: 334.114026\n",
      "Average loss at step 2897000: 332.874043\n",
      "Average loss at step 2897100: 334.896197\n",
      "Average loss at step 2982500: 329.393710\n",
      "Average loss at step 2982600: 328.170105\n",
      "Average loss at step 2982700: 331.240117\n",
      "Average loss at step 2982800: 328.748414\n",
      "Average loss at step 2982900: 328.697136\n",
      "Average loss at step 2983000: 327.374788\n",
      "Average loss at step 2983100: 325.900223\n",
      "Average loss at step 2983200: 331.330063\n",
      "Average loss at step 2983300: 330.169964\n",
      "Average loss at step 2983400: 329.682170\n",
      "Average loss at step 2983500: 333.252227\n",
      "Average loss at step 2983600: 332.433298\n",
      "Average loss at step 2983700: 333.595445\n",
      "Average loss at step 2983800: 332.799929\n",
      "Average loss at step 2983900: 330.369684\n",
      "Average loss at step 2984000: 332.889948\n",
      "Average loss at step 2984100: 330.909081\n",
      "Average loss at step 2984200: 329.161115\n",
      "Average loss at step 2984300: 328.207867\n",
      "Average loss at step 2984400: 328.424868\n",
      "Average loss at step 2984500: 327.576539\n",
      "Average loss at step 2984600: 330.548472\n",
      "Average loss at step 2984700: 326.480421\n",
      "Average loss at step 2984800: 331.383651\n",
      "Average loss at step 2984900: 329.190643\n",
      "Average loss at step 2985000: 331.211220\n",
      "Graph 597: 21 nodes\n",
      "Average loss at step 2985100: 337.872918\n",
      "Average loss at step 2985200: 330.527158\n",
      "Average loss at step 2985300: 333.384052\n",
      "Average loss at step 2985400: 328.992234\n",
      "Average loss at step 2985500: 327.316120\n",
      "Average loss at step 2985600: 324.805322\n",
      "Average loss at step 2985700: 327.108927\n",
      "Average loss at step 2985800: 324.427342\n",
      "Average loss at step 2985900: 328.136994\n",
      "Average loss at step 2986000: 327.329451\n",
      "Average loss at step 2986100: 328.053773\n",
      "Average loss at step 2986200: 326.689038\n",
      "Average loss at step 2986300: 326.449436\n",
      "Average loss at step 2986400: 326.326665\n",
      "Average loss at step 2986500: 327.475585\n",
      "Average loss at step 2986600: 327.607401\n",
      "Average loss at step 2986700: 323.056789\n",
      "Average loss at step 2986800: 323.612983\n",
      "Average loss at step 2986900: 326.307710\n",
      "Average loss at step 2987000: 324.922560\n",
      "Average loss at step 2987100: 327.197032\n",
      "Average loss at step 2987200: 322.759021\n",
      "Average loss at step 2987300: 323.278032\n",
      "Average loss at step 2987400: 324.137848\n",
      "Average loss at step 2987500: 324.807277\n",
      "Average loss at step 2987600: 330.686957\n",
      "Average loss at step 2987700: 325.302661\n",
      "Average loss at step 2987800: 329.025050\n",
      "Average loss at step 2987900: 324.735790\n",
      "Average loss at step 2988000: 325.172491\n",
      "Average loss at step 2988100: 326.076542\n",
      "Average loss at step 2988200: 326.686590\n",
      "Average loss at step 2988300: 327.525091\n",
      "Average loss at step 2988400: 326.044310\n",
      "Average loss at step 2988500: 326.609883\n",
      "Average loss at step 2988600: 328.736041\n",
      "Average loss at step 2988700: 325.687009\n",
      "Average loss at step 2988800: 327.916980\n",
      "Average loss at step 2988900: 326.823034\n",
      "Average loss at step 2989000: 322.761476\n",
      "Average loss at step 2989100: 323.970062\n",
      "Average loss at step 2989200: 326.506652\n",
      "Average loss at step 2989300: 325.741720\n",
      "Average loss at step 2989400: 326.020324\n",
      "Average loss at step 2989500: 326.130079\n",
      "Average loss at step 2989600: 324.019557\n",
      "Average loss at step 2989700: 325.557844\n",
      "Average loss at step 2989800: 323.800811\n",
      "Average loss at step 2989900: 325.707570\n",
      "Average loss at step 2990000: 327.762928\n",
      "Graph 598: 17 nodes\n",
      "Average loss at step 2990100: 333.280562\n",
      "Average loss at step 2990200: 329.648094\n",
      "Average loss at step 2990300: 330.380930\n",
      "Average loss at step 2990400: 327.892325\n",
      "Average loss at step 2990500: 330.203101\n",
      "Average loss at step 2990600: 329.026150\n",
      "Average loss at step 2990700: 327.241867\n",
      "Average loss at step 2990800: 329.138062\n",
      "Average loss at step 2990900: 328.302270\n",
      "Average loss at step 2991000: 329.365359\n",
      "Average loss at step 2991100: 326.917051\n",
      "Average loss at step 2991200: 328.347862\n",
      "Average loss at step 2991300: 328.983980\n",
      "Average loss at step 2991400: 332.243255\n",
      "Average loss at step 2991500: 328.458765\n",
      "Average loss at step 2991600: 326.024874\n",
      "Average loss at step 2991700: 328.360047\n",
      "Average loss at step 2991800: 327.083694\n",
      "Average loss at step 2991900: 327.355631\n",
      "Average loss at step 2992000: 328.146430\n",
      "Average loss at step 2992100: 323.711120\n",
      "Average loss at step 2992200: 328.635089\n",
      "Average loss at step 2992300: 326.289449\n",
      "Average loss at step 2992400: 330.311003\n",
      "Average loss at step 2992500: 328.567414\n",
      "Average loss at step 2992600: 324.403869\n",
      "Average loss at step 2992700: 328.137301\n",
      "Average loss at step 2992800: 328.440184\n",
      "Average loss at step 2992900: 326.986958\n",
      "Average loss at step 2993000: 328.357464\n",
      "Average loss at step 2993100: 328.753495\n",
      "Average loss at step 2993200: 329.860033\n",
      "Average loss at step 2993300: 327.068781\n",
      "Average loss at step 2993400: 328.149028\n",
      "Average loss at step 2993500: 330.778347\n",
      "Average loss at step 2993600: 327.137754\n",
      "Average loss at step 2993700: 330.201053\n",
      "Average loss at step 2993800: 330.507425\n",
      "Average loss at step 2993900: 332.414566\n",
      "Average loss at step 2994000: 329.834282\n",
      "Average loss at step 2994100: 330.939668\n",
      "Average loss at step 2994200: 330.070727\n",
      "Average loss at step 2994300: 327.154654\n",
      "Average loss at step 2994400: 329.521238\n",
      "Average loss at step 2994500: 323.575444\n",
      "Average loss at step 2994600: 330.145811\n",
      "Average loss at step 2994700: 325.538041\n",
      "Average loss at step 2994800: 330.327432\n",
      "Average loss at step 2994900: 327.103142\n",
      "Average loss at step 2995000: 328.804789\n",
      "Graph 599: 11 nodes\n",
      "Average loss at step 2995100: 326.729762\n",
      "Average loss at step 2995200: 322.762823\n",
      "Average loss at step 2995300: 325.621398\n",
      "Average loss at step 2995400: 319.555131\n",
      "Average loss at step 2995500: 318.361777\n",
      "Average loss at step 2995600: 317.736051\n",
      "Average loss at step 2995700: 316.765553\n",
      "Average loss at step 2995800: 317.336866\n",
      "Average loss at step 2995900: 322.784138\n",
      "Average loss at step 2996000: 318.605291\n",
      "Average loss at step 2996100: 319.781620\n",
      "Average loss at step 2996200: 317.305396\n",
      "Average loss at step 2996300: 318.443737\n",
      "Average loss at step 2996400: 318.031710\n",
      "Average loss at step 2996500: 311.641990\n",
      "Average loss at step 2996600: 317.314435\n",
      "Average loss at step 2996700: 312.024189\n",
      "Average loss at step 2996800: 317.595448\n",
      "Average loss at step 2996900: 317.489087\n",
      "Average loss at step 2997000: 315.053754\n",
      "Average loss at step 2997100: 316.537799\n",
      "Average loss at step 2997200: 310.396337\n",
      "Average loss at step 2997300: 309.538436\n",
      "Average loss at step 2997400: 313.749517\n",
      "Average loss at step 2997500: 313.367160\n",
      "Average loss at step 2997600: 315.273885\n",
      "Average loss at step 2997700: 315.794068\n",
      "Average loss at step 2997800: 314.274921\n",
      "Average loss at step 3089600: 328.916366\n",
      "Average loss at step 3089700: 333.598147\n",
      "Average loss at step 3089800: 330.177993\n",
      "Average loss at step 3089900: 332.695913\n",
      "Average loss at step 3090000: 334.628984\n",
      "Graph 618: 21 nodes\n",
      "Average loss at step 3090100: 334.556602\n",
      "Average loss at step 3090200: 327.826731\n",
      "Average loss at step 3090300: 327.604504\n",
      "Average loss at step 3090400: 327.854123\n",
      "Average loss at step 3090500: 332.341805\n",
      "Average loss at step 3090600: 327.872411\n",
      "Average loss at step 3090700: 330.954046\n",
      "Average loss at step 3090800: 328.563065\n",
      "Average loss at step 3090900: 328.137747\n",
      "Average loss at step 3091000: 330.352499\n",
      "Average loss at step 3091100: 326.836912\n",
      "Average loss at step 3091200: 330.808766\n",
      "Average loss at step 3091300: 327.118751\n",
      "Average loss at step 3091400: 327.844032\n",
      "Average loss at step 3091500: 328.246229\n",
      "Average loss at step 3091600: 328.766734\n",
      "Average loss at step 3091700: 322.663199\n",
      "Average loss at step 3091800: 330.118603\n",
      "Average loss at step 3091900: 325.961286\n",
      "Average loss at step 3092000: 331.392151\n",
      "Average loss at step 3092100: 329.060536\n",
      "Average loss at step 3092200: 329.405715\n",
      "Average loss at step 3092300: 330.348276\n",
      "Average loss at step 3092400: 327.438884\n",
      "Average loss at step 3092500: 325.754053\n",
      "Average loss at step 3092600: 327.636803\n",
      "Average loss at step 3092700: 328.743798\n",
      "Average loss at step 3092800: 326.317142\n",
      "Average loss at step 3092900: 328.605792\n",
      "Average loss at step 3093000: 327.647856\n",
      "Average loss at step 3093100: 321.560034\n",
      "Average loss at step 3093200: 329.660922\n",
      "Average loss at step 3093300: 325.149318\n",
      "Average loss at step 3093400: 326.955620\n",
      "Average loss at step 3093500: 323.704643\n",
      "Average loss at step 3093600: 328.980108\n",
      "Average loss at step 3093700: 327.039489\n",
      "Average loss at step 3093800: 327.836468\n",
      "Average loss at step 3093900: 329.753123\n",
      "Average loss at step 3094000: 329.816639\n",
      "Average loss at step 3094100: 324.944002\n",
      "Average loss at step 3094200: 323.467307\n",
      "Average loss at step 3094300: 328.819328\n",
      "Average loss at step 3094400: 331.652100\n",
      "Average loss at step 3094500: 327.667138\n",
      "Average loss at step 3094600: 326.272645\n",
      "Average loss at step 3094700: 331.100444\n",
      "Average loss at step 3094800: 326.143001\n",
      "Average loss at step 3094900: 326.233058\n",
      "Average loss at step 3095000: 324.631660\n",
      "Graph 619: 29 nodes\n",
      "Average loss at step 3095100: 335.059014\n",
      "Average loss at step 3095200: 334.536776\n",
      "Average loss at step 3095300: 334.523822\n",
      "Average loss at step 3095400: 333.998574\n",
      "Average loss at step 3095500: 331.927479\n",
      "Average loss at step 3095600: 326.096615\n",
      "Average loss at step 3095700: 329.176129\n",
      "Average loss at step 3095800: 330.041577\n",
      "Average loss at step 3095900: 333.438428\n",
      "Average loss at step 3096000: 332.160058\n",
      "Average loss at step 3096100: 328.878922\n",
      "Average loss at step 3096200: 328.895882\n",
      "Average loss at step 3096300: 329.151244\n",
      "Average loss at step 3096400: 331.146346\n",
      "Average loss at step 3096500: 329.746891\n",
      "Average loss at step 3096600: 328.963368\n",
      "Average loss at step 3096700: 328.124985\n",
      "Average loss at step 3096800: 325.983234\n",
      "Average loss at step 3096900: 329.156208\n",
      "Average loss at step 3097000: 331.581482\n",
      "Average loss at step 3097100: 330.531452\n",
      "Average loss at step 3097200: 326.730597\n",
      "Average loss at step 3097300: 329.063648\n",
      "Average loss at step 3097400: 331.168504\n",
      "Average loss at step 3097500: 331.808197\n",
      "Average loss at step 3097600: 330.792546\n",
      "Average loss at step 3097700: 331.787649\n",
      "Average loss at step 3097800: 327.540453\n",
      "Average loss at step 3097900: 328.999615\n",
      "Average loss at step 3098000: 330.538548\n",
      "Average loss at step 3098100: 331.223731\n",
      "Average loss at step 3098200: 328.008153\n",
      "Average loss at step 3098300: 328.003640\n",
      "Average loss at step 3098400: 332.066817\n",
      "Average loss at step 3098500: 327.912986\n",
      "Average loss at step 3098600: 326.627120\n",
      "Average loss at step 3098700: 331.965549\n",
      "Average loss at step 3098800: 330.640793\n",
      "Average loss at step 3098900: 329.328963\n",
      "Average loss at step 3099000: 333.130144\n",
      "Average loss at step 3099100: 332.759042\n",
      "Average loss at step 3099200: 327.859564\n",
      "Average loss at step 3099300: 332.066769\n",
      "Average loss at step 3099400: 332.318123\n",
      "Average loss at step 3099500: 329.318329\n",
      "Average loss at step 3099600: 332.379734\n",
      "Average loss at step 3099700: 330.846991\n",
      "Average loss at step 3099800: 330.224915\n",
      "Average loss at step 3099900: 329.986039\n",
      "Average loss at step 3100000: 328.216916\n",
      "Graph 620: 19 nodes\n",
      "Average loss at step 3100100: 334.225364\n",
      "Average loss at step 3100200: 329.419163\n",
      "Average loss at step 3100300: 331.082395\n",
      "Average loss at step 3100400: 326.829158\n",
      "Average loss at step 3100500: 328.304091\n",
      "Average loss at step 3100600: 331.857197\n",
      "Average loss at step 3100700: 330.860396\n",
      "Average loss at step 3100800: 328.819176\n",
      "Average loss at step 3100900: 331.334891\n",
      "Average loss at step 3101000: 330.096833\n",
      "Average loss at step 3101100: 328.236415\n",
      "Average loss at step 3101200: 329.687117\n",
      "Average loss at step 3101300: 330.771878\n",
      "Average loss at step 3101400: 329.047660\n",
      "Average loss at step 3101500: 330.634750\n",
      "Average loss at step 3101600: 330.292628\n",
      "Average loss at step 3101700: 331.961384\n",
      "Average loss at step 3101800: 329.125144\n",
      "Average loss at step 3101900: 327.322408\n",
      "Average loss at step 3102000: 329.869031\n",
      "Average loss at step 3102100: 329.305285\n",
      "Average loss at step 3102200: 328.109197\n",
      "Average loss at step 3102300: 324.485129\n",
      "Average loss at step 3102400: 328.936038\n",
      "Average loss at step 3102500: 328.623859\n",
      "Average loss at step 3102600: 327.506656\n",
      "Average loss at step 3102700: 328.750024\n",
      "Average loss at step 3102800: 329.770539\n",
      "Average loss at step 3102900: 329.339960\n",
      "Average loss at step 3103000: 330.180805\n",
      "Average loss at step 3103100: 327.609895\n",
      "Average loss at step 3103200: 329.177554\n",
      "Average loss at step 3103300: 329.887485\n",
      "Average loss at step 3103400: 327.045810\n",
      "Average loss at step 3103500: 330.535351\n",
      "Average loss at step 3103600: 330.451316\n",
      "Average loss at step 3103700: 329.658279\n",
      "Average loss at step 3103800: 328.076537\n",
      "Average loss at step 3103900: 328.544407\n",
      "Average loss at step 3104000: 327.981063\n",
      "Average loss at step 3104100: 327.667659\n",
      "Average loss at step 3104200: 330.227627\n",
      "Average loss at step 3104300: 325.885560\n",
      "Average loss at step 3104400: 329.452550\n",
      "Average loss at step 3104500: 324.849684\n",
      "Average loss at step 3104600: 330.952337\n",
      "Average loss at step 3104700: 327.083363\n",
      "Average loss at step 3104800: 330.882284\n",
      "Average loss at step 3104900: 329.444537\n",
      "Average loss at step 3196700: 352.687935\n",
      "Average loss at step 3196800: 352.585789\n",
      "Average loss at step 3196900: 353.204938\n",
      "Average loss at step 3197000: 348.205173\n",
      "Average loss at step 3197100: 357.521605\n",
      "Average loss at step 3197200: 352.359021\n",
      "Average loss at step 3197300: 352.654552\n",
      "Average loss at step 3197400: 356.288464\n",
      "Average loss at step 3197500: 354.167058\n",
      "Average loss at step 3197600: 348.997272\n",
      "Average loss at step 3197700: 351.348993\n",
      "Average loss at step 3197800: 350.099377\n",
      "Average loss at step 3197900: 352.354868\n",
      "Average loss at step 3198000: 356.146434\n",
      "Average loss at step 3198100: 355.136836\n",
      "Average loss at step 3198200: 348.883578\n",
      "Average loss at step 3198300: 349.217521\n",
      "Average loss at step 3198400: 353.042466\n",
      "Average loss at step 3198500: 351.952006\n",
      "Average loss at step 3198600: 350.496202\n",
      "Average loss at step 3198700: 357.611210\n",
      "Average loss at step 3198800: 352.372872\n",
      "Average loss at step 3198900: 356.032375\n",
      "Average loss at step 3199000: 351.877522\n",
      "Average loss at step 3199100: 351.917489\n",
      "Average loss at step 3199200: 353.488911\n",
      "Average loss at step 3199300: 354.470976\n",
      "Average loss at step 3199400: 346.347617\n",
      "Average loss at step 3199500: 347.633497\n",
      "Average loss at step 3199600: 349.712732\n",
      "Average loss at step 3199700: 351.552384\n",
      "Average loss at step 3199800: 350.790329\n",
      "Average loss at step 3199900: 353.036568\n",
      "Average loss at step 3200000: 350.248352\n",
      "Graph 640: 28 nodes\n",
      "Average loss at step 3200100: 342.104002\n",
      "Average loss at step 3200200: 339.393169\n",
      "Average loss at step 3200300: 336.836001\n",
      "Average loss at step 3200400: 337.439859\n",
      "Average loss at step 3200500: 338.085227\n",
      "Average loss at step 3200600: 337.180648\n",
      "Average loss at step 3200700: 334.693412\n",
      "Average loss at step 3200800: 333.898205\n",
      "Average loss at step 3200900: 335.836769\n",
      "Average loss at step 3201000: 333.790324\n",
      "Average loss at step 3201100: 338.459836\n",
      "Average loss at step 3201200: 334.570251\n",
      "Average loss at step 3201300: 335.331382\n",
      "Average loss at step 3201400: 338.068241\n",
      "Average loss at step 3201500: 335.030963\n",
      "Average loss at step 3201600: 335.532466\n",
      "Average loss at step 3201700: 338.794316\n",
      "Average loss at step 3201800: 330.774941\n",
      "Average loss at step 3201900: 333.920357\n",
      "Average loss at step 3202000: 337.189736\n",
      "Average loss at step 3202100: 337.884458\n",
      "Average loss at step 3202200: 333.609869\n",
      "Average loss at step 3202300: 334.618374\n",
      "Average loss at step 3202400: 334.735581\n",
      "Average loss at step 3202500: 332.486005\n",
      "Average loss at step 3202600: 334.993884\n",
      "Average loss at step 3202700: 336.805994\n",
      "Average loss at step 3202800: 335.473022\n",
      "Average loss at step 3202900: 330.439191\n",
      "Average loss at step 3203000: 332.390590\n",
      "Average loss at step 3203100: 334.759500\n",
      "Average loss at step 3203200: 334.628866\n",
      "Average loss at step 3203300: 337.704504\n",
      "Average loss at step 3203400: 334.810210\n",
      "Average loss at step 3203500: 337.832623\n",
      "Average loss at step 3203600: 336.287959\n",
      "Average loss at step 3203700: 336.655274\n",
      "Average loss at step 3203800: 335.514980\n",
      "Average loss at step 3203900: 334.727531\n",
      "Average loss at step 3204000: 338.133344\n",
      "Average loss at step 3204100: 335.700200\n",
      "Average loss at step 3204200: 334.133867\n",
      "Average loss at step 3204300: 330.607615\n",
      "Average loss at step 3204400: 331.379003\n",
      "Average loss at step 3204500: 332.658603\n",
      "Average loss at step 3204600: 337.334845\n",
      "Average loss at step 3204700: 333.859094\n",
      "Average loss at step 3204800: 332.338056\n",
      "Average loss at step 3204900: 332.775302\n",
      "Average loss at step 3205000: 330.314053\n",
      "Time: 23.6129479408\n",
      "Graph 641: 15 nodes\n",
      "Average loss at step 3205100: 326.901724\n",
      "Average loss at step 3205200: 321.659678\n",
      "Average loss at step 3205300: 324.689099\n",
      "Average loss at step 3205400: 319.159290\n",
      "Average loss at step 3205500: 318.167450\n",
      "Average loss at step 3205600: 320.643883\n",
      "Average loss at step 3205700: 320.646572\n",
      "Average loss at step 3205800: 319.240080\n",
      "Average loss at step 3205900: 320.721193\n",
      "Average loss at step 3206000: 316.229228\n",
      "Average loss at step 3206100: 320.651037\n",
      "Average loss at step 3206200: 319.393321\n",
      "Average loss at step 3206300: 319.423094\n",
      "Average loss at step 3206400: 316.346807\n",
      "Average loss at step 3206500: 320.152212\n",
      "Average loss at step 3206600: 322.094840\n",
      "Average loss at step 3206700: 319.621881\n",
      "Average loss at step 3206800: 315.259950\n",
      "Average loss at step 3206900: 315.448051\n",
      "Average loss at step 3207000: 321.015902\n",
      "Average loss at step 3207100: 318.432050\n",
      "Average loss at step 3207200: 315.643504\n",
      "Average loss at step 3207300: 315.793181\n",
      "Average loss at step 3207400: 316.920161\n",
      "Average loss at step 3207500: 316.959920\n",
      "Average loss at step 3207600: 314.937204\n",
      "Average loss at step 3207700: 314.081275\n",
      "Average loss at step 3207800: 310.495926\n",
      "Average loss at step 3207900: 313.942685\n",
      "Average loss at step 3208000: 317.390636\n",
      "Average loss at step 3208100: 314.558513\n",
      "Average loss at step 3208200: 312.774792\n",
      "Average loss at step 3208300: 315.548771\n",
      "Average loss at step 3208400: 314.711067\n",
      "Average loss at step 3208500: 314.947926\n",
      "Average loss at step 3208600: 317.579734\n",
      "Average loss at step 3208700: 314.686593\n",
      "Average loss at step 3208800: 314.993675\n",
      "Average loss at step 3208900: 315.929585\n",
      "Average loss at step 3209000: 317.423400\n",
      "Average loss at step 3209100: 315.563653\n",
      "Average loss at step 3209200: 312.191988\n",
      "Average loss at step 3209300: 314.771950\n",
      "Average loss at step 3209400: 312.717274\n",
      "Average loss at step 3209500: 317.510951\n",
      "Average loss at step 3209600: 318.452668\n",
      "Average loss at step 3209700: 315.280396\n",
      "Average loss at step 3209800: 316.016082\n",
      "Average loss at step 3209900: 313.877215\n",
      "Average loss at step 3210000: 315.559537\n",
      "Graph 642: 12 nodes\n",
      "Average loss at step 3210100: 323.050277\n",
      "Average loss at step 3210200: 322.958474\n",
      "Average loss at step 3210300: 319.267268\n",
      "Average loss at step 3210400: 317.749610\n",
      "Average loss at step 3210500: 317.397526\n",
      "Average loss at step 3210600: 321.768424\n",
      "Average loss at step 3210700: 316.178397\n",
      "Average loss at step 3210800: 319.818839\n",
      "Average loss at step 3210900: 319.403718\n",
      "Average loss at step 3211000: 316.140563\n",
      "Average loss at step 3211100: 314.499215\n",
      "Average loss at step 3211200: 315.128984\n",
      "Average loss at step 3211300: 318.284619\n",
      "Average loss at step 3211400: 318.988381\n",
      "Average loss at step 3211500: 314.823012\n",
      "Average loss at step 3211600: 316.082017\n",
      "Average loss at step 3211700: 315.612951\n",
      "Average loss at step 3211800: 317.244897\n",
      "Average loss at step 3211900: 317.273674\n",
      "Average loss at step 3212000: 315.162064\n",
      "Average loss at step 3304200: 327.560884\n",
      "Average loss at step 3304300: 329.321156\n",
      "Average loss at step 3304400: 331.207845\n",
      "Average loss at step 3304500: 330.075704\n",
      "Average loss at step 3304600: 332.451292\n",
      "Average loss at step 3304700: 327.800867\n",
      "Average loss at step 3304800: 326.709726\n",
      "Average loss at step 3304900: 327.548763\n",
      "Average loss at step 3305000: 327.655293\n",
      "Time: 24.0905790329\n",
      "Graph 661: 13 nodes\n",
      "Average loss at step 3305100: 340.806663\n",
      "Average loss at step 3305200: 333.689939\n",
      "Average loss at step 3305300: 331.460773\n",
      "Average loss at step 3305400: 331.947054\n",
      "Average loss at step 3305500: 333.503943\n",
      "Average loss at step 3305600: 336.246456\n",
      "Average loss at step 3305700: 328.617003\n",
      "Average loss at step 3305800: 331.307537\n",
      "Average loss at step 3305900: 328.681506\n",
      "Average loss at step 3306000: 331.968462\n",
      "Average loss at step 3306100: 333.187359\n",
      "Average loss at step 3306200: 331.166665\n",
      "Average loss at step 3306300: 326.939018\n",
      "Average loss at step 3306400: 330.089201\n",
      "Average loss at step 3306500: 327.145823\n",
      "Average loss at step 3306600: 329.603764\n",
      "Average loss at step 3306700: 326.478305\n",
      "Average loss at step 3306800: 329.716459\n",
      "Average loss at step 3306900: 328.770640\n",
      "Average loss at step 3307000: 331.049951\n",
      "Average loss at step 3307100: 329.361298\n",
      "Average loss at step 3307200: 328.696655\n",
      "Average loss at step 3307300: 330.260792\n",
      "Average loss at step 3307400: 330.548221\n",
      "Average loss at step 3307500: 330.355086\n",
      "Average loss at step 3307600: 328.022050\n",
      "Average loss at step 3307700: 332.017398\n",
      "Average loss at step 3307800: 330.626599\n",
      "Average loss at step 3307900: 331.972778\n",
      "Average loss at step 3308000: 329.973827\n",
      "Average loss at step 3308100: 328.411109\n",
      "Average loss at step 3308200: 330.826099\n",
      "Average loss at step 3308300: 328.956880\n",
      "Average loss at step 3308400: 328.373221\n",
      "Average loss at step 3308500: 327.173815\n",
      "Average loss at step 3308600: 332.209960\n",
      "Average loss at step 3308700: 324.610344\n",
      "Average loss at step 3308800: 329.631200\n",
      "Average loss at step 3308900: 330.611476\n",
      "Average loss at step 3309000: 328.875362\n",
      "Average loss at step 3309100: 326.523938\n",
      "Average loss at step 3309200: 331.679391\n",
      "Average loss at step 3309300: 327.347203\n",
      "Average loss at step 3309400: 325.707790\n",
      "Average loss at step 3309500: 326.663669\n",
      "Average loss at step 3309600: 325.058498\n",
      "Average loss at step 3309700: 326.535354\n",
      "Average loss at step 3309800: 331.964100\n",
      "Average loss at step 3309900: 327.760828\n",
      "Average loss at step 3310000: 324.536325\n",
      "Graph 662: 16 nodes\n",
      "Average loss at step 3310100: 331.690203\n",
      "Average loss at step 3310200: 332.995888\n",
      "Average loss at step 3310300: 329.860610\n",
      "Average loss at step 3310400: 333.262409\n",
      "Average loss at step 3310500: 333.647285\n",
      "Average loss at step 3310600: 330.623550\n",
      "Average loss at step 3310700: 329.430093\n",
      "Average loss at step 3310800: 328.271852\n",
      "Average loss at step 3310900: 333.867623\n",
      "Average loss at step 3311000: 335.627793\n",
      "Average loss at step 3311100: 331.286013\n",
      "Average loss at step 3311200: 330.244090\n",
      "Average loss at step 3311300: 331.447757\n",
      "Average loss at step 3311400: 327.676542\n",
      "Average loss at step 3311500: 331.400242\n",
      "Average loss at step 3311600: 333.575783\n",
      "Average loss at step 3311700: 329.150019\n",
      "Average loss at step 3311800: 325.408602\n",
      "Average loss at step 3311900: 329.674828\n",
      "Average loss at step 3312000: 327.958562\n",
      "Average loss at step 3312100: 328.875222\n",
      "Average loss at step 3312200: 331.810382\n",
      "Average loss at step 3312300: 327.592969\n",
      "Average loss at step 3312400: 324.137170\n",
      "Average loss at step 3312500: 329.740827\n",
      "Average loss at step 3312600: 333.844404\n",
      "Average loss at step 3312700: 330.091075\n",
      "Average loss at step 3312800: 331.089340\n",
      "Average loss at step 3312900: 327.740317\n",
      "Average loss at step 3313000: 330.130831\n",
      "Average loss at step 3313100: 328.904459\n",
      "Average loss at step 3313200: 328.024830\n",
      "Average loss at step 3313300: 331.532320\n",
      "Average loss at step 3313400: 326.111158\n",
      "Average loss at step 3313500: 329.987017\n",
      "Average loss at step 3313600: 324.060568\n",
      "Average loss at step 3313700: 328.039703\n",
      "Average loss at step 3313800: 330.312640\n",
      "Average loss at step 3313900: 334.756132\n",
      "Average loss at step 3314000: 327.231663\n",
      "Average loss at step 3314100: 325.803388\n",
      "Average loss at step 3314200: 325.475524\n",
      "Average loss at step 3314300: 329.405338\n",
      "Average loss at step 3314400: 327.980466\n",
      "Average loss at step 3314500: 326.980645\n",
      "Average loss at step 3314600: 327.713910\n",
      "Average loss at step 3314700: 327.688554\n",
      "Average loss at step 3314800: 331.297642\n",
      "Average loss at step 3314900: 323.320633\n",
      "Average loss at step 3315000: 329.102980\n",
      "Graph 663: 10 nodes\n",
      "Average loss at step 3315100: 334.034758\n",
      "Average loss at step 3315200: 329.511641\n",
      "Average loss at step 3315300: 322.958856\n",
      "Average loss at step 3315400: 325.241457\n",
      "Average loss at step 3315500: 325.469152\n",
      "Average loss at step 3315600: 324.183160\n",
      "Average loss at step 3315700: 325.869185\n",
      "Average loss at step 3315800: 320.880309\n",
      "Average loss at step 3315900: 321.604655\n",
      "Average loss at step 3316000: 325.167152\n",
      "Average loss at step 3316100: 322.069169\n",
      "Average loss at step 3316200: 324.098725\n",
      "Average loss at step 3316300: 325.729092\n",
      "Average loss at step 3316400: 323.839894\n",
      "Average loss at step 3316500: 323.257921\n",
      "Average loss at step 3316600: 321.715096\n",
      "Average loss at step 3316700: 322.349677\n",
      "Average loss at step 3316800: 322.629906\n",
      "Average loss at step 3316900: 325.493237\n",
      "Average loss at step 3317000: 327.330247\n",
      "Average loss at step 3317100: 321.761018\n",
      "Average loss at step 3317200: 321.329282\n",
      "Average loss at step 3317300: 323.541964\n",
      "Average loss at step 3317400: 327.132215\n",
      "Average loss at step 3317500: 323.349556\n",
      "Average loss at step 3317600: 323.615423\n",
      "Average loss at step 3317700: 320.275869\n",
      "Average loss at step 3317800: 325.896899\n",
      "Average loss at step 3317900: 322.213507\n",
      "Average loss at step 3318000: 324.889473\n",
      "Average loss at step 3318100: 323.933097\n",
      "Average loss at step 3318200: 321.152184\n",
      "Average loss at step 3318300: 322.424658\n",
      "Average loss at step 3318400: 325.005403\n",
      "Average loss at step 3318500: 319.439836\n",
      "Average loss at step 3318600: 323.673745\n",
      "Average loss at step 3318700: 320.233771\n",
      "Average loss at step 3318800: 319.699956\n",
      "Average loss at step 3318900: 324.476776\n",
      "Average loss at step 3319000: 322.581633\n",
      "Average loss at step 3319100: 321.900028\n",
      "Average loss at step 3319200: 324.303430\n",
      "Average loss at step 3319300: 323.425591\n",
      "Average loss at step 3319400: 321.378689\n",
      "Average loss at step 3319500: 321.660600\n",
      "Average loss at step 3412400: 326.626549\n",
      "Average loss at step 3412500: 325.264797\n",
      "Average loss at step 3412600: 321.445688\n",
      "Average loss at step 3412700: 323.587534\n",
      "Average loss at step 3412800: 324.109063\n",
      "Average loss at step 3412900: 325.507316\n",
      "Average loss at step 3413000: 326.931409\n",
      "Average loss at step 3413100: 326.660763\n",
      "Average loss at step 3413200: 324.424385\n",
      "Average loss at step 3413300: 327.448471\n",
      "Average loss at step 3413400: 325.439008\n",
      "Average loss at step 3413500: 325.913409\n",
      "Average loss at step 3413600: 326.353534\n",
      "Average loss at step 3413700: 323.917414\n",
      "Average loss at step 3413800: 323.818126\n",
      "Average loss at step 3413900: 324.976588\n",
      "Average loss at step 3414000: 327.585618\n",
      "Average loss at step 3414100: 326.273414\n",
      "Average loss at step 3414200: 323.332987\n",
      "Average loss at step 3414300: 324.184812\n",
      "Average loss at step 3414400: 325.643910\n",
      "Average loss at step 3414500: 325.997577\n",
      "Average loss at step 3414600: 323.696981\n",
      "Average loss at step 3414700: 326.886740\n",
      "Average loss at step 3414800: 324.297483\n",
      "Average loss at step 3414900: 324.678551\n",
      "Average loss at step 3415000: 324.318908\n",
      "Graph 683: 19 nodes\n",
      "Average loss at step 3415100: 336.086204\n",
      "Average loss at step 3415200: 329.079887\n",
      "Average loss at step 3415300: 333.200948\n",
      "Average loss at step 3415400: 329.625380\n",
      "Average loss at step 3415500: 329.930455\n",
      "Average loss at step 3415600: 325.928669\n",
      "Average loss at step 3415700: 331.673618\n",
      "Average loss at step 3415800: 327.089493\n",
      "Average loss at step 3415900: 331.608903\n",
      "Average loss at step 3416000: 325.585769\n",
      "Average loss at step 3416100: 331.483308\n",
      "Average loss at step 3416200: 326.450647\n",
      "Average loss at step 3416300: 324.435117\n",
      "Average loss at step 3416400: 329.344949\n",
      "Average loss at step 3416500: 330.009831\n",
      "Average loss at step 3416600: 330.725821\n",
      "Average loss at step 3416700: 330.154559\n",
      "Average loss at step 3416800: 328.873309\n",
      "Average loss at step 3416900: 330.456388\n",
      "Average loss at step 3417000: 325.741208\n",
      "Average loss at step 3417100: 329.886575\n",
      "Average loss at step 3417200: 327.392040\n",
      "Average loss at step 3417300: 327.033054\n",
      "Average loss at step 3417400: 323.751092\n",
      "Average loss at step 3417500: 328.665132\n",
      "Average loss at step 3417600: 329.176536\n",
      "Average loss at step 3417700: 329.720002\n",
      "Average loss at step 3417800: 328.071458\n",
      "Average loss at step 3417900: 327.788953\n",
      "Average loss at step 3418000: 326.568289\n",
      "Average loss at step 3418100: 327.141795\n",
      "Average loss at step 3418200: 327.296883\n",
      "Average loss at step 3418300: 326.544433\n",
      "Average loss at step 3418400: 325.712240\n",
      "Average loss at step 3418500: 326.463954\n",
      "Average loss at step 3418600: 329.856534\n",
      "Average loss at step 3418700: 332.274004\n",
      "Average loss at step 3418800: 327.043557\n",
      "Average loss at step 3418900: 327.881719\n",
      "Average loss at step 3419000: 327.408348\n",
      "Average loss at step 3419100: 327.072187\n",
      "Average loss at step 3419200: 328.682933\n",
      "Average loss at step 3419300: 327.670921\n",
      "Average loss at step 3419400: 327.664007\n",
      "Average loss at step 3419500: 328.872038\n",
      "Average loss at step 3419600: 323.959386\n",
      "Average loss at step 3419700: 328.703429\n",
      "Average loss at step 3419800: 332.082287\n",
      "Average loss at step 3419900: 330.566629\n",
      "Average loss at step 3420000: 326.298865\n",
      "Graph 684: 18 nodes\n",
      "Average loss at step 3420100: 331.611178\n",
      "Average loss at step 3420200: 331.615454\n",
      "Average loss at step 3420300: 332.260864\n",
      "Average loss at step 3420400: 331.539554\n",
      "Average loss at step 3420500: 330.950683\n",
      "Average loss at step 3420600: 328.890494\n",
      "Average loss at step 3420700: 333.159085\n",
      "Average loss at step 3420800: 332.025173\n",
      "Average loss at step 3420900: 331.698838\n",
      "Average loss at step 3421000: 328.204864\n",
      "Average loss at step 3421100: 330.284398\n",
      "Average loss at step 3421200: 330.634192\n",
      "Average loss at step 3421300: 327.992006\n",
      "Average loss at step 3421400: 329.799014\n",
      "Average loss at step 3421500: 329.772299\n",
      "Average loss at step 3421600: 330.332289\n",
      "Average loss at step 3421700: 329.284695\n",
      "Average loss at step 3421800: 332.217725\n",
      "Average loss at step 3421900: 332.283729\n",
      "Average loss at step 3422000: 331.233931\n",
      "Average loss at step 3422100: 326.247328\n",
      "Average loss at step 3422200: 333.009317\n",
      "Average loss at step 3422300: 328.067089\n",
      "Average loss at step 3422400: 330.720508\n",
      "Average loss at step 3422500: 332.844921\n",
      "Average loss at step 3422600: 329.757278\n",
      "Average loss at step 3422700: 329.190952\n",
      "Average loss at step 3422800: 328.544361\n",
      "Average loss at step 3422900: 329.317484\n",
      "Average loss at step 3423000: 328.767453\n",
      "Average loss at step 3423100: 329.596167\n",
      "Average loss at step 3423200: 331.968748\n",
      "Average loss at step 3423300: 328.756509\n",
      "Average loss at step 3423400: 332.619000\n",
      "Average loss at step 3423500: 326.909250\n",
      "Average loss at step 3423600: 328.339123\n",
      "Average loss at step 3423700: 331.699209\n",
      "Average loss at step 3423800: 325.372076\n",
      "Average loss at step 3423900: 327.257312\n",
      "Average loss at step 3424000: 325.607621\n",
      "Average loss at step 3424100: 330.238784\n",
      "Average loss at step 3424200: 329.726863\n",
      "Average loss at step 3424300: 328.840297\n",
      "Average loss at step 3424400: 328.137401\n",
      "Average loss at step 3424500: 331.360825\n",
      "Average loss at step 3424600: 330.563663\n",
      "Average loss at step 3424700: 327.624382\n",
      "Average loss at step 3424800: 327.077347\n",
      "Average loss at step 3424900: 331.154906\n",
      "Average loss at step 3425000: 331.359476\n",
      "Graph 685: 12 nodes\n",
      "Average loss at step 3425100: 339.498917\n",
      "Average loss at step 3425200: 339.617480\n",
      "Average loss at step 3425300: 337.584691\n",
      "Average loss at step 3425400: 332.761124\n",
      "Average loss at step 3425500: 341.143701\n",
      "Average loss at step 3425600: 337.248850\n",
      "Average loss at step 3425700: 338.433835\n",
      "Average loss at step 3425800: 336.659382\n",
      "Average loss at step 3425900: 335.352829\n",
      "Average loss at step 3426000: 338.811685\n",
      "Average loss at step 3426100: 337.325615\n",
      "Average loss at step 3426200: 333.320950\n",
      "Average loss at step 3426300: 337.032703\n",
      "Average loss at step 3426400: 335.119969\n",
      "Average loss at step 3426500: 336.650029\n",
      "Average loss at step 3426600: 337.493558\n",
      "Average loss at step 3426700: 336.710031\n",
      "Average loss at step 3426800: 337.436904\n",
      "Average loss at step 3426900: 333.804077\n",
      "Average loss at step 3427000: 331.945714\n",
      "Average loss at step 3427100: 335.984393\n",
      "Average loss at step 3427200: 334.699531\n",
      "Average loss at step 3427300: 334.591393\n",
      "Average loss at step 3427400: 334.368798\n",
      "Average loss at step 3427500: 332.659263\n",
      "Average loss at step 3427600: 338.013060\n",
      "Average loss at step 3427700: 333.404683\n",
      "Average loss at step 3520900: 321.368229\n",
      "Average loss at step 3521000: 322.528873\n",
      "Average loss at step 3521100: 320.456535\n",
      "Average loss at step 3521200: 324.973413\n",
      "Average loss at step 3521300: 318.120837\n",
      "Average loss at step 3521400: 317.845575\n",
      "Average loss at step 3521500: 321.033814\n",
      "Average loss at step 3521600: 318.274330\n",
      "Average loss at step 3521700: 319.908005\n",
      "Average loss at step 3521800: 319.579195\n",
      "Average loss at step 3521900: 315.179961\n",
      "Average loss at step 3522000: 317.096400\n",
      "Average loss at step 3522100: 321.768238\n",
      "Average loss at step 3522200: 319.909522\n",
      "Average loss at step 3522300: 318.476547\n",
      "Average loss at step 3522400: 316.535280\n",
      "Average loss at step 3522500: 313.868085\n",
      "Average loss at step 3522600: 320.421251\n",
      "Average loss at step 3522700: 317.721427\n",
      "Average loss at step 3522800: 318.247044\n",
      "Average loss at step 3522900: 322.041947\n",
      "Average loss at step 3523000: 321.085542\n",
      "Average loss at step 3523100: 319.138182\n",
      "Average loss at step 3523200: 317.311189\n",
      "Average loss at step 3523300: 316.874493\n",
      "Average loss at step 3523400: 319.396974\n",
      "Average loss at step 3523500: 317.430684\n",
      "Average loss at step 3523600: 314.098614\n",
      "Average loss at step 3523700: 319.553008\n",
      "Average loss at step 3523800: 319.920781\n",
      "Average loss at step 3523900: 317.630254\n",
      "Average loss at step 3524000: 316.760592\n",
      "Average loss at step 3524100: 316.176547\n",
      "Average loss at step 3524200: 313.139503\n",
      "Average loss at step 3524300: 314.457702\n",
      "Average loss at step 3524400: 321.222671\n",
      "Average loss at step 3524500: 317.633344\n",
      "Average loss at step 3524600: 317.394305\n",
      "Average loss at step 3524700: 314.891429\n",
      "Average loss at step 3524800: 316.941241\n",
      "Average loss at step 3524900: 315.437524\n",
      "Average loss at step 3525000: 318.278077\n",
      "Graph 705: 4 nodes\n",
      "Average loss at step 3525100: 262.336273\n",
      "Average loss at step 3525200: 250.354015\n",
      "Average loss at step 3525300: 238.084697\n",
      "Average loss at step 3525400: 240.405132\n",
      "Average loss at step 3525500: 229.204396\n",
      "Average loss at step 3525600: 237.393888\n",
      "Average loss at step 3525700: 228.550200\n",
      "Average loss at step 3525800: 229.786822\n",
      "Average loss at step 3525900: 227.189878\n",
      "Average loss at step 3526000: 225.829185\n",
      "Average loss at step 3526100: 227.979404\n",
      "Average loss at step 3526200: 222.043705\n",
      "Average loss at step 3526300: 222.860728\n",
      "Average loss at step 3526400: 226.468583\n",
      "Average loss at step 3526500: 226.528984\n",
      "Average loss at step 3526600: 230.063014\n",
      "Average loss at step 3526700: 225.556228\n",
      "Average loss at step 3526800: 225.639769\n",
      "Average loss at step 3526900: 225.918853\n",
      "Average loss at step 3527000: 225.327734\n",
      "Average loss at step 3527100: 224.925406\n",
      "Average loss at step 3527200: 227.165469\n",
      "Average loss at step 3527300: 225.039135\n",
      "Average loss at step 3527400: 224.278582\n",
      "Average loss at step 3527500: 222.442029\n",
      "Average loss at step 3527600: 227.494530\n",
      "Average loss at step 3527700: 222.061614\n",
      "Average loss at step 3527800: 224.817863\n",
      "Average loss at step 3527900: 220.763543\n",
      "Average loss at step 3528000: 226.020967\n",
      "Average loss at step 3528100: 220.768830\n",
      "Average loss at step 3528200: 224.924614\n",
      "Average loss at step 3528300: 225.431928\n",
      "Average loss at step 3528400: 220.521201\n",
      "Average loss at step 3528500: 225.615504\n",
      "Average loss at step 3528600: 218.215137\n",
      "Average loss at step 3528700: 220.289987\n",
      "Average loss at step 3528800: 223.290353\n",
      "Average loss at step 3528900: 227.316494\n",
      "Average loss at step 3529000: 225.064237\n",
      "Average loss at step 3529100: 225.471622\n",
      "Average loss at step 3529200: 220.935125\n",
      "Average loss at step 3529300: 227.759877\n",
      "Average loss at step 3529400: 226.546049\n",
      "Average loss at step 3529500: 222.347712\n",
      "Average loss at step 3529600: 223.996011\n",
      "Average loss at step 3529700: 220.835684\n",
      "Average loss at step 3529800: 223.287336\n",
      "Average loss at step 3529900: 226.837439\n",
      "Average loss at step 3530000: 221.957304\n",
      "Graph 706: 14 nodes\n",
      "Average loss at step 3530100: 359.468845\n",
      "Average loss at step 3530200: 338.652419\n",
      "Average loss at step 3530300: 333.002572\n",
      "Average loss at step 3530400: 335.095741\n",
      "Average loss at step 3530500: 335.572715\n",
      "Average loss at step 3530600: 332.148577\n",
      "Average loss at step 3530700: 329.634800\n",
      "Average loss at step 3530800: 330.499041\n",
      "Average loss at step 3530900: 326.228597\n",
      "Average loss at step 3531000: 328.173689\n",
      "Average loss at step 3531100: 329.865945\n",
      "Average loss at step 3531200: 330.125345\n",
      "Average loss at step 3531300: 328.511597\n",
      "Average loss at step 3531400: 329.294131\n",
      "Average loss at step 3531500: 328.343734\n",
      "Average loss at step 3531600: 331.174527\n",
      "Average loss at step 3531700: 329.410049\n",
      "Average loss at step 3531800: 329.016312\n",
      "Average loss at step 3531900: 327.847956\n",
      "Average loss at step 3532000: 329.982818\n",
      "Average loss at step 3532100: 326.223586\n",
      "Average loss at step 3532200: 327.222014\n",
      "Average loss at step 3532300: 328.664052\n",
      "Average loss at step 3532400: 328.242438\n",
      "Average loss at step 3532500: 326.525079\n",
      "Average loss at step 3532600: 327.557083\n",
      "Average loss at step 3532700: 332.530190\n",
      "Average loss at step 3532800: 329.484355\n",
      "Average loss at step 3532900: 329.493496\n",
      "Average loss at step 3533000: 326.650007\n",
      "Average loss at step 3533100: 328.788651\n",
      "Average loss at step 3533200: 326.333302\n",
      "Average loss at step 3533300: 326.524155\n",
      "Average loss at step 3533400: 328.787268\n",
      "Average loss at step 3533500: 325.408108\n",
      "Average loss at step 3533600: 328.018365\n",
      "Average loss at step 3533700: 326.329860\n",
      "Average loss at step 3533800: 327.300798\n",
      "Average loss at step 3533900: 327.874907\n",
      "Average loss at step 3534000: 325.648231\n",
      "Average loss at step 3534100: 327.391248\n",
      "Average loss at step 3534200: 331.113450\n",
      "Average loss at step 3534300: 329.378112\n",
      "Average loss at step 3534400: 327.772319\n",
      "Average loss at step 3534500: 326.365613\n",
      "Average loss at step 3534600: 328.978204\n",
      "Average loss at step 3534700: 323.546756\n",
      "Average loss at step 3534800: 324.554660\n",
      "Average loss at step 3534900: 328.061252\n",
      "Average loss at step 3535000: 329.440857\n",
      "Graph 707: 12 nodes\n",
      "Average loss at step 3535100: 340.208474\n",
      "Average loss at step 3535200: 333.210654\n",
      "Average loss at step 3535300: 329.554591\n",
      "Average loss at step 3535400: 331.790143\n",
      "Average loss at step 3535500: 334.223804\n",
      "Average loss at step 3535600: 332.215477\n",
      "Average loss at step 3535700: 331.837214\n",
      "Average loss at step 3535800: 330.964127\n",
      "Average loss at step 3535900: 333.573449\n",
      "Average loss at step 3536000: 332.469984\n",
      "Average loss at step 3536100: 331.415040\n",
      "Average loss at step 3536200: 329.895893\n",
      "Average loss at step 3635400: 298.781154\n",
      "Average loss at step 3635500: 296.605588\n",
      "Average loss at step 3635600: 294.825300\n",
      "Average loss at step 3635700: 294.725075\n",
      "Average loss at step 3635800: 291.792076\n",
      "Average loss at step 3635900: 295.899682\n",
      "Average loss at step 3636000: 296.666133\n",
      "Average loss at step 3636100: 287.587047\n",
      "Average loss at step 3636200: 294.836908\n",
      "Average loss at step 3636300: 293.708502\n",
      "Average loss at step 3636400: 289.584774\n",
      "Average loss at step 3636500: 288.879637\n",
      "Average loss at step 3636600: 290.525312\n",
      "Average loss at step 3636700: 289.703665\n",
      "Average loss at step 3636800: 294.690364\n",
      "Average loss at step 3636900: 296.198640\n",
      "Average loss at step 3637000: 291.541725\n",
      "Average loss at step 3637100: 295.800915\n",
      "Average loss at step 3637200: 287.350750\n",
      "Average loss at step 3637300: 290.584769\n",
      "Average loss at step 3637400: 288.111048\n",
      "Average loss at step 3637500: 292.412517\n",
      "Average loss at step 3637600: 289.517944\n",
      "Average loss at step 3637700: 289.653205\n",
      "Average loss at step 3637800: 286.819434\n",
      "Average loss at step 3637900: 288.549064\n",
      "Average loss at step 3638000: 292.432599\n",
      "Average loss at step 3638100: 289.688665\n",
      "Average loss at step 3638200: 287.650771\n",
      "Average loss at step 3638300: 286.174560\n",
      "Average loss at step 3638400: 287.398275\n",
      "Average loss at step 3638500: 294.376970\n",
      "Average loss at step 3638600: 290.447599\n",
      "Average loss at step 3638700: 293.437150\n",
      "Average loss at step 3638800: 292.987057\n",
      "Average loss at step 3638900: 293.620892\n",
      "Average loss at step 3639000: 292.250415\n",
      "Average loss at step 3639100: 289.744249\n",
      "Average loss at step 3639200: 289.244127\n",
      "Average loss at step 3639300: 290.501070\n",
      "Average loss at step 3639400: 287.170314\n",
      "Average loss at step 3639500: 286.646244\n",
      "Average loss at step 3639600: 294.138035\n",
      "Average loss at step 3639700: 292.401379\n",
      "Average loss at step 3639800: 291.475770\n",
      "Average loss at step 3639900: 293.022540\n",
      "Average loss at step 3640000: 288.673219\n",
      "Graph 728: 14 nodes\n",
      "Average loss at step 3640100: 353.300604\n",
      "Average loss at step 3640200: 350.646443\n",
      "Average loss at step 3640300: 343.125731\n",
      "Average loss at step 3640400: 343.159331\n",
      "Average loss at step 3640500: 341.772638\n",
      "Average loss at step 3640600: 339.549526\n",
      "Average loss at step 3640700: 338.972181\n",
      "Average loss at step 3640800: 337.942315\n",
      "Average loss at step 3640900: 335.110008\n",
      "Average loss at step 3641000: 336.256930\n",
      "Average loss at step 3641100: 337.802955\n",
      "Average loss at step 3641200: 338.930332\n",
      "Average loss at step 3641300: 334.223671\n",
      "Average loss at step 3641400: 336.734770\n",
      "Average loss at step 3641500: 337.184459\n",
      "Average loss at step 3641600: 337.576733\n",
      "Average loss at step 3641700: 338.047066\n",
      "Average loss at step 3641800: 338.884421\n",
      "Average loss at step 3641900: 338.961231\n",
      "Average loss at step 3642000: 338.156295\n",
      "Average loss at step 3642100: 333.350688\n",
      "Average loss at step 3642200: 335.636704\n",
      "Average loss at step 3642300: 333.588506\n",
      "Average loss at step 3642400: 335.527330\n",
      "Average loss at step 3642500: 334.021277\n",
      "Average loss at step 3642600: 334.270797\n",
      "Average loss at step 3642700: 336.707800\n",
      "Average loss at step 3642800: 339.097754\n",
      "Average loss at step 3642900: 336.000122\n",
      "Average loss at step 3643000: 331.696872\n",
      "Average loss at step 3643100: 335.180250\n",
      "Average loss at step 3643200: 337.824284\n",
      "Average loss at step 3643300: 335.840517\n",
      "Average loss at step 3643400: 337.509696\n",
      "Average loss at step 3643500: 336.417081\n",
      "Average loss at step 3643600: 333.263879\n",
      "Average loss at step 3643700: 334.231611\n",
      "Average loss at step 3643800: 332.944104\n",
      "Average loss at step 3643900: 336.338386\n",
      "Average loss at step 3644000: 332.097747\n",
      "Average loss at step 3644100: 337.886728\n",
      "Average loss at step 3644200: 336.939406\n",
      "Average loss at step 3644300: 336.121271\n",
      "Average loss at step 3644400: 332.944906\n",
      "Average loss at step 3644500: 336.749405\n",
      "Average loss at step 3644600: 335.058906\n",
      "Average loss at step 3644700: 333.957965\n",
      "Average loss at step 3644800: 334.275641\n",
      "Average loss at step 3644900: 335.234512\n",
      "Average loss at step 3645000: 333.723443\n",
      "Graph 729: 14 nodes\n",
      "Average loss at step 3645100: 336.134646\n",
      "Average loss at step 3645200: 333.065551\n",
      "Average loss at step 3645300: 327.598056\n",
      "Average loss at step 3645400: 331.499392\n",
      "Average loss at step 3645500: 329.730624\n",
      "Average loss at step 3645600: 327.946262\n",
      "Average loss at step 3645700: 325.662827\n",
      "Average loss at step 3645800: 323.424827\n",
      "Average loss at step 3645900: 324.459049\n",
      "Average loss at step 3646000: 324.662598\n",
      "Average loss at step 3646100: 325.741219\n",
      "Average loss at step 3646200: 325.054999\n",
      "Average loss at step 3646300: 321.935324\n",
      "Average loss at step 3646400: 326.485537\n",
      "Average loss at step 3646500: 324.325589\n",
      "Average loss at step 3646600: 322.828663\n",
      "Average loss at step 3646700: 322.999461\n",
      "Average loss at step 3646800: 323.505134\n",
      "Average loss at step 3646900: 325.965809\n",
      "Average loss at step 3647000: 321.909117\n",
      "Average loss at step 3647100: 324.304086\n",
      "Average loss at step 3647200: 320.428201\n",
      "Average loss at step 3647300: 323.057931\n",
      "Average loss at step 3647400: 321.201092\n",
      "Average loss at step 3647500: 321.643513\n",
      "Average loss at step 3647600: 324.612067\n",
      "Average loss at step 3647700: 324.068759\n",
      "Average loss at step 3647800: 324.281033\n",
      "Average loss at step 3647900: 323.346714\n",
      "Average loss at step 3648000: 323.585668\n",
      "Average loss at step 3648100: 323.128315\n",
      "Average loss at step 3648200: 321.595099\n",
      "Average loss at step 3648300: 323.814301\n",
      "Average loss at step 3648400: 322.326918\n",
      "Average loss at step 3648500: 323.590073\n",
      "Average loss at step 3648600: 327.058265\n",
      "Average loss at step 3648700: 324.768375\n",
      "Average loss at step 3648800: 323.156240\n",
      "Average loss at step 3648900: 322.855710\n",
      "Average loss at step 3649000: 321.468448\n",
      "Average loss at step 3649100: 326.589633\n",
      "Average loss at step 3649200: 320.334760\n",
      "Average loss at step 3649300: 322.865855\n",
      "Average loss at step 3649400: 325.527663\n",
      "Average loss at step 3649500: 323.256370\n",
      "Average loss at step 3649600: 323.895638\n",
      "Average loss at step 3649700: 322.766754\n",
      "Average loss at step 3649800: 322.612872\n",
      "Average loss at step 3649900: 323.369648\n",
      "Average loss at step 3650000: 325.279237\n",
      "Graph 730: 26 nodes\n",
      "Average loss at step 3650100: 337.440711\n",
      "Average loss at step 3650200: 334.114737\n",
      "Average loss at step 3650300: 335.634167\n",
      "Average loss at step 3650400: 335.296941\n",
      "Average loss at step 3650500: 333.343547\n",
      "Average loss at step 3650600: 331.511191\n",
      "Average loss at step 3650700: 332.195396\n",
      "Average loss at step 3751000: 327.430919\n",
      "Average loss at step 3751100: 328.009034\n",
      "Average loss at step 3751200: 326.960373\n",
      "Average loss at step 3751300: 322.464637\n",
      "Average loss at step 3751400: 329.293955\n",
      "Average loss at step 3751500: 327.542573\n",
      "Average loss at step 3751600: 325.989300\n",
      "Average loss at step 3751700: 326.671139\n",
      "Average loss at step 3751800: 330.147864\n",
      "Average loss at step 3751900: 329.615977\n",
      "Average loss at step 3752000: 329.602939\n",
      "Average loss at step 3752100: 327.131106\n",
      "Average loss at step 3752200: 327.627002\n",
      "Average loss at step 3752300: 326.277486\n",
      "Average loss at step 3752400: 327.582773\n",
      "Average loss at step 3752500: 325.829453\n",
      "Average loss at step 3752600: 328.199044\n",
      "Average loss at step 3752700: 327.901440\n",
      "Average loss at step 3752800: 329.725181\n",
      "Average loss at step 3752900: 326.747324\n",
      "Average loss at step 3753000: 331.307268\n",
      "Average loss at step 3753100: 330.449086\n",
      "Average loss at step 3753200: 323.815531\n",
      "Average loss at step 3753300: 326.546395\n",
      "Average loss at step 3753400: 329.070658\n",
      "Average loss at step 3753500: 327.366935\n",
      "Average loss at step 3753600: 324.984016\n",
      "Average loss at step 3753700: 328.019660\n",
      "Average loss at step 3753800: 323.767878\n",
      "Average loss at step 3753900: 324.826400\n",
      "Average loss at step 3754000: 327.356750\n",
      "Average loss at step 3754100: 323.879452\n",
      "Average loss at step 3754200: 325.972885\n",
      "Average loss at step 3754300: 321.927624\n",
      "Average loss at step 3754400: 328.312493\n",
      "Average loss at step 3754500: 327.641750\n",
      "Average loss at step 3754600: 326.206075\n",
      "Average loss at step 3754700: 326.977301\n",
      "Average loss at step 3754800: 329.793903\n",
      "Average loss at step 3754900: 325.828377\n",
      "Average loss at step 3755000: 329.916979\n",
      "Time: 24.1530489922\n",
      "Graph 751: 64 nodes\n",
      "Average loss at step 3755100: 350.127255\n",
      "Average loss at step 3755200: 339.837714\n",
      "Average loss at step 3755300: 331.467284\n",
      "Average loss at step 3755400: 330.858777\n",
      "Average loss at step 3755500: 334.183415\n",
      "Average loss at step 3755600: 329.660955\n",
      "Average loss at step 3755700: 328.752295\n",
      "Average loss at step 3755800: 328.749404\n",
      "Average loss at step 3755900: 332.099114\n",
      "Average loss at step 3756000: 328.904769\n",
      "Average loss at step 3756100: 332.190565\n",
      "Average loss at step 3756200: 334.016712\n",
      "Average loss at step 3756300: 332.355698\n",
      "Average loss at step 3756400: 324.707476\n",
      "Average loss at step 3756500: 331.732078\n",
      "Average loss at step 3756600: 331.345376\n",
      "Average loss at step 3756700: 331.283231\n",
      "Average loss at step 3756800: 327.878351\n",
      "Average loss at step 3756900: 333.503479\n",
      "Average loss at step 3757000: 327.750693\n",
      "Average loss at step 3757100: 330.918474\n",
      "Average loss at step 3757200: 330.091998\n",
      "Average loss at step 3757300: 329.012571\n",
      "Average loss at step 3757400: 331.338368\n",
      "Average loss at step 3757500: 327.321080\n",
      "Average loss at step 3757600: 331.026803\n",
      "Average loss at step 3757700: 330.718689\n",
      "Average loss at step 3757800: 328.890721\n",
      "Average loss at step 3757900: 331.193148\n",
      "Average loss at step 3758000: 331.512537\n",
      "Average loss at step 3758100: 332.704205\n",
      "Average loss at step 3758200: 329.566157\n",
      "Average loss at step 3758300: 331.560371\n",
      "Average loss at step 3758400: 330.948861\n",
      "Average loss at step 3758500: 332.279494\n",
      "Average loss at step 3758600: 334.158059\n",
      "Average loss at step 3758700: 329.510360\n",
      "Average loss at step 3758800: 330.668574\n",
      "Average loss at step 3758900: 333.353413\n",
      "Average loss at step 3759000: 327.257957\n",
      "Average loss at step 3759100: 330.350904\n",
      "Average loss at step 3759200: 331.374519\n",
      "Average loss at step 3759300: 332.331924\n",
      "Average loss at step 3759400: 329.171502\n",
      "Average loss at step 3759500: 333.860339\n",
      "Average loss at step 3759600: 335.076536\n",
      "Average loss at step 3759700: 330.515947\n",
      "Average loss at step 3759800: 331.946673\n",
      "Average loss at step 3759900: 325.463880\n",
      "Average loss at step 3760000: 332.074950\n",
      "Graph 752: 26 nodes\n",
      "Average loss at step 3760100: 336.538419\n",
      "Average loss at step 3760200: 332.132034\n",
      "Average loss at step 3760300: 330.728898\n",
      "Average loss at step 3760400: 326.138720\n",
      "Average loss at step 3760500: 329.678737\n",
      "Average loss at step 3760600: 328.063591\n",
      "Average loss at step 3760700: 329.721869\n",
      "Average loss at step 3760800: 323.163496\n",
      "Average loss at step 3760900: 326.704142\n",
      "Average loss at step 3761000: 326.604760\n",
      "Average loss at step 3761100: 329.830453\n",
      "Average loss at step 3761200: 324.919778\n",
      "Average loss at step 3761300: 325.848342\n",
      "Average loss at step 3761400: 328.566360\n",
      "Average loss at step 3761500: 328.567845\n",
      "Average loss at step 3761600: 326.584826\n",
      "Average loss at step 3761700: 326.278977\n",
      "Average loss at step 3761800: 327.052276\n",
      "Average loss at step 3761900: 327.875316\n",
      "Average loss at step 3762000: 324.541439\n",
      "Average loss at step 3762100: 323.237359\n",
      "Average loss at step 3762200: 325.275459\n",
      "Average loss at step 3762300: 327.872682\n",
      "Average loss at step 3762400: 325.066555\n",
      "Average loss at step 3762500: 327.845853\n",
      "Average loss at step 3762600: 327.326978\n",
      "Average loss at step 3762700: 328.449077\n",
      "Average loss at step 3762800: 324.974136\n",
      "Average loss at step 3762900: 324.887738\n",
      "Average loss at step 3763000: 325.718671\n",
      "Average loss at step 3763100: 329.443964\n",
      "Average loss at step 3763200: 325.987765\n",
      "Average loss at step 3763300: 326.893051\n",
      "Average loss at step 3763400: 324.171003\n",
      "Average loss at step 3763500: 323.558017\n",
      "Average loss at step 3763600: 327.295279\n",
      "Average loss at step 3763700: 326.445628\n",
      "Average loss at step 3763800: 328.003223\n",
      "Average loss at step 3763900: 325.187458\n",
      "Average loss at step 3764000: 327.177140\n",
      "Average loss at step 3764100: 324.971474\n",
      "Average loss at step 3764200: 323.565119\n",
      "Average loss at step 3764300: 325.070845\n",
      "Average loss at step 3764400: 324.856287\n",
      "Average loss at step 3764500: 324.888717\n",
      "Average loss at step 3764600: 325.278060\n",
      "Average loss at step 3764700: 326.695088\n",
      "Average loss at step 3764800: 326.562911\n",
      "Average loss at step 3764900: 328.007864\n",
      "Average loss at step 3765000: 326.609113\n",
      "Graph 753: 27 nodes\n",
      "Average loss at step 3765100: 380.302377\n",
      "Average loss at step 3765200: 353.006101\n",
      "Average loss at step 3765300: 348.616032\n",
      "Average loss at step 3765400: 349.387414\n",
      "Average loss at step 3765500: 345.521241\n",
      "Average loss at step 3765600: 347.842309\n",
      "Average loss at step 3765700: 346.211423\n",
      "Average loss at step 3765800: 345.646179\n",
      "Average loss at step 3765900: 354.484361\n",
      "Average loss at step 3766000: 344.046989\n",
      "Average loss at step 3766100: 345.134616\n",
      "Average loss at step 3766200: 354.270960\n",
      "Average loss at step 3766300: 342.010226\n",
      "Average loss at step 3866600: 326.277061\n",
      "Average loss at step 3866700: 321.704390\n",
      "Average loss at step 3866800: 322.168594\n",
      "Average loss at step 3866900: 322.315627\n",
      "Average loss at step 3867000: 327.856598\n",
      "Average loss at step 3867100: 320.127365\n",
      "Average loss at step 3867200: 323.333256\n",
      "Average loss at step 3867300: 321.948904\n",
      "Average loss at step 3867400: 326.541354\n",
      "Average loss at step 3867500: 319.253403\n",
      "Average loss at step 3867600: 325.792153\n",
      "Average loss at step 3867700: 324.314058\n",
      "Average loss at step 3867800: 327.384373\n",
      "Average loss at step 3867900: 325.918450\n",
      "Average loss at step 3868000: 322.334263\n",
      "Average loss at step 3868100: 326.860114\n",
      "Average loss at step 3868200: 321.308711\n",
      "Average loss at step 3868300: 322.043420\n",
      "Average loss at step 3868400: 325.913529\n",
      "Average loss at step 3868500: 323.790061\n",
      "Average loss at step 3868600: 324.182363\n",
      "Average loss at step 3868700: 322.909932\n",
      "Average loss at step 3868800: 322.712973\n",
      "Average loss at step 3868900: 322.250347\n",
      "Average loss at step 3869000: 323.608592\n",
      "Average loss at step 3869100: 325.581965\n",
      "Average loss at step 3869200: 325.100527\n",
      "Average loss at step 3869300: 322.772830\n",
      "Average loss at step 3869400: 320.057652\n",
      "Average loss at step 3869500: 324.663171\n",
      "Average loss at step 3869600: 324.493384\n",
      "Average loss at step 3869700: 322.990415\n",
      "Average loss at step 3869800: 321.426381\n",
      "Average loss at step 3869900: 324.757685\n",
      "Average loss at step 3870000: 327.530866\n",
      "Graph 774: 25 nodes\n",
      "Average loss at step 3870100: 340.260819\n",
      "Average loss at step 3870200: 337.457546\n",
      "Average loss at step 3870300: 336.777090\n",
      "Average loss at step 3870400: 338.037155\n",
      "Average loss at step 3870500: 340.264236\n",
      "Average loss at step 3870600: 331.928237\n",
      "Average loss at step 3870700: 334.545599\n",
      "Average loss at step 3870800: 330.281848\n",
      "Average loss at step 3870900: 337.726916\n",
      "Average loss at step 3871000: 334.618164\n",
      "Average loss at step 3871100: 337.113158\n",
      "Average loss at step 3871200: 333.997333\n",
      "Average loss at step 3871300: 331.822684\n",
      "Average loss at step 3871400: 335.865374\n",
      "Average loss at step 3871500: 331.093226\n",
      "Average loss at step 3871600: 333.717466\n",
      "Average loss at step 3871700: 337.013712\n",
      "Average loss at step 3871800: 336.413115\n",
      "Average loss at step 3871900: 332.844195\n",
      "Average loss at step 3872000: 329.724528\n",
      "Average loss at step 3872100: 334.409204\n",
      "Average loss at step 3872200: 331.667657\n",
      "Average loss at step 3872300: 334.383832\n",
      "Average loss at step 3872400: 330.929181\n",
      "Average loss at step 3872500: 335.588648\n",
      "Average loss at step 3872600: 332.751031\n",
      "Average loss at step 3872700: 333.626916\n",
      "Average loss at step 3872800: 332.252449\n",
      "Average loss at step 3872900: 335.322485\n",
      "Average loss at step 3873000: 331.859488\n",
      "Average loss at step 3873100: 335.366185\n",
      "Average loss at step 3873200: 331.875397\n",
      "Average loss at step 3873300: 332.112745\n",
      "Average loss at step 3873400: 337.989485\n",
      "Average loss at step 3873500: 338.478209\n",
      "Average loss at step 3873600: 331.788773\n",
      "Average loss at step 3873700: 335.609167\n",
      "Average loss at step 3873800: 331.343761\n",
      "Average loss at step 3873900: 332.471184\n",
      "Average loss at step 3874000: 335.974080\n",
      "Average loss at step 3874100: 339.508870\n",
      "Average loss at step 3874200: 332.869032\n",
      "Average loss at step 3874300: 336.003244\n",
      "Average loss at step 3874400: 333.463660\n",
      "Average loss at step 3874500: 333.779771\n",
      "Average loss at step 3874600: 332.447314\n",
      "Average loss at step 3874700: 334.829866\n",
      "Average loss at step 3874800: 331.030303\n",
      "Average loss at step 3874900: 333.188231\n",
      "Average loss at step 3875000: 336.822379\n",
      "Graph 775: 32 nodes\n",
      "Average loss at step 3875100: 329.281836\n",
      "Average loss at step 3875200: 327.758023\n",
      "Average loss at step 3875300: 327.188673\n",
      "Average loss at step 3875400: 323.888682\n",
      "Average loss at step 3875500: 324.620302\n",
      "Average loss at step 3875600: 327.127842\n",
      "Average loss at step 3875700: 323.831156\n",
      "Average loss at step 3875800: 322.159303\n",
      "Average loss at step 3875900: 326.465808\n",
      "Average loss at step 3876000: 319.145452\n",
      "Average loss at step 3876100: 327.649273\n",
      "Average loss at step 3876200: 324.698021\n",
      "Average loss at step 3876300: 325.352383\n",
      "Average loss at step 3876400: 324.219999\n",
      "Average loss at step 3876500: 324.046295\n",
      "Average loss at step 3876600: 322.073559\n",
      "Average loss at step 3876700: 327.140063\n",
      "Average loss at step 3876800: 320.699312\n",
      "Average loss at step 3876900: 322.093024\n",
      "Average loss at step 3877000: 323.362300\n",
      "Average loss at step 3877100: 324.288840\n",
      "Average loss at step 3877200: 321.912479\n",
      "Average loss at step 3877300: 321.985195\n",
      "Average loss at step 3877400: 321.472846\n",
      "Average loss at step 3877500: 321.721775\n",
      "Average loss at step 3877600: 322.637292\n",
      "Average loss at step 3877700: 323.600240\n",
      "Average loss at step 3877800: 319.808627\n",
      "Average loss at step 3877900: 320.381722\n",
      "Average loss at step 3878000: 320.753126\n",
      "Average loss at step 3878100: 319.943997\n",
      "Average loss at step 3878200: 323.148373\n",
      "Average loss at step 3878300: 324.055639\n",
      "Average loss at step 3878400: 322.148018\n",
      "Average loss at step 3878500: 323.145854\n",
      "Average loss at step 3878600: 322.349096\n",
      "Average loss at step 3878700: 320.472118\n",
      "Average loss at step 3878800: 322.048279\n",
      "Average loss at step 3878900: 324.410681\n",
      "Average loss at step 3879000: 323.607102\n",
      "Average loss at step 3879100: 320.920420\n",
      "Average loss at step 3879200: 321.728375\n",
      "Average loss at step 3879300: 324.999717\n",
      "Average loss at step 3879400: 318.942046\n",
      "Average loss at step 3879500: 322.855911\n",
      "Average loss at step 3879600: 321.018703\n",
      "Average loss at step 3879700: 321.599616\n",
      "Average loss at step 3879800: 322.274613\n",
      "Average loss at step 3879900: 318.231275\n",
      "Average loss at step 3880000: 321.356738\n",
      "Graph 776: 38 nodes\n",
      "Average loss at step 3880100: 336.924894\n",
      "Average loss at step 3880200: 335.424943\n",
      "Average loss at step 3880300: 333.812097\n",
      "Average loss at step 3880400: 338.406624\n",
      "Average loss at step 3880500: 335.366582\n",
      "Average loss at step 3880600: 332.551762\n",
      "Average loss at step 3880700: 333.366307\n",
      "Average loss at step 3880800: 333.043525\n",
      "Average loss at step 3880900: 334.042331\n",
      "Average loss at step 3881000: 327.183624\n",
      "Average loss at step 3881100: 333.106990\n",
      "Average loss at step 3881200: 337.813425\n",
      "Average loss at step 3881300: 335.444362\n",
      "Average loss at step 3881400: 333.488606\n",
      "Average loss at step 3881500: 328.604174\n",
      "Average loss at step 3881600: 334.638854\n",
      "Average loss at step 3881700: 330.801451\n",
      "Average loss at step 3881800: 332.577959\n",
      "Average loss at step 3881900: 332.325824\n",
      "Average loss at step 3982300: 330.494866\n",
      "Average loss at step 3982400: 327.200941\n",
      "Average loss at step 3982500: 329.662399\n",
      "Average loss at step 3982600: 328.126190\n",
      "Average loss at step 3982700: 324.216742\n",
      "Average loss at step 3982800: 327.236378\n",
      "Average loss at step 3982900: 325.585235\n",
      "Average loss at step 3983000: 326.388958\n",
      "Average loss at step 3983100: 328.202353\n",
      "Average loss at step 3983200: 326.476694\n",
      "Average loss at step 3983300: 324.597562\n",
      "Average loss at step 3983400: 327.679794\n",
      "Average loss at step 3983500: 324.828693\n",
      "Average loss at step 3983600: 325.984113\n",
      "Average loss at step 3983700: 325.588343\n",
      "Average loss at step 3983800: 321.004402\n",
      "Average loss at step 3983900: 324.796649\n",
      "Average loss at step 3984000: 327.743003\n",
      "Average loss at step 3984100: 326.906564\n",
      "Average loss at step 3984200: 324.407546\n",
      "Average loss at step 3984300: 323.115202\n",
      "Average loss at step 3984400: 326.270439\n",
      "Average loss at step 3984500: 327.973938\n",
      "Average loss at step 3984600: 327.827886\n",
      "Average loss at step 3984700: 324.076086\n",
      "Average loss at step 3984800: 324.104878\n",
      "Average loss at step 3984900: 326.923846\n",
      "Average loss at step 3985000: 325.930091\n",
      "Graph 797: 38 nodes\n",
      "Average loss at step 3985100: 360.563014\n",
      "Average loss at step 3985200: 349.726154\n",
      "Average loss at step 3985300: 345.070724\n",
      "Average loss at step 3985400: 341.749089\n",
      "Average loss at step 3985500: 346.689226\n",
      "Average loss at step 3985600: 342.956424\n",
      "Average loss at step 3985700: 347.656169\n",
      "Average loss at step 3985800: 347.610758\n",
      "Average loss at step 3985900: 344.054952\n",
      "Average loss at step 3986000: 344.811007\n",
      "Average loss at step 3986100: 345.409670\n",
      "Average loss at step 3986200: 350.087720\n",
      "Average loss at step 3986300: 342.660229\n",
      "Average loss at step 3986400: 339.958105\n",
      "Average loss at step 3986500: 341.273710\n",
      "Average loss at step 3986600: 346.682943\n",
      "Average loss at step 3986700: 342.769666\n",
      "Average loss at step 3986800: 343.966875\n",
      "Average loss at step 3986900: 343.969547\n",
      "Average loss at step 3987000: 340.631603\n",
      "Average loss at step 3987100: 343.350017\n",
      "Average loss at step 3987200: 344.891062\n",
      "Average loss at step 3987300: 342.083694\n",
      "Average loss at step 3987400: 342.770086\n",
      "Average loss at step 3987500: 348.446450\n",
      "Average loss at step 3987600: 345.956833\n",
      "Average loss at step 3987700: 341.162448\n",
      "Average loss at step 3987800: 341.554572\n",
      "Average loss at step 3987900: 344.719267\n",
      "Average loss at step 3988000: 343.652757\n",
      "Average loss at step 3988100: 344.367472\n",
      "Average loss at step 3988200: 343.498486\n",
      "Average loss at step 3988300: 341.015941\n",
      "Average loss at step 3988400: 345.519081\n",
      "Average loss at step 3988500: 340.732693\n",
      "Average loss at step 3988600: 345.642499\n",
      "Average loss at step 3988700: 344.481767\n",
      "Average loss at step 3988800: 343.589154\n",
      "Average loss at step 3988900: 341.144189\n",
      "Average loss at step 3989000: 342.489652\n",
      "Average loss at step 3989100: 337.512990\n",
      "Average loss at step 3989200: 345.187616\n",
      "Average loss at step 3989300: 339.574657\n",
      "Average loss at step 3989400: 346.200325\n",
      "Average loss at step 3989500: 342.968730\n",
      "Average loss at step 3989600: 342.173518\n",
      "Average loss at step 3989700: 345.668401\n",
      "Average loss at step 3989800: 337.630042\n",
      "Average loss at step 3989900: 339.993291\n",
      "Average loss at step 3990000: 340.014111\n",
      "Graph 798: 22 nodes\n",
      "Average loss at step 3990100: 332.814440\n",
      "Average loss at step 3990200: 327.003573\n",
      "Average loss at step 3990300: 326.523477\n",
      "Average loss at step 3990400: 334.723863\n",
      "Average loss at step 3990500: 327.424912\n",
      "Average loss at step 3990600: 328.268580\n",
      "Average loss at step 3990700: 322.273976\n",
      "Average loss at step 3990800: 320.618523\n",
      "Average loss at step 3990900: 325.394635\n",
      "Average loss at step 3991000: 324.199488\n",
      "Average loss at step 3991100: 325.416001\n",
      "Average loss at step 3991200: 327.050601\n",
      "Average loss at step 3991300: 324.408890\n",
      "Average loss at step 3991400: 327.935156\n",
      "Average loss at step 3991500: 326.727422\n",
      "Average loss at step 3991600: 325.387772\n",
      "Average loss at step 3991700: 327.316141\n",
      "Average loss at step 3991800: 323.321546\n",
      "Average loss at step 3991900: 328.386707\n",
      "Average loss at step 3992000: 324.222055\n",
      "Average loss at step 3992100: 324.876698\n",
      "Average loss at step 3992200: 327.455472\n",
      "Average loss at step 3992300: 323.238461\n",
      "Average loss at step 3992400: 328.750518\n",
      "Average loss at step 3992500: 328.197646\n",
      "Average loss at step 3992600: 329.226835\n",
      "Average loss at step 3992700: 328.954303\n",
      "Average loss at step 3992800: 324.295138\n",
      "Average loss at step 3992900: 326.279540\n",
      "Average loss at step 3993000: 327.368334\n",
      "Average loss at step 3993100: 322.849413\n",
      "Average loss at step 3993200: 327.914056\n",
      "Average loss at step 3993300: 325.952498\n",
      "Average loss at step 3993400: 327.988345\n",
      "Average loss at step 3993500: 327.801365\n",
      "Average loss at step 3993600: 327.215020\n",
      "Average loss at step 3993700: 325.526718\n",
      "Average loss at step 3993800: 327.127511\n",
      "Average loss at step 3993900: 328.641077\n",
      "Average loss at step 3994000: 325.108983\n",
      "Average loss at step 3994100: 325.537426\n",
      "Average loss at step 3994200: 321.797071\n",
      "Average loss at step 3994300: 321.087601\n",
      "Average loss at step 3994400: 327.605468\n",
      "Average loss at step 3994500: 319.273835\n",
      "Average loss at step 3994600: 326.834032\n",
      "Average loss at step 3994700: 325.362951\n",
      "Average loss at step 3994800: 325.467045\n",
      "Average loss at step 3994900: 327.297518\n",
      "Average loss at step 3995000: 325.543861\n",
      "Graph 799: 18 nodes\n",
      "Average loss at step 3995100: 329.508532\n",
      "Average loss at step 3995200: 332.939489\n",
      "Average loss at step 3995300: 332.079853\n",
      "Average loss at step 3995400: 330.292288\n",
      "Average loss at step 3995500: 327.274858\n",
      "Average loss at step 3995600: 331.882035\n",
      "Average loss at step 3995700: 331.278293\n",
      "Average loss at step 3995800: 326.337765\n",
      "Average loss at step 3995900: 326.708238\n",
      "Average loss at step 3996000: 329.869337\n",
      "Average loss at step 3996100: 330.665395\n",
      "Average loss at step 3996200: 328.537407\n",
      "Average loss at step 3996300: 329.815156\n",
      "Average loss at step 3996400: 327.015030\n",
      "Average loss at step 3996500: 326.630848\n",
      "Average loss at step 3996600: 327.164191\n",
      "Average loss at step 3996700: 330.595396\n",
      "Average loss at step 3996800: 329.130196\n",
      "Average loss at step 3996900: 324.557604\n",
      "Average loss at step 3997000: 327.529491\n",
      "Average loss at step 3997100: 330.855391\n",
      "Average loss at step 3997200: 328.194267\n",
      "Average loss at step 3997300: 328.377222\n",
      "Average loss at step 3997400: 328.516957\n",
      "Average loss at step 3997500: 326.783950\n",
      "Average loss at step 3997600: 326.190869\n"
     ]
    }
   ],
   "source": [
    "dataset = ''\n",
    "\n",
    "batch_size = 10\n",
    "window_size = 4\n",
    "num_samples = 128\n",
    "\n",
    "root = '../NCI1/'\n",
    "ext = 'graphml'\n",
    "epochs = 1\n",
    "batches_per_epoch = 5000\n",
    "candidate_func = 'uniform'\n",
    "graph_labels = 'edges'\n",
    "\n",
    "model1 = AnonymousWalkEmbeddings.AWE(dataset = dataset, batch_size = batch_size, window_size = window_size,\n",
    "                  num_samples = num_samples, root = root,\n",
    "                  ext = ext,epochs = epochs, batches_per_epoch = batches_per_epoch,\n",
    "                  candidate_func = candidate_func, graph_labels=graph_labels)\n",
    "\n",
    "model1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Подгрузим разметку для графов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = open('../NCI109/NCI109_label.txt')\n",
    "labels = labels.read()\n",
    "labels = np.array(map(lambda x: int(x), labels.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels1 = open('../enron/parsed_graphs/labels.txt')\n",
    "labels1 = labels1.read()\n",
    "labels1 = np.array(map(lambda x: int(x), labels1.strip().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PCA approximation\n",
    "Здесь пробуем построить для части графов pca и потом посчитать проекции следующих эмбедингов на полученные направления. Нас интересует, как хорошо полученные проекции могут аппроксимировать сами эмбединги.\n",
    "\n",
    "Видим, что аутлаеры не особо выбиваются из общего паттерна. Дальше попробуем посмотреть, чем можно такое поведение объяснить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15ef3910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca_comp_number = 100\n",
    "\n",
    "test_set = model.graph_embeddings[:pca_comp_number]\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(scale(test_set))\n",
    "\n",
    "projection_set = pca.components_[:]\n",
    "\n",
    "res = []\n",
    "for embedding in model.graph_embeddings[pca_comp_number:]:\n",
    "    proj = projection_on_set(embedding, projection_set)\n",
    "    res.append(np.linalg.norm(embedding - proj))\n",
    "res = np.array(res)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(np.where(labels[pca_comp_number:] == 0)[0]+pca_comp_number, \n",
    "            res[np.where(labels[pca_comp_number:] == 0)[0]], label='normal', c='b')\n",
    "plt.scatter(np.where(labels[pca_comp_number:] == 1)[0]+pca_comp_number, \n",
    "            res[np.where(labels[pca_comp_number:] == 1)[0]], label='outlier', c='r')\n",
    "plt.xlabel(\"days\")\n",
    "plt.ylabel(\"$||X - X_{approx}||$\")\n",
    "plt.title(\"PCA approximation quality on time\")\n",
    "plt.legend()\n",
    "plt.grid(ls='dashed')\n",
    "#fig.savefig('graph_series_without_norm.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Norm differences \n",
    "\n",
    "Посмотрим, как меняются векторы эмбедингов от времени. Для этого, например, можно порисовать норму разностей двух последующих векторов. Как видно из графиков какой-то хорошей шшеометрической структуры пока не получили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-443-68f356bb3c88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                          \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m                          \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                          edgecolors=edgecolors, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   3435\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4064\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4066\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36mautoscale_view\u001b[0;34m(self, tight, scalex, scaley)\u001b[0m\n\u001b[1;32m   2334\u001b[0m         handle_single_axis(\n\u001b[1;32m   2335\u001b[0m             \u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autoscaleXon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_x_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'intervalx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m             'minposx', self.xaxis, self._xmargin, x_stickies, self.set_xbound)\n\u001b[0m\u001b[1;32m   2337\u001b[0m         handle_single_axis(\n\u001b[1;32m   2338\u001b[0m             \u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autoscaleYon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_y_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'intervaly'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36mhandle_single_axis\u001b[0;34m(scale, autoscaleon, shared_axes, interval, minpos, axis, margin, stickies, set_bound)\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;31m# We cannot use exact equality due to floating point issues e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2317\u001b[0m             \u001b[0;31m# with streamplot.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2318\u001b[0;31m             \u001b[0mdo_lower_margin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstickies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2319\u001b[0m             \u001b[0mdo_upper_margin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstickies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m             \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit_range_for_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36many\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2018\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2e655490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i, label in zip(range(len(labels[:-1])), labels[:-1]):\n",
    "    if label == 0:\n",
    "        plt.scatter(i, np.linalg.norm(model.graph_embeddings[i+1] - model.graph_embeddings[i]), c='b')\n",
    "    else:\n",
    "        plt.scatter(i, np.linalg.norm(model.graph_embeddings[i+1] - model.graph_embeddings[i]), c='r')\n",
    "    \n",
    "plt.xlabel('days')\n",
    "plt.ylabel('$||X_{i+1} - X_{i}||$')\n",
    "plt.title(\"Norm of embedding differences on time\")\n",
    "plt.grid(alpha=0.6)\n",
    "#fig.savefig('difference_embedding.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Norm series \n",
    "\n",
    "Есть предположение о том, что аутлаеры сильно будут отличаться по норме от обычных векторов. Построим график зависимости норм векторов от времени. Получаем, что мало того, что аутлаеры не выбиваются из общей кучи, так еще необычное поведение с ростом нормы эмбедингов. Дальше попробуем разобраться, чем оно вызвано."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEWCAYAAACT7WsrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a285b6610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "norms = np.linalg.norm(model.graph_embeddings, axis=1)\n",
    "\n",
    "plt.scatter(np.where(labels == 0)[0], norms[np.where(labels == 0)[0]], label='normal', c='b')\n",
    "plt.scatter(np.where(labels == 1)[0], norms[np.where(labels == 1)[0]], label='outlier', c='r')\n",
    "\n",
    "plt.xlabel(\"days\")\n",
    "plt.ylabel(\"$||X||$\")\n",
    "plt.title(\"Norm of embedding on time\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(ls='dashed')\n",
    "#fig.savefig('norms_series.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Norm growth\n",
    "\n",
    "Попробуем более отдельно рассмотреть вопрос с прошлого пункта. Интерсно, чем конкретно вызвано такое поведение у эмбедингов: либо это специфика модели, либо датасета. \n",
    "\n",
    "Чтобы проверить это предположение попробуем случайно помешать первые графы при обучении и снова отрисовать графики зависимости нормы от времени.\n",
    "\n",
    "Получаем, что на самом деле такой вид графика не зависит от датасета, а относится к самой модели. Плюс к этому, вид графика завист от парметра batches_per_epoch. Чем он больше, тем более гладким получается график. При совсем мелких значениях такая зависимость нормы от времени пропадает. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 637\n",
      "Number of words: 52\n",
      "Initialized\n",
      "Epoch: 0\n",
      "Graph 0: 20 nodes\n",
      "Time: 0.71936416626\n",
      "Graph 1: 277 nodes\n",
      "Graph 2: 274 nodes\n",
      "Graph 3: 278 nodes\n",
      "Graph 4: 293 nodes\n",
      "Graph 5: 262 nodes\n",
      "Graph 6: 9 nodes\n",
      "Graph 7: 8 nodes\n",
      "Graph 8: 31 nodes\n",
      "Graph 9: 32 nodes\n",
      "Average loss at step 100: 2627.964609\n",
      "Graph 10: 264 nodes\n",
      "Time: 0.589840173721\n",
      "Graph 11: 229 nodes\n",
      "Graph 12: 230 nodes\n",
      "Graph 13: 18 nodes\n",
      "Graph 14: 24 nodes\n",
      "Graph 15: 306 nodes\n",
      "Graph 16: 262 nodes\n",
      "Graph 17: 229 nodes\n",
      "Graph 18: 277 nodes\n",
      "Graph 19: 200 nodes\n",
      "Average loss at step 200: 606.774198\n",
      "Graph 20: 16 nodes\n",
      "Time: 0.440363883972\n",
      "Graph 21: 12 nodes\n",
      "Graph 22: 232 nodes\n",
      "Graph 23: 245 nodes\n",
      "Graph 24: 252 nodes\n",
      "Graph 25: 207 nodes\n",
      "Graph 26: 207 nodes\n",
      "Graph 27: 8 nodes\n",
      "Graph 28: 50 nodes\n",
      "Graph 29: 249 nodes\n",
      "Average loss at step 300: 235.295613\n",
      "Graph 30: 263 nodes\n",
      "Time: 0.569179058075\n",
      "Graph 31: 237 nodes\n",
      "Graph 32: 248 nodes\n",
      "Graph 33: 250 nodes\n",
      "Graph 34: 13 nodes\n",
      "Graph 35: 45 nodes\n",
      "Graph 36: 333 nodes\n",
      "Graph 37: 266 nodes\n",
      "Graph 38: 241 nodes\n",
      "Graph 39: 235 nodes\n",
      "Average loss at step 400: 211.649718\n",
      "Graph 40: 244 nodes\n",
      "Time: 0.718559026718\n",
      "Graph 41: 15 nodes\n",
      "Graph 42: 42 nodes\n",
      "Graph 43: 257 nodes\n",
      "Graph 44: 242 nodes\n",
      "Graph 45: 260 nodes\n",
      "Graph 46: 236 nodes\n",
      "Graph 47: 263 nodes\n",
      "Graph 48: 10 nodes\n",
      "Graph 49: 21 nodes\n",
      "Average loss at step 500: 148.818047\n",
      "Graph 50: 236 nodes\n",
      "Time: 0.46209192276\n",
      "Graph 51: 266 nodes\n",
      "Graph 52: 295 nodes\n",
      "Graph 53: 270 nodes\n",
      "Graph 54: 249 nodes\n",
      "Graph 55: 10 nodes\n",
      "Graph 56: 48 nodes\n",
      "Graph 57: 308 nodes\n",
      "Graph 58: 300 nodes\n",
      "Graph 59: 340 nodes\n",
      "Average loss at step 600: 132.938907\n",
      "Graph 60: 303 nodes\n",
      "Time: 0.599319934845\n",
      "Graph 61: 316 nodes\n",
      "Graph 62: 31 nodes\n",
      "Graph 63: 39 nodes\n",
      "Graph 64: 381 nodes\n",
      "Graph 65: 367 nodes\n",
      "Graph 66: 352 nodes\n",
      "Graph 67: 321 nodes\n",
      "Graph 68: 290 nodes\n",
      "Graph 69: 13 nodes\n",
      "Average loss at step 700: 113.251860\n",
      "Graph 70: 20 nodes\n",
      "Time: 0.801142930984\n",
      "Graph 71: 44 nodes\n",
      "Graph 72: 302 nodes\n",
      "Graph 73: 340 nodes\n",
      "Graph 74: 336 nodes\n",
      "Graph 75: 330 nodes\n",
      "Graph 76: 28 nodes\n",
      "Graph 77: 62 nodes\n",
      "Graph 78: 351 nodes\n",
      "Graph 79: 304 nodes\n",
      "Average loss at step 800: 130.317491\n",
      "Graph 80: 318 nodes\n",
      "Time: 0.592924118042\n",
      "Graph 81: 315 nodes\n",
      "Graph 82: 304 nodes\n",
      "Graph 83: 26 nodes\n",
      "Graph 84: 53 nodes\n",
      "Graph 85: 339 nodes\n",
      "Graph 86: 329 nodes\n",
      "Graph 87: 357 nodes\n",
      "Graph 88: 349 nodes\n",
      "Graph 89: 315 nodes\n",
      "Average loss at step 900: 120.218543\n",
      "Graph 90: 21 nodes\n",
      "Time: 0.376842021942\n",
      "Graph 91: 44 nodes\n",
      "Graph 92: 364 nodes\n",
      "Graph 93: 367 nodes\n",
      "Graph 94: 391 nodes\n",
      "Graph 95: 323 nodes\n",
      "Graph 96: 305 nodes\n",
      "Graph 97: 24 nodes\n",
      "Graph 98: 53 nodes\n",
      "Graph 99: 377 nodes\n",
      "Average loss at step 1000: 108.740857\n",
      "Graph 100: 355 nodes\n",
      "Time: 0.558574914932\n",
      "Graph 101: 328 nodes\n",
      "Graph 102: 308 nodes\n",
      "Graph 103: 290 nodes\n",
      "Graph 104: 28 nodes\n",
      "Graph 105: 68 nodes\n",
      "Graph 106: 356 nodes\n",
      "Graph 107: 380 nodes\n",
      "Graph 108: 386 nodes\n",
      "Graph 109: 397 nodes\n",
      "Average loss at step 1100: 109.482390\n",
      "Graph 110: 341 nodes\n",
      "Time: 0.65504193306\n",
      "Graph 111: 32 nodes\n",
      "Graph 112: 48 nodes\n",
      "Graph 113: 343 nodes\n",
      "Graph 114: 410 nodes\n",
      "Graph 115: 351 nodes\n",
      "Graph 116: 334 nodes\n",
      "Graph 117: 335 nodes\n",
      "Graph 118: 30 nodes\n",
      "Graph 119: 42 nodes\n",
      "Average loss at step 1200: 94.182399\n",
      "Graph 120: 392 nodes\n",
      "Time: 0.735352039337\n",
      "Graph 121: 440 nodes\n",
      "Graph 122: 439 nodes\n",
      "Graph 123: 396 nodes\n",
      "Graph 124: 423 nodes\n",
      "Graph 125: 29 nodes\n",
      "Graph 126: 77 nodes\n",
      "Graph 127: 423 nodes\n",
      "Graph 128: 407 nodes\n",
      "Graph 129: 484 nodes\n",
      "Average loss at step 1300: 122.873484\n",
      "Graph 130: 434 nodes\n",
      "Time: 0.734303951263\n",
      "Graph 131: 358 nodes\n",
      "Graph 132: 43 nodes\n",
      "Graph 133: 63 nodes\n",
      "Graph 134: 484 nodes\n",
      "Graph 135: 499 nodes\n",
      "Graph 136: 419 nodes\n",
      "Graph 137: 523 nodes\n",
      "Graph 138: 508 nodes\n",
      "Graph 139: 38 nodes\n",
      "Average loss at step 1400: 105.492258\n",
      "Graph 140: 110 nodes\n",
      "Time: 0.621059179306\n",
      "Graph 141: 588 nodes\n",
      "Graph 142: 563 nodes\n",
      "Graph 143: 564 nodes\n",
      "Graph 144: 514 nodes\n",
      "Graph 145: 420 nodes\n",
      "Graph 146: 64 nodes\n",
      "Graph 147: 105 nodes\n",
      "Graph 148: 506 nodes\n",
      "Graph 149: 480 nodes\n",
      "Average loss at step 1500: 95.783273\n",
      "Graph 150: 410 nodes\n",
      "Time: 0.86288690567\n",
      "Graph 151: 49 nodes\n",
      "Graph 152: 54 nodes\n",
      "Graph 153: 30 nodes\n",
      "Graph 154: 132 nodes\n",
      "Graph 155: 607 nodes\n",
      "Graph 156: 682 nodes\n",
      "Graph 157: 720 nodes\n",
      "Graph 158: 630 nodes\n",
      "Graph 159: 568 nodes\n",
      "Average loss at step 1600: 89.071948\n",
      "Graph 160: 62 nodes\n",
      "Time: 0.827445030212\n",
      "Graph 161: 131 nodes\n",
      "Graph 162: 689 nodes\n",
      "Graph 163: 623 nodes\n",
      "Graph 164: 707 nodes\n",
      "Graph 165: 650 nodes\n",
      "Graph 166: 625 nodes\n",
      "Graph 167: 73 nodes\n",
      "Graph 168: 171 nodes\n",
      "Graph 169: 808 nodes\n",
      "Average loss at step 1700: 93.597006\n",
      "Graph 170: 897 nodes\n",
      "Time: 0.880839109421\n",
      "Graph 171: 958 nodes\n",
      "Graph 172: 397 nodes\n",
      "Graph 173: 339 nodes\n",
      "Graph 174: 18 nodes\n",
      "Graph 175: 59 nodes\n",
      "Graph 176: 378 nodes\n",
      "Graph 177: 374 nodes\n",
      "Graph 178: 375 nodes\n",
      "Graph 179: 317 nodes\n",
      "Average loss at step 1800: 104.495161\n",
      "Graph 180: 244 nodes\n",
      "Time: 0.628254175186\n",
      "Graph 181: 17 nodes\n",
      "Graph 182: 10 nodes\n",
      "Graph 183: 19 nodes\n",
      "Graph 184: 90 nodes\n",
      "Graph 185: 210 nodes\n",
      "Graph 186: 226 nodes\n",
      "Graph 187: 162 nodes\n",
      "Graph 188: 10 nodes\n",
      "Graph 189: 7 nodes\n",
      "Average loss at step 1900: 60.784049\n",
      "Graph 190: 62 nodes\n",
      "Time: 0.643234014511\n",
      "Graph 191: 349 nodes\n",
      "Graph 192: 341 nodes\n",
      "Graph 193: 365 nodes\n",
      "Graph 194: 334 nodes\n",
      "Graph 195: 21 nodes\n",
      "Graph 196: 70 nodes\n",
      "Graph 197: 364 nodes\n",
      "Graph 198: 339 nodes\n",
      "Graph 199: 344 nodes\n",
      "Average loss at step 2000: 100.717502\n",
      "Graph 200: 341 nodes\n",
      "Time: 0.751102924347\n",
      "Graph 201: 328 nodes\n",
      "Graph 202: 56 nodes\n",
      "Graph 203: 37 nodes\n",
      "Graph 204: 108 nodes\n",
      "Graph 205: 385 nodes\n",
      "Graph 206: 380 nodes\n",
      "Graph 207: 315 nodes\n",
      "Graph 208: 348 nodes\n",
      "Graph 209: 44 nodes\n",
      "Average loss at step 2100: 88.565771\n",
      "Graph 210: 78 nodes\n",
      "Time: 0.62127494812\n",
      "Graph 211: 411 nodes\n",
      "Graph 212: 415 nodes\n",
      "Graph 213: 429 nodes\n",
      "Graph 214: 378 nodes\n",
      "Graph 215: 405 nodes\n",
      "Graph 216: 33 nodes\n",
      "Graph 217: 44 nodes\n",
      "Graph 218: 407 nodes\n",
      "Graph 219: 397 nodes\n",
      "Average loss at step 2200: 95.076512\n",
      "Graph 220: 441 nodes\n",
      "Time: 0.486532211304\n",
      "Graph 221: 393 nodes\n",
      "Graph 222: 353 nodes\n",
      "Graph 223: 29 nodes\n",
      "Graph 224: 79 nodes\n",
      "Graph 225: 424 nodes\n",
      "Graph 226: 483 nodes\n",
      "Graph 227: 432 nodes\n",
      "Graph 228: 349 nodes\n",
      "Graph 229: 396 nodes\n",
      "Average loss at step 2300: 101.686082\n",
      "Graph 230: 17 nodes\n",
      "Time: 0.555427074432\n",
      "Graph 231: 69 nodes\n",
      "Graph 232: 381 nodes\n",
      "Graph 233: 358 nodes\n",
      "Graph 234: 393 nodes\n",
      "Graph 235: 399 nodes\n",
      "Graph 236: 330 nodes\n",
      "Graph 237: 23 nodes\n",
      "Graph 238: 35 nodes\n",
      "Graph 239: 111 nodes\n",
      "Average loss at step 2400: 80.009502\n",
      "Graph 240: 453 nodes\n",
      "Time: 0.497457027435\n",
      "Graph 241: 448 nodes\n",
      "Graph 242: 439 nodes\n",
      "Graph 243: 383 nodes\n",
      "Graph 244: 19 nodes\n",
      "Graph 245: 76 nodes\n",
      "Graph 246: 422 nodes\n",
      "Graph 247: 409 nodes\n",
      "Graph 248: 439 nodes\n",
      "Graph 249: 386 nodes\n",
      "Average loss at step 2500: 103.459974\n",
      "Graph 250: 349 nodes\n",
      "Time: 0.536628007889\n",
      "Graph 251: 22 nodes\n",
      "Graph 252: 71 nodes\n",
      "Graph 253: 396 nodes\n",
      "Graph 254: 422 nodes\n",
      "Graph 255: 434 nodes\n",
      "Graph 256: 444 nodes\n",
      "Graph 257: 366 nodes\n",
      "Graph 258: 31 nodes\n",
      "Graph 259: 35 nodes\n",
      "Average loss at step 2600: 87.591544\n",
      "Graph 260: 454 nodes\n",
      "Time: 0.560186862946\n",
      "Graph 261: 387 nodes\n",
      "Graph 262: 427 nodes\n",
      "Graph 263: 440 nodes\n",
      "Graph 264: 353 nodes\n",
      "Graph 265: 19 nodes\n",
      "Graph 266: 77 nodes\n",
      "Graph 267: 482 nodes\n",
      "Graph 268: 493 nodes\n",
      "Graph 269: 478 nodes\n",
      "Average loss at step 2700: 108.250794\n",
      "Graph 270: 422 nodes\n",
      "Time: 1.15059494972\n",
      "Graph 271: 485 nodes\n",
      "Graph 272: 35 nodes\n",
      "Graph 273: 107 nodes\n",
      "Graph 274: 414 nodes\n",
      "Graph 275: 402 nodes\n",
      "Graph 276: 461 nodes\n",
      "Graph 277: 414 nodes\n",
      "Graph 278: 390 nodes\n",
      "Graph 279: 37 nodes\n",
      "Average loss at step 2800: 98.935818\n",
      "Graph 280: 52 nodes\n",
      "Time: 1.47071385384\n",
      "Graph 281: 472 nodes\n",
      "Graph 282: 466 nodes\n",
      "Graph 283: 460 nodes\n",
      "Graph 284: 455 nodes\n",
      "Graph 285: 380 nodes\n",
      "Graph 286: 35 nodes\n",
      "Graph 287: 68 nodes\n",
      "Graph 288: 568 nodes\n",
      "Graph 289: 523 nodes\n",
      "Average loss at step 2900: 92.158857\n",
      "Graph 290: 506 nodes\n",
      "Time: 1.54833102226\n",
      "Graph 291: 514 nodes\n",
      "Graph 292: 252 nodes\n",
      "Graph 293: 29 nodes\n",
      "Graph 294: 77 nodes\n",
      "Graph 295: 509 nodes\n",
      "Graph 296: 584 nodes\n",
      "Graph 297: 611 nodes\n",
      "Graph 298: 625 nodes\n",
      "Graph 299: 562 nodes\n",
      "Average loss at step 3000: 103.360489\n",
      "Graph 300: 59 nodes\n",
      "Time: 0.40024805069\n",
      "Graph 301: 83 nodes\n",
      "Graph 302: 589 nodes\n",
      "Graph 303: 672 nodes\n",
      "Graph 304: 599 nodes\n",
      "Graph 305: 564 nodes\n",
      "Graph 306: 567 nodes\n",
      "Graph 307: 68 nodes\n",
      "Graph 308: 114 nodes\n",
      "Graph 309: 678 nodes\n",
      "Average loss at step 3100: 98.025728\n",
      "Graph 310: 644 nodes\n",
      "Time: 0.89471912384\n",
      "Graph 311: 610 nodes\n",
      "Graph 312: 443 nodes\n",
      "Graph 313: 545 nodes\n",
      "Graph 314: 48 nodes\n",
      "Graph 315: 87 nodes\n",
      "Graph 316: 708 nodes\n",
      "Graph 317: 660 nodes\n",
      "Graph 318: 674 nodes\n",
      "Graph 319: 679 nodes\n",
      "Average loss at step 3200: 95.864210\n",
      "Graph 320: 664 nodes\n",
      "Time: 0.679609060287\n",
      "Graph 321: 52 nodes\n",
      "Graph 322: 94 nodes\n",
      "Graph 323: 684 nodes\n",
      "Graph 324: 668 nodes\n",
      "Graph 325: 689 nodes\n",
      "Graph 326: 719 nodes\n",
      "Graph 327: 517 nodes\n",
      "Graph 328: 50 nodes\n",
      "Graph 329: 91 nodes\n",
      "Average loss at step 3300: 84.806329\n",
      "Graph 330: 714 nodes\n",
      "Time: 0.853281021118\n",
      "Graph 331: 749 nodes\n",
      "Graph 332: 729 nodes\n",
      "Graph 333: 747 nodes\n",
      "Graph 334: 608 nodes\n",
      "Graph 335: 47 nodes\n",
      "Graph 336: 70 nodes\n",
      "Graph 337: 106 nodes\n",
      "Graph 338: 798 nodes\n",
      "Graph 339: 802 nodes\n",
      "Average loss at step 3400: 99.766628\n",
      "Graph 340: 878 nodes\n",
      "Time: 0.895530939102\n",
      "Graph 341: 795 nodes\n",
      "Graph 342: 88 nodes\n",
      "Graph 343: 130 nodes\n",
      "Graph 344: 879 nodes\n",
      "Graph 345: 925 nodes\n",
      "Graph 346: 947 nodes\n",
      "Graph 347: 945 nodes\n",
      "Graph 348: 670 nodes\n",
      "Graph 349: 22 nodes\n",
      "Average loss at step 3500: 98.129822\n",
      "Graph 350: 56 nodes\n",
      "Time: 0.522953987122\n",
      "Graph 351: 387 nodes\n",
      "Graph 352: 294 nodes\n",
      "Graph 353: 294 nodes\n",
      "Graph 354: 289 nodes\n",
      "Graph 355: 269 nodes\n",
      "Graph 356: 43 nodes\n",
      "Graph 357: 45 nodes\n",
      "Graph 358: 338 nodes\n",
      "Graph 359: 306 nodes\n",
      "Average loss at step 3600: 75.831922\n",
      "Graph 360: 322 nodes\n",
      "Time: 0.614063024521\n",
      "Graph 361: 342 nodes\n",
      "Graph 362: 251 nodes\n",
      "Graph 363: 16 nodes\n",
      "Graph 364: 35 nodes\n",
      "Graph 365: 326 nodes\n",
      "Graph 366: 589 nodes\n",
      "Graph 367: 617 nodes\n",
      "Graph 368: 290 nodes\n",
      "Graph 369: 272 nodes\n",
      "Average loss at step 3700: 80.276429\n",
      "Graph 370: 23 nodes\n",
      "Time: 0.377179861069\n",
      "Graph 371: 43 nodes\n",
      "Graph 372: 326 nodes\n",
      "Graph 373: 272 nodes\n",
      "Graph 374: 55 nodes\n",
      "Graph 375: 305 nodes\n",
      "Graph 376: 229 nodes\n",
      "Graph 377: 26 nodes\n",
      "Graph 378: 59 nodes\n",
      "Graph 379: 335 nodes\n",
      "Average loss at step 3800: 84.852488\n",
      "Graph 380: 319 nodes\n",
      "Time: 0.782419919968\n",
      "Graph 381: 338 nodes\n",
      "Graph 382: 320 nodes\n",
      "Graph 383: 317 nodes\n",
      "Graph 384: 21 nodes\n",
      "Graph 385: 29 nodes\n",
      "Graph 386: 345 nodes\n",
      "Graph 387: 353 nodes\n",
      "Graph 388: 343 nodes\n",
      "Graph 389: 349 nodes\n",
      "Average loss at step 3900: 90.215089\n",
      "Graph 390: 320 nodes\n",
      "Time: 0.605185985565\n",
      "Graph 391: 23 nodes\n",
      "Graph 392: 34 nodes\n",
      "Graph 393: 354 nodes\n",
      "Graph 394: 342 nodes\n",
      "Graph 395: 332 nodes\n",
      "Graph 396: 458 nodes\n",
      "Graph 397: 351 nodes\n",
      "Graph 398: 33 nodes\n",
      "Graph 399: 39 nodes\n",
      "Average loss at step 4000: 87.736741\n",
      "Graph 400: 453 nodes\n",
      "Time: 0.749502897263\n",
      "Graph 401: 382 nodes\n",
      "Graph 402: 369 nodes\n",
      "Graph 403: 326 nodes\n",
      "Graph 404: 286 nodes\n",
      "Graph 405: 12 nodes\n",
      "Graph 406: 32 nodes\n",
      "Graph 407: 351 nodes\n",
      "Graph 408: 361 nodes\n",
      "Graph 409: 309 nodes\n",
      "Average loss at step 4100: 89.353898\n",
      "Graph 410: 372 nodes\n",
      "Time: 1.13155508041\n",
      "Graph 411: 338 nodes\n",
      "Graph 412: 18 nodes\n",
      "Graph 413: 86 nodes\n",
      "Graph 414: 411 nodes\n",
      "Graph 415: 417 nodes\n",
      "Graph 416: 348 nodes\n",
      "Graph 417: 338 nodes\n",
      "Graph 418: 292 nodes\n",
      "Graph 419: 44 nodes\n",
      "Average loss at step 4200: 88.936861\n",
      "Graph 420: 54 nodes\n",
      "Time: 0.493660926819\n",
      "Graph 421: 416 nodes\n",
      "Graph 422: 398 nodes\n",
      "Graph 423: 405 nodes\n",
      "Graph 424: 380 nodes\n",
      "Graph 425: 342 nodes\n",
      "Graph 426: 45 nodes\n",
      "Graph 427: 56 nodes\n",
      "Graph 428: 459 nodes\n",
      "Graph 429: 453 nodes\n",
      "Average loss at step 4300: 96.200816\n",
      "Graph 430: 415 nodes\n",
      "Time: 0.490213155746\n",
      "Graph 431: 420 nodes\n",
      "Graph 432: 335 nodes\n",
      "Graph 433: 25 nodes\n",
      "Graph 434: 20 nodes\n",
      "Graph 435: 54 nodes\n",
      "Graph 436: 423 nodes\n",
      "Graph 437: 431 nodes\n",
      "Graph 438: 417 nodes\n",
      "Graph 439: 370 nodes\n",
      "Average loss at step 4400: 80.562219\n",
      "Graph 440: 42 nodes\n",
      "Time: 0.517857074738\n",
      "Graph 441: 37 nodes\n",
      "Graph 442: 494 nodes\n",
      "Graph 443: 173 nodes\n",
      "Graph 444: 435 nodes\n",
      "Graph 445: 445 nodes\n",
      "Graph 446: 438 nodes\n",
      "Graph 447: 52 nodes\n",
      "Graph 448: 52 nodes\n",
      "Graph 449: 492 nodes\n",
      "Average loss at step 4500: 77.752867\n",
      "Graph 450: 534 nodes\n",
      "Time: 0.969798088074\n",
      "Graph 451: 584 nodes\n",
      "Graph 452: 496 nodes\n",
      "Graph 453: 508 nodes\n",
      "Graph 454: 40 nodes\n",
      "Graph 455: 65 nodes\n",
      "Graph 456: 580 nodes\n",
      "Graph 457: 690 nodes\n",
      "Graph 458: 672 nodes\n",
      "Graph 459: 709 nodes\n",
      "Average loss at step 4600: 94.166192\n",
      "Graph 460: 615 nodes\n",
      "Time: 0.692560911179\n",
      "Graph 461: 39 nodes\n",
      "Graph 462: 81 nodes\n",
      "Graph 463: 715 nodes\n",
      "Graph 464: 716 nodes\n",
      "Graph 465: 624 nodes\n",
      "Graph 466: 620 nodes\n",
      "Graph 467: 594 nodes\n",
      "Graph 468: 46 nodes\n",
      "Graph 469: 90 nodes\n",
      "Average loss at step 4700: 93.458322\n",
      "Graph 470: 715 nodes\n",
      "Time: 0.962829113007\n",
      "Graph 471: 770 nodes\n",
      "Graph 472: 742 nodes\n",
      "Graph 473: 707 nodes\n",
      "Graph 474: 724 nodes\n",
      "Graph 475: 86 nodes\n",
      "Graph 476: 115 nodes\n",
      "Graph 477: 1057 nodes\n",
      "Graph 478: 1055 nodes\n",
      "Graph 479: 1216 nodes\n",
      "Average loss at step 4800: 114.784976\n",
      "Graph 480: 1426 nodes\n",
      "Time: 0.740460157394\n",
      "Graph 481: 1211 nodes\n",
      "Graph 482: 201 nodes\n",
      "Graph 483: 237 nodes\n",
      "Graph 484: 1410 nodes\n",
      "Graph 485: 1418 nodes\n",
      "Graph 486: 1457 nodes\n",
      "Graph 487: 1537 nodes\n",
      "Graph 488: 1374 nodes\n",
      "Graph 489: 233 nodes\n",
      "Average loss at step 4900: 119.002636\n",
      "Graph 490: 287 nodes\n",
      "Time: 0.510400772095\n",
      "Graph 491: 1470 nodes\n",
      "Graph 492: 721 nodes\n",
      "Graph 493: 728 nodes\n",
      "Graph 494: 693 nodes\n",
      "Graph 495: 688 nodes\n",
      "Graph 496: 78 nodes\n",
      "Graph 497: 81 nodes\n",
      "Graph 498: 757 nodes\n",
      "Graph 499: 811 nodes\n",
      "Average loss at step 5000: 117.423540\n",
      "Graph 500: 752 nodes\n",
      "Time: 0.552163124084\n",
      "Graph 501: 678 nodes\n",
      "Graph 502: 616 nodes\n",
      "Graph 503: 44 nodes\n",
      "Graph 504: 82 nodes\n",
      "Graph 505: 777 nodes\n",
      "Graph 506: 867 nodes\n",
      "Graph 507: 947 nodes\n",
      "Graph 508: 969 nodes\n",
      "Graph 509: 1080 nodes\n",
      "Average loss at step 5100: 113.398879\n",
      "Graph 510: 246 nodes\n",
      "Time: 0.634778022766\n",
      "Graph 511: 317 nodes\n",
      "Graph 512: 1452 nodes\n",
      "Graph 513: 1425 nodes\n",
      "Graph 514: 1098 nodes\n",
      "Graph 515: 234 nodes\n",
      "Graph 516: 284 nodes\n",
      "Graph 517: 179 nodes\n",
      "Graph 518: 309 nodes\n",
      "Graph 519: 1449 nodes\n",
      "Average loss at step 5200: 110.778055\n",
      "Graph 520: 1345 nodes\n",
      "Time: 0.786997795105\n",
      "Graph 521: 491 nodes\n",
      "Graph 522: 508 nodes\n",
      "Graph 523: 504 nodes\n",
      "Graph 524: 75 nodes\n",
      "Graph 525: 109 nodes\n",
      "Graph 526: 446 nodes\n",
      "Graph 527: 457 nodes\n",
      "Graph 528: 412 nodes\n",
      "Graph 529: 405 nodes\n",
      "Average loss at step 5300: 106.709942\n",
      "Graph 530: 358 nodes\n",
      "Time: 0.656394958496\n",
      "Graph 531: 43 nodes\n",
      "Graph 532: 88 nodes\n",
      "Graph 533: 443 nodes\n",
      "Graph 534: 473 nodes\n",
      "Graph 535: 491 nodes\n",
      "Graph 536: 525 nodes\n",
      "Graph 537: 352 nodes\n",
      "Graph 538: 55 nodes\n",
      "Graph 539: 61 nodes\n",
      "Average loss at step 5400: 82.328118\n",
      "Graph 540: 506 nodes\n",
      "Time: 0.54048705101\n",
      "Graph 541: 355 nodes\n",
      "Graph 542: 635 nodes\n",
      "Graph 543: 589 nodes\n",
      "Graph 544: 542 nodes\n",
      "Graph 545: 137 nodes\n",
      "Graph 546: 136 nodes\n",
      "Graph 547: 202 nodes\n",
      "Graph 548: 111 nodes\n",
      "Graph 549: 438 nodes\n",
      "Average loss at step 5500: 104.673115\n",
      "Graph 550: 523 nodes\n",
      "Time: 0.557321071625\n",
      "Graph 551: 429 nodes\n",
      "Graph 552: 149 nodes\n",
      "Graph 553: 142 nodes\n",
      "Graph 554: 401 nodes\n",
      "Graph 555: 131 nodes\n",
      "Graph 556: 439 nodes\n",
      "Graph 557: 394 nodes\n",
      "Graph 558: 383 nodes\n",
      "Graph 559: 83 nodes\n",
      "Average loss at step 5600: 100.776066\n",
      "Graph 560: 66 nodes\n",
      "Time: 0.542157888412\n",
      "Graph 561: 487 nodes\n",
      "Graph 562: 525 nodes\n",
      "Graph 563: 628 nodes\n",
      "Graph 564: 640 nodes\n",
      "Graph 565: 566 nodes\n",
      "Graph 566: 114 nodes\n",
      "Graph 567: 131 nodes\n",
      "Graph 568: 582 nodes\n",
      "Graph 569: 590 nodes\n",
      "Average loss at step 5700: 100.605415\n",
      "Graph 570: 634 nodes\n",
      "Time: 0.54877114296\n",
      "Graph 571: 597 nodes\n",
      "Graph 572: 547 nodes\n",
      "Graph 573: 82 nodes\n",
      "Graph 574: 115 nodes\n",
      "Graph 575: 258 nodes\n",
      "Graph 576: 584 nodes\n",
      "Graph 577: 565 nodes\n",
      "Graph 578: 552 nodes\n",
      "Graph 579: 539 nodes\n",
      "Average loss at step 5800: 109.905722\n",
      "Graph 580: 106 nodes\n",
      "Time: 0.623659849167\n",
      "Graph 581: 138 nodes\n",
      "Graph 582: 573 nodes\n",
      "Graph 583: 697 nodes\n",
      "Graph 584: 1771 nodes\n",
      "Graph 585: 590 nodes\n",
      "Graph 586: 542 nodes\n",
      "Graph 587: 87 nodes\n",
      "Graph 588: 124 nodes\n",
      "Graph 589: 585 nodes\n",
      "Average loss at step 5900: 108.129812\n",
      "Graph 590: 637 nodes\n",
      "Time: 0.708341121674\n",
      "Graph 591: 638 nodes\n",
      "Graph 592: 241 nodes\n",
      "Graph 593: 150 nodes\n",
      "Graph 594: 21 nodes\n",
      "Graph 595: 28 nodes\n",
      "Graph 596: 175 nodes\n",
      "Graph 597: 223 nodes\n",
      "Graph 598: 210 nodes\n",
      "Graph 599: 196 nodes\n",
      "Average loss at step 6000: 109.081691\n",
      "Graph 600: 149 nodes\n",
      "Time: 0.648394823074\n",
      "Graph 601: 10 nodes\n",
      "Graph 602: 20 nodes\n",
      "Graph 603: 39 nodes\n",
      "Graph 604: 196 nodes\n",
      "Graph 605: 154 nodes\n",
      "Graph 606: 165 nodes\n",
      "Graph 607: 183 nodes\n",
      "Graph 608: 16 nodes\n",
      "Graph 609: 37 nodes\n",
      "Average loss at step 6100: 69.550187\n",
      "Graph 610: 220 nodes\n",
      "Time: 0.669919967651\n",
      "Graph 611: 188 nodes\n",
      "Graph 612: 206 nodes\n",
      "Graph 613: 203 nodes\n",
      "Graph 614: 163 nodes\n",
      "Graph 615: 12 nodes\n",
      "Graph 616: 25 nodes\n",
      "Graph 617: 149 nodes\n",
      "Graph 618: 142 nodes\n",
      "Graph 619: 159 nodes\n",
      "Average loss at step 6200: 103.365427\n",
      "Graph 620: 181 nodes\n",
      "Time: 0.730714797974\n",
      "Graph 621: 172 nodes\n",
      "Graph 622: 10 nodes\n",
      "Graph 623: 30 nodes\n",
      "Graph 624: 112 nodes\n",
      "Graph 625: 163 nodes\n",
      "Graph 626: 127 nodes\n",
      "Graph 627: 157 nodes\n",
      "Graph 628: 117 nodes\n",
      "Graph 629: 10 nodes\n",
      "Average loss at step 6300: 78.685917\n",
      "Graph 630: 16 nodes\n",
      "Time: 0.677660942078\n",
      "Graph 631: 121 nodes\n",
      "Graph 632: 140 nodes\n",
      "Graph 633: 162 nodes\n",
      "Graph 634: 137 nodes\n",
      "Graph 635: 116 nodes\n",
      "Graph 636: 14 nodes\n",
      "Epoch: 1\n",
      "Graph 0: 20 nodes\n",
      "Time: 0.601312160492\n",
      "Graph 1: 277 nodes\n",
      "Graph 2: 274 nodes\n",
      "Average loss at step 6400: 91.855940\n",
      "Graph 3: 278 nodes\n",
      "Graph 4: 293 nodes\n",
      "Graph 5: 262 nodes\n",
      "Graph 6: 9 nodes\n",
      "Graph 7: 8 nodes\n",
      "Graph 8: 31 nodes\n",
      "Graph 9: 32 nodes\n",
      "Graph 10: 264 nodes\n",
      "Time: 1.01634001732\n",
      "Graph 11: 229 nodes\n",
      "Graph 12: 230 nodes\n",
      "Average loss at step 6500: 78.699862\n",
      "Graph 13: 18 nodes\n",
      "Graph 14: 24 nodes\n",
      "Graph 15: 306 nodes\n",
      "Graph 16: 262 nodes\n",
      "Graph 17: 229 nodes\n",
      "Graph 18: 277 nodes\n",
      "Graph 19: 200 nodes\n",
      "Graph 20: 16 nodes\n",
      "Time: 0.406750917435\n",
      "Graph 21: 12 nodes\n",
      "Graph 22: 232 nodes\n",
      "Average loss at step 6600: 79.328303\n",
      "Graph 23: 245 nodes\n",
      "Graph 24: 252 nodes\n",
      "Graph 25: 207 nodes\n",
      "Graph 26: 207 nodes\n",
      "Graph 27: 8 nodes\n",
      "Graph 28: 50 nodes\n",
      "Graph 29: 249 nodes\n",
      "Graph 30: 263 nodes\n",
      "Time: 0.488411903381\n",
      "Graph 31: 237 nodes\n",
      "Graph 32: 248 nodes\n",
      "Average loss at step 6700: 99.677965\n",
      "Graph 33: 250 nodes\n",
      "Graph 34: 13 nodes\n",
      "Graph 35: 45 nodes\n",
      "Graph 36: 333 nodes\n",
      "Graph 37: 266 nodes\n",
      "Graph 38: 241 nodes\n",
      "Graph 39: 235 nodes\n",
      "Graph 40: 244 nodes\n",
      "Time: 0.673891067505\n",
      "Graph 41: 15 nodes\n",
      "Graph 42: 42 nodes\n",
      "Average loss at step 6800: 86.292279\n",
      "Graph 43: 257 nodes\n",
      "Graph 44: 242 nodes\n",
      "Graph 45: 260 nodes\n",
      "Graph 46: 236 nodes\n",
      "Graph 47: 263 nodes\n",
      "Graph 48: 10 nodes\n",
      "Graph 49: 21 nodes\n",
      "Graph 50: 236 nodes\n",
      "Time: 0.835323095322\n",
      "Graph 51: 266 nodes\n",
      "Graph 52: 295 nodes\n",
      "Average loss at step 6900: 90.303076\n",
      "Graph 53: 270 nodes\n",
      "Graph 54: 249 nodes\n",
      "Graph 55: 10 nodes\n",
      "Graph 56: 48 nodes\n",
      "Graph 57: 308 nodes\n",
      "Graph 58: 300 nodes\n",
      "Graph 59: 340 nodes\n",
      "Graph 60: 303 nodes\n",
      "Time: 0.780642032623\n",
      "Graph 61: 316 nodes\n",
      "Graph 62: 31 nodes\n",
      "Average loss at step 7000: 83.722056\n",
      "Graph 63: 39 nodes\n",
      "Graph 64: 381 nodes\n",
      "Graph 65: 367 nodes\n",
      "Graph 66: 352 nodes\n",
      "Graph 67: 321 nodes\n",
      "Graph 68: 290 nodes\n",
      "Graph 69: 13 nodes\n",
      "Graph 70: 20 nodes\n",
      "Time: 1.15641188622\n",
      "Graph 71: 44 nodes\n",
      "Graph 72: 302 nodes\n",
      "Average loss at step 7100: 79.374839\n",
      "Graph 73: 340 nodes\n",
      "Graph 74: 336 nodes\n",
      "Graph 75: 330 nodes\n",
      "Graph 76: 28 nodes\n",
      "Graph 77: 62 nodes\n",
      "Graph 78: 351 nodes\n",
      "Graph 79: 304 nodes\n",
      "Graph 80: 318 nodes\n",
      "Time: 0.661153078079\n",
      "Graph 81: 315 nodes\n",
      "Graph 82: 304 nodes\n",
      "Average loss at step 7200: 104.585860\n",
      "Graph 83: 26 nodes\n",
      "Graph 84: 53 nodes\n",
      "Graph 85: 339 nodes\n",
      "Graph 86: 329 nodes\n",
      "Graph 87: 357 nodes\n",
      "Graph 88: 349 nodes\n",
      "Graph 89: 315 nodes\n",
      "Graph 90: 21 nodes\n",
      "Time: 0.527670860291\n",
      "Graph 91: 44 nodes\n",
      "Graph 92: 364 nodes\n",
      "Average loss at step 7300: 91.808390\n",
      "Graph 93: 367 nodes\n",
      "Graph 94: 391 nodes\n",
      "Graph 95: 323 nodes\n",
      "Graph 96: 305 nodes\n",
      "Graph 97: 24 nodes\n",
      "Graph 98: 53 nodes\n",
      "Graph 99: 377 nodes\n",
      "Graph 100: 355 nodes\n",
      "Time: 0.540710926056\n",
      "Graph 101: 328 nodes\n",
      "Graph 102: 308 nodes\n",
      "Average loss at step 7400: 92.118676\n",
      "Graph 103: 290 nodes\n",
      "Graph 104: 28 nodes\n",
      "Graph 105: 68 nodes\n",
      "Graph 106: 356 nodes\n",
      "Graph 107: 380 nodes\n",
      "Graph 108: 386 nodes\n",
      "Graph 109: 397 nodes\n",
      "Graph 110: 341 nodes\n",
      "Time: 1.34823513031\n",
      "Graph 111: 32 nodes\n",
      "Graph 112: 48 nodes\n",
      "Average loss at step 7500: 90.842282\n",
      "Graph 113: 343 nodes\n",
      "Graph 114: 410 nodes\n",
      "Graph 115: 351 nodes\n",
      "Graph 116: 334 nodes\n",
      "Graph 117: 335 nodes\n",
      "Graph 118: 30 nodes\n",
      "Graph 119: 42 nodes\n",
      "Graph 120: 392 nodes\n",
      "Time: 0.754309892654\n",
      "Graph 121: 440 nodes\n",
      "Graph 122: 439 nodes\n",
      "Average loss at step 7600: 93.817073\n",
      "Graph 123: 396 nodes\n",
      "Graph 124: 423 nodes\n",
      "Graph 125: 29 nodes\n",
      "Graph 126: 77 nodes\n",
      "Graph 127: 423 nodes\n",
      "Graph 128: 407 nodes\n",
      "Graph 129: 484 nodes\n",
      "Graph 130: 434 nodes\n",
      "Time: 0.559201955795\n",
      "Graph 131: 358 nodes\n",
      "Graph 132: 43 nodes\n",
      "Average loss at step 7700: 103.545417\n",
      "Graph 133: 63 nodes\n",
      "Graph 134: 484 nodes\n",
      "Graph 135: 499 nodes\n",
      "Graph 136: 419 nodes\n",
      "Graph 137: 523 nodes\n",
      "Graph 138: 508 nodes\n",
      "Graph 139: 38 nodes\n",
      "Graph 140: 110 nodes\n",
      "Time: 0.547811985016\n",
      "Graph 141: 588 nodes\n",
      "Graph 142: 563 nodes\n",
      "Average loss at step 7800: 96.585587\n",
      "Graph 143: 564 nodes\n",
      "Graph 144: 514 nodes\n",
      "Graph 145: 420 nodes\n",
      "Graph 146: 64 nodes\n",
      "Graph 147: 105 nodes\n",
      "Graph 148: 506 nodes\n",
      "Graph 149: 480 nodes\n",
      "Graph 150: 410 nodes\n",
      "Time: 0.65066409111\n",
      "Graph 151: 49 nodes\n",
      "Graph 152: 54 nodes\n",
      "Average loss at step 7900: 75.963042\n",
      "Graph 153: 30 nodes\n",
      "Graph 154: 132 nodes\n",
      "Graph 155: 607 nodes\n",
      "Graph 156: 682 nodes\n",
      "Graph 157: 720 nodes\n",
      "Graph 158: 630 nodes\n",
      "Graph 159: 568 nodes\n",
      "Graph 160: 62 nodes\n",
      "Time: 0.459021806717\n",
      "Graph 161: 131 nodes\n",
      "Graph 162: 689 nodes\n",
      "Average loss at step 8000: 92.103459\n",
      "Graph 163: 623 nodes\n",
      "Graph 164: 707 nodes\n",
      "Graph 165: 650 nodes\n",
      "Graph 166: 625 nodes\n",
      "Graph 167: 73 nodes\n",
      "Graph 168: 171 nodes\n",
      "Graph 169: 808 nodes\n",
      "Graph 170: 897 nodes\n",
      "Time: 0.876003026962\n",
      "Graph 171: 958 nodes\n",
      "Graph 172: 397 nodes\n",
      "Average loss at step 8100: 98.457473\n",
      "Graph 173: 339 nodes\n",
      "Graph 174: 18 nodes\n",
      "Graph 175: 59 nodes\n",
      "Graph 176: 378 nodes\n",
      "Graph 177: 374 nodes\n",
      "Graph 178: 375 nodes\n",
      "Graph 179: 317 nodes\n",
      "Graph 180: 244 nodes\n",
      "Time: 0.479350090027\n",
      "Graph 181: 17 nodes\n",
      "Graph 182: 10 nodes\n",
      "Average loss at step 8200: 76.067907\n",
      "Graph 183: 19 nodes\n",
      "Graph 184: 90 nodes\n",
      "Graph 185: 210 nodes\n",
      "Graph 186: 226 nodes\n",
      "Graph 187: 162 nodes\n",
      "Graph 188: 10 nodes\n",
      "Graph 189: 7 nodes\n",
      "Graph 190: 62 nodes\n",
      "Time: 0.4366979599\n",
      "Graph 191: 349 nodes\n",
      "Graph 192: 341 nodes\n",
      "Average loss at step 8300: 76.548908\n",
      "Graph 193: 365 nodes\n",
      "Graph 194: 334 nodes\n",
      "Graph 195: 21 nodes\n",
      "Graph 196: 70 nodes\n",
      "Graph 197: 364 nodes\n",
      "Graph 198: 339 nodes\n",
      "Graph 199: 344 nodes\n",
      "Graph 200: 341 nodes\n",
      "Time: 0.495707988739\n",
      "Graph 201: 328 nodes\n",
      "Graph 202: 56 nodes\n",
      "Average loss at step 8400: 91.463418\n",
      "Graph 203: 37 nodes\n",
      "Graph 204: 108 nodes\n",
      "Graph 205: 385 nodes\n",
      "Graph 206: 380 nodes\n",
      "Graph 207: 315 nodes\n",
      "Graph 208: 348 nodes\n",
      "Graph 209: 44 nodes\n",
      "Graph 210: 78 nodes\n",
      "Time: 0.499396085739\n",
      "Graph 211: 411 nodes\n",
      "Graph 212: 415 nodes\n",
      "Average loss at step 8500: 89.976723\n",
      "Graph 213: 429 nodes\n",
      "Graph 214: 378 nodes\n",
      "Graph 215: 405 nodes\n",
      "Graph 216: 33 nodes\n",
      "Graph 217: 44 nodes\n",
      "Graph 218: 407 nodes\n",
      "Graph 219: 397 nodes\n",
      "Graph 220: 441 nodes\n",
      "Time: 0.661988019943\n",
      "Graph 221: 393 nodes\n",
      "Graph 222: 353 nodes\n",
      "Average loss at step 8600: 89.363182\n",
      "Graph 223: 29 nodes\n",
      "Graph 224: 79 nodes\n",
      "Graph 225: 424 nodes\n",
      "Graph 226: 483 nodes\n",
      "Graph 227: 432 nodes\n",
      "Graph 228: 349 nodes\n",
      "Graph 229: 396 nodes\n",
      "Graph 230: 17 nodes\n",
      "Time: 0.425852060318\n",
      "Graph 231: 69 nodes\n",
      "Graph 232: 381 nodes\n",
      "Average loss at step 8700: 87.845108\n",
      "Graph 233: 358 nodes\n",
      "Graph 234: 393 nodes\n",
      "Graph 235: 399 nodes\n",
      "Graph 236: 330 nodes\n",
      "Graph 237: 23 nodes\n",
      "Graph 238: 35 nodes\n",
      "Graph 239: 111 nodes\n",
      "Graph 240: 453 nodes\n",
      "Time: 0.767635822296\n",
      "Graph 241: 448 nodes\n",
      "Graph 242: 439 nodes\n",
      "Average loss at step 8800: 91.955586\n",
      "Graph 243: 383 nodes\n",
      "Graph 244: 19 nodes\n",
      "Graph 245: 76 nodes\n",
      "Graph 246: 422 nodes\n",
      "Graph 247: 409 nodes\n",
      "Graph 248: 439 nodes\n",
      "Graph 249: 386 nodes\n",
      "Graph 250: 349 nodes\n",
      "Time: 0.654533863068\n",
      "Graph 251: 22 nodes\n",
      "Graph 252: 71 nodes\n",
      "Average loss at step 8900: 86.682747\n",
      "Graph 253: 396 nodes\n",
      "Graph 254: 422 nodes\n",
      "Graph 255: 434 nodes\n",
      "Graph 256: 444 nodes\n",
      "Graph 257: 366 nodes\n",
      "Graph 258: 31 nodes\n",
      "Graph 259: 35 nodes\n",
      "Graph 260: 454 nodes\n",
      "Time: 0.555490016937\n",
      "Graph 261: 387 nodes\n",
      "Graph 262: 427 nodes\n",
      "Average loss at step 9000: 94.211292\n",
      "Graph 263: 440 nodes\n",
      "Graph 264: 353 nodes\n",
      "Graph 265: 19 nodes\n",
      "Graph 266: 77 nodes\n",
      "Graph 267: 482 nodes\n",
      "Graph 268: 493 nodes\n",
      "Graph 269: 478 nodes\n",
      "Graph 270: 422 nodes\n",
      "Time: 0.599510908127\n",
      "Graph 271: 485 nodes\n",
      "Graph 272: 35 nodes\n",
      "Average loss at step 9100: 95.887218\n",
      "Graph 273: 107 nodes\n",
      "Graph 274: 414 nodes\n",
      "Graph 275: 402 nodes\n",
      "Graph 276: 461 nodes\n",
      "Graph 277: 414 nodes\n",
      "Graph 278: 390 nodes\n",
      "Graph 279: 37 nodes\n",
      "Graph 280: 52 nodes\n",
      "Time: 0.55246090889\n",
      "Graph 281: 472 nodes\n",
      "Graph 282: 466 nodes\n",
      "Average loss at step 9200: 96.880485\n",
      "Graph 283: 460 nodes\n",
      "Graph 284: 455 nodes\n",
      "Graph 285: 380 nodes\n",
      "Graph 286: 35 nodes\n",
      "Graph 287: 68 nodes\n",
      "Graph 288: 568 nodes\n",
      "Graph 289: 523 nodes\n",
      "Graph 290: 506 nodes\n",
      "Time: 0.788402080536\n",
      "Graph 291: 514 nodes\n",
      "Graph 292: 252 nodes\n",
      "Average loss at step 9300: 94.172949\n",
      "Graph 293: 29 nodes\n",
      "Graph 294: 77 nodes\n",
      "Graph 295: 509 nodes\n",
      "Graph 296: 584 nodes\n",
      "Graph 297: 611 nodes\n",
      "Graph 298: 625 nodes\n",
      "Graph 299: 562 nodes\n",
      "Graph 300: 59 nodes\n",
      "Time: 0.445019006729\n",
      "Graph 301: 83 nodes\n",
      "Graph 302: 589 nodes\n",
      "Average loss at step 9400: 87.220507\n",
      "Graph 303: 672 nodes\n",
      "Graph 304: 599 nodes\n",
      "Graph 305: 564 nodes\n",
      "Graph 306: 567 nodes\n",
      "Graph 307: 68 nodes\n",
      "Graph 308: 114 nodes\n",
      "Graph 309: 678 nodes\n",
      "Graph 310: 644 nodes\n",
      "Time: 0.937523841858\n",
      "Graph 311: 610 nodes\n",
      "Graph 312: 443 nodes\n",
      "Average loss at step 9500: 108.055434\n",
      "Graph 313: 545 nodes\n",
      "Graph 314: 48 nodes\n",
      "Graph 315: 87 nodes\n",
      "Graph 316: 708 nodes\n",
      "Graph 317: 660 nodes\n",
      "Graph 318: 674 nodes\n",
      "Graph 319: 679 nodes\n",
      "Graph 320: 664 nodes\n",
      "Time: 0.799067974091\n",
      "Graph 321: 52 nodes\n",
      "Graph 322: 94 nodes\n",
      "Average loss at step 9600: 85.867320\n",
      "Graph 323: 684 nodes\n",
      "Graph 324: 668 nodes\n",
      "Graph 325: 689 nodes\n",
      "Graph 326: 719 nodes\n",
      "Graph 327: 517 nodes\n",
      "Graph 328: 50 nodes\n",
      "Graph 329: 91 nodes\n",
      "Graph 330: 714 nodes\n",
      "Time: 0.707864999771\n",
      "Graph 331: 749 nodes\n",
      "Graph 332: 729 nodes\n",
      "Average loss at step 9700: 99.805317\n",
      "Graph 333: 747 nodes\n",
      "Graph 334: 608 nodes\n",
      "Graph 335: 47 nodes\n",
      "Graph 336: 70 nodes\n",
      "Graph 337: 106 nodes\n",
      "Graph 338: 798 nodes\n",
      "Graph 339: 802 nodes\n",
      "Graph 340: 878 nodes\n",
      "Time: 0.862045049667\n",
      "Graph 341: 795 nodes\n",
      "Graph 342: 88 nodes\n",
      "Average loss at step 9800: 82.518393\n",
      "Graph 343: 130 nodes\n",
      "Graph 344: 879 nodes\n",
      "Graph 345: 925 nodes\n",
      "Graph 346: 947 nodes\n",
      "Graph 347: 945 nodes\n",
      "Graph 348: 670 nodes\n",
      "Graph 349: 22 nodes\n",
      "Graph 350: 56 nodes\n",
      "Time: 0.705442905426\n",
      "Graph 351: 387 nodes\n",
      "Graph 352: 294 nodes\n",
      "Average loss at step 9900: 88.244320\n",
      "Graph 353: 294 nodes\n",
      "Graph 354: 289 nodes\n",
      "Graph 355: 269 nodes\n",
      "Graph 356: 43 nodes\n",
      "Graph 357: 45 nodes\n",
      "Graph 358: 338 nodes\n",
      "Graph 359: 306 nodes\n",
      "Graph 360: 322 nodes\n",
      "Time: 0.647174835205\n",
      "Graph 361: 342 nodes\n",
      "Graph 362: 251 nodes\n",
      "Average loss at step 10000: 81.276769\n",
      "Graph 363: 16 nodes\n",
      "Graph 364: 35 nodes\n",
      "Graph 365: 326 nodes\n",
      "Graph 366: 589 nodes\n",
      "Graph 367: 617 nodes\n",
      "Graph 368: 290 nodes\n",
      "Graph 369: 272 nodes\n",
      "Graph 370: 23 nodes\n",
      "Time: 0.626009941101\n",
      "Graph 371: 43 nodes\n",
      "Graph 372: 326 nodes\n",
      "Average loss at step 10100: 70.526676\n",
      "Graph 373: 272 nodes\n",
      "Graph 374: 55 nodes\n",
      "Graph 375: 305 nodes\n",
      "Graph 376: 229 nodes\n",
      "Graph 377: 26 nodes\n",
      "Graph 378: 59 nodes\n",
      "Graph 379: 335 nodes\n",
      "Graph 380: 319 nodes\n",
      "Time: 0.613540172577\n",
      "Graph 381: 338 nodes\n",
      "Graph 382: 320 nodes\n",
      "Average loss at step 10200: 93.332070\n",
      "Graph 383: 317 nodes\n",
      "Graph 384: 21 nodes\n",
      "Graph 385: 29 nodes\n",
      "Graph 386: 345 nodes\n",
      "Graph 387: 353 nodes\n",
      "Graph 388: 343 nodes\n",
      "Graph 389: 349 nodes\n",
      "Graph 390: 320 nodes\n",
      "Time: 0.648753881454\n",
      "Graph 391: 23 nodes\n",
      "Graph 392: 34 nodes\n",
      "Average loss at step 10300: 81.022673\n",
      "Graph 393: 354 nodes\n",
      "Graph 394: 342 nodes\n",
      "Graph 395: 332 nodes\n",
      "Graph 396: 458 nodes\n",
      "Graph 397: 351 nodes\n",
      "Graph 398: 33 nodes\n",
      "Graph 399: 39 nodes\n",
      "Graph 400: 453 nodes\n",
      "Time: 0.658799886703\n",
      "Graph 401: 382 nodes\n",
      "Graph 402: 369 nodes\n",
      "Average loss at step 10400: 96.574331\n",
      "Graph 403: 326 nodes\n",
      "Graph 404: 286 nodes\n",
      "Graph 405: 12 nodes\n",
      "Graph 406: 32 nodes\n",
      "Graph 407: 351 nodes\n",
      "Graph 408: 361 nodes\n",
      "Graph 409: 309 nodes\n",
      "Graph 410: 372 nodes\n",
      "Time: 0.829944849014\n",
      "Graph 411: 338 nodes\n",
      "Graph 412: 18 nodes\n",
      "Average loss at step 10500: 78.813106\n",
      "Graph 413: 86 nodes\n",
      "Graph 414: 411 nodes\n",
      "Graph 415: 417 nodes\n",
      "Graph 416: 348 nodes\n",
      "Graph 417: 338 nodes\n",
      "Graph 418: 292 nodes\n",
      "Graph 419: 44 nodes\n",
      "Graph 420: 54 nodes\n",
      "Time: 0.561131000519\n",
      "Graph 421: 416 nodes\n",
      "Graph 422: 398 nodes\n",
      "Average loss at step 10600: 90.746623\n",
      "Graph 423: 405 nodes\n",
      "Graph 424: 380 nodes\n",
      "Graph 425: 342 nodes\n",
      "Graph 426: 45 nodes\n",
      "Graph 427: 56 nodes\n",
      "Graph 428: 459 nodes\n",
      "Graph 429: 453 nodes\n",
      "Graph 430: 415 nodes\n",
      "Time: 0.63062286377\n",
      "Graph 431: 420 nodes\n",
      "Graph 432: 335 nodes\n",
      "Average loss at step 10700: 94.873144\n",
      "Graph 433: 25 nodes\n",
      "Graph 434: 20 nodes\n",
      "Graph 435: 54 nodes\n",
      "Graph 436: 423 nodes\n",
      "Graph 437: 431 nodes\n",
      "Graph 438: 417 nodes\n",
      "Graph 439: 370 nodes\n",
      "Graph 440: 42 nodes\n",
      "Time: 0.440535068512\n",
      "Graph 441: 37 nodes\n",
      "Graph 442: 494 nodes\n",
      "Average loss at step 10800: 66.155149\n",
      "Graph 443: 173 nodes\n",
      "Graph 444: 435 nodes\n",
      "Graph 445: 445 nodes\n",
      "Graph 446: 438 nodes\n",
      "Graph 447: 52 nodes\n",
      "Graph 448: 52 nodes\n",
      "Graph 449: 492 nodes\n",
      "Graph 450: 534 nodes\n",
      "Time: 0.738555908203\n",
      "Graph 451: 584 nodes\n",
      "Graph 452: 496 nodes\n",
      "Average loss at step 10900: 90.426680\n",
      "Graph 453: 508 nodes\n",
      "Graph 454: 40 nodes\n",
      "Graph 455: 65 nodes\n",
      "Graph 456: 580 nodes\n",
      "Graph 457: 690 nodes\n",
      "Graph 458: 672 nodes\n",
      "Graph 459: 709 nodes\n",
      "Graph 460: 615 nodes\n",
      "Time: 0.811681032181\n",
      "Graph 461: 39 nodes\n",
      "Graph 462: 81 nodes\n",
      "Average loss at step 11000: 81.433410\n",
      "Graph 463: 715 nodes\n",
      "Graph 464: 716 nodes\n",
      "Graph 465: 624 nodes\n",
      "Graph 466: 620 nodes\n",
      "Graph 467: 594 nodes\n",
      "Graph 468: 46 nodes\n",
      "Graph 469: 90 nodes\n",
      "Graph 470: 715 nodes\n",
      "Time: 0.742980003357\n",
      "Graph 471: 770 nodes\n",
      "Graph 472: 742 nodes\n",
      "Average loss at step 11100: 110.327756\n",
      "Graph 473: 707 nodes\n",
      "Graph 474: 724 nodes\n",
      "Graph 475: 86 nodes\n",
      "Graph 476: 115 nodes\n",
      "Graph 477: 1057 nodes\n",
      "Graph 478: 1055 nodes\n",
      "Graph 479: 1216 nodes\n",
      "Graph 480: 1426 nodes\n",
      "Time: 0.800853967667\n",
      "Graph 481: 1211 nodes\n",
      "Graph 482: 201 nodes\n",
      "Average loss at step 11200: 106.425568\n",
      "Graph 483: 237 nodes\n",
      "Graph 484: 1410 nodes\n",
      "Graph 485: 1418 nodes\n",
      "Graph 486: 1457 nodes\n",
      "Graph 487: 1537 nodes\n",
      "Graph 488: 1374 nodes\n",
      "Graph 489: 233 nodes\n",
      "Graph 490: 287 nodes\n",
      "Time: 0.613966941833\n",
      "Graph 491: 1470 nodes\n",
      "Graph 492: 721 nodes\n",
      "Average loss at step 11300: 118.064921\n",
      "Graph 493: 728 nodes\n",
      "Graph 494: 693 nodes\n",
      "Graph 495: 688 nodes\n",
      "Graph 496: 78 nodes\n",
      "Graph 497: 81 nodes\n",
      "Graph 498: 757 nodes\n",
      "Graph 499: 811 nodes\n",
      "Graph 500: 752 nodes\n",
      "Time: 0.694382190704\n",
      "Graph 501: 678 nodes\n",
      "Graph 502: 616 nodes\n",
      "Average loss at step 11400: 112.099416\n",
      "Graph 503: 44 nodes\n",
      "Graph 504: 82 nodes\n",
      "Graph 505: 777 nodes\n",
      "Graph 506: 867 nodes\n",
      "Graph 507: 947 nodes\n",
      "Graph 508: 969 nodes\n",
      "Graph 509: 1080 nodes\n",
      "Graph 510: 246 nodes\n",
      "Time: 0.512779951096\n",
      "Graph 511: 317 nodes\n",
      "Graph 512: 1452 nodes\n",
      "Average loss at step 11500: 105.543008\n",
      "Graph 513: 1425 nodes\n",
      "Graph 514: 1098 nodes\n",
      "Graph 515: 234 nodes\n",
      "Graph 516: 284 nodes\n",
      "Graph 517: 179 nodes\n",
      "Graph 518: 309 nodes\n",
      "Graph 519: 1449 nodes\n",
      "Graph 520: 1345 nodes\n",
      "Time: 0.893393039703\n",
      "Graph 521: 491 nodes\n",
      "Graph 522: 508 nodes\n",
      "Average loss at step 11600: 113.832037\n",
      "Graph 523: 504 nodes\n",
      "Graph 524: 75 nodes\n",
      "Graph 525: 109 nodes\n",
      "Graph 526: 446 nodes\n",
      "Graph 527: 457 nodes\n",
      "Graph 528: 412 nodes\n",
      "Graph 529: 405 nodes\n",
      "Graph 530: 358 nodes\n",
      "Time: 0.617276906967\n",
      "Graph 531: 43 nodes\n",
      "Graph 532: 88 nodes\n",
      "Average loss at step 11700: 89.110350\n",
      "Graph 533: 443 nodes\n",
      "Graph 534: 473 nodes\n",
      "Graph 535: 491 nodes\n",
      "Graph 536: 525 nodes\n",
      "Graph 537: 352 nodes\n",
      "Graph 538: 55 nodes\n",
      "Graph 539: 61 nodes\n",
      "Graph 540: 506 nodes\n",
      "Time: 0.576679944992\n",
      "Graph 541: 355 nodes\n",
      "Graph 542: 635 nodes\n",
      "Average loss at step 11800: 90.860578\n",
      "Graph 543: 589 nodes\n",
      "Graph 544: 542 nodes\n",
      "Graph 545: 137 nodes\n",
      "Graph 546: 136 nodes\n",
      "Graph 547: 202 nodes\n",
      "Graph 548: 111 nodes\n",
      "Graph 549: 438 nodes\n",
      "Graph 550: 523 nodes\n",
      "Time: 0.636262893677\n",
      "Graph 551: 429 nodes\n",
      "Graph 552: 149 nodes\n",
      "Average loss at step 11900: 105.247316\n",
      "Graph 553: 142 nodes\n",
      "Graph 554: 401 nodes\n",
      "Graph 555: 131 nodes\n",
      "Graph 556: 439 nodes\n",
      "Graph 557: 394 nodes\n",
      "Graph 558: 383 nodes\n",
      "Graph 559: 83 nodes\n",
      "Graph 560: 66 nodes\n",
      "Time: 0.510829925537\n",
      "Graph 561: 487 nodes\n",
      "Graph 562: 525 nodes\n",
      "Average loss at step 12000: 100.595610\n",
      "Graph 563: 628 nodes\n",
      "Graph 564: 640 nodes\n",
      "Graph 565: 566 nodes\n",
      "Graph 566: 114 nodes\n",
      "Graph 567: 131 nodes\n",
      "Graph 568: 582 nodes\n",
      "Graph 569: 590 nodes\n",
      "Graph 570: 634 nodes\n",
      "Time: 0.85432100296\n",
      "Graph 571: 597 nodes\n",
      "Graph 572: 547 nodes\n",
      "Average loss at step 12100: 102.319884\n",
      "Graph 573: 82 nodes\n",
      "Graph 574: 115 nodes\n",
      "Graph 575: 258 nodes\n",
      "Graph 576: 584 nodes\n",
      "Graph 577: 565 nodes\n",
      "Graph 578: 552 nodes\n",
      "Graph 579: 539 nodes\n",
      "Graph 580: 106 nodes\n",
      "Time: 0.772296905518\n",
      "Graph 581: 138 nodes\n",
      "Graph 582: 573 nodes\n",
      "Average loss at step 12200: 100.304422\n",
      "Graph 583: 697 nodes\n",
      "Graph 584: 1771 nodes\n",
      "Graph 585: 590 nodes\n",
      "Graph 586: 542 nodes\n",
      "Graph 587: 87 nodes\n",
      "Graph 588: 124 nodes\n",
      "Graph 589: 585 nodes\n",
      "Graph 590: 637 nodes\n",
      "Time: 0.704403162003\n",
      "Graph 591: 638 nodes\n",
      "Graph 592: 241 nodes\n",
      "Average loss at step 12300: 114.708639\n",
      "Graph 593: 150 nodes\n",
      "Graph 594: 21 nodes\n",
      "Graph 595: 28 nodes\n",
      "Graph 596: 175 nodes\n",
      "Graph 597: 223 nodes\n",
      "Graph 598: 210 nodes\n",
      "Graph 599: 196 nodes\n",
      "Graph 600: 149 nodes\n",
      "Time: 0.711974143982\n",
      "Graph 601: 10 nodes\n",
      "Graph 602: 20 nodes\n",
      "Average loss at step 12400: 82.410767\n",
      "Graph 603: 39 nodes\n",
      "Graph 604: 196 nodes\n",
      "Graph 605: 154 nodes\n",
      "Graph 606: 165 nodes\n",
      "Graph 607: 183 nodes\n",
      "Graph 608: 16 nodes\n",
      "Graph 609: 37 nodes\n",
      "Graph 610: 220 nodes\n",
      "Time: 0.60608792305\n",
      "Graph 611: 188 nodes\n",
      "Graph 612: 206 nodes\n",
      "Average loss at step 12500: 90.571790\n",
      "Graph 613: 203 nodes\n",
      "Graph 614: 163 nodes\n",
      "Graph 615: 12 nodes\n",
      "Graph 616: 25 nodes\n",
      "Graph 617: 149 nodes\n",
      "Graph 618: 142 nodes\n",
      "Graph 619: 159 nodes\n",
      "Graph 620: 181 nodes\n",
      "Time: 0.632217168808\n",
      "Graph 621: 172 nodes\n",
      "Graph 622: 10 nodes\n",
      "Average loss at step 12600: 90.271416\n",
      "Graph 623: 30 nodes\n",
      "Graph 624: 112 nodes\n",
      "Graph 625: 163 nodes\n",
      "Graph 626: 127 nodes\n",
      "Graph 627: 157 nodes\n",
      "Graph 628: 117 nodes\n",
      "Graph 629: 10 nodes\n",
      "Graph 630: 16 nodes\n",
      "Time: 0.560750961304\n",
      "Graph 631: 121 nodes\n",
      "Graph 632: 140 nodes\n",
      "Average loss at step 12700: 73.792502\n",
      "Graph 633: 162 nodes\n",
      "Graph 634: 137 nodes\n",
      "Graph 635: 116 nodes\n",
      "Graph 636: 14 nodes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AnonymousWalkEmbeddings.AWE at 0x1a329599d0>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'parsed_graphs'\n",
    "\n",
    "batch_size = 100\n",
    "window_size = 10\n",
    "embedding_size_w = 64\n",
    "embedding_size_d = 2\n",
    "num_samples = 32\n",
    "\n",
    "concat = False\n",
    "loss_type = 'sampled_softmax'\n",
    "optimize = 'Adagrad'\n",
    "learning_rate = 1.0\n",
    "root = '../enron/'\n",
    "ext = 'graphml'\n",
    "steps = 5\n",
    "epochs = 2\n",
    "batches_per_epoch = 10\n",
    "candidate_func = None\n",
    "graph_labels = None\n",
    "\n",
    "model2 = AnonymousWalkEmbeddings.AWE(dataset = dataset, batch_size = batch_size, window_size = window_size,\n",
    "                  embedding_size_w = embedding_size_w, embedding_size_d = embedding_size_d,\n",
    "                  num_samples = num_samples, concat = concat, loss_type = loss_type,\n",
    "                  optimize = optimize, learning_rate = learning_rate, root = root,\n",
    "                  ext = ext, steps = steps, epochs = epochs, batches_per_epoch = batches_per_epoch,\n",
    "                  candidate_func = candidate_func, graph_labels=graph_labels)\n",
    "\n",
    "model2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c06da90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shuffles = ['timestep_3.graphml', 'timestep_7.graphml', 'timestep_9.graphml',\n",
    "            'timestep_4.graphml', 'timestep_8.graphml', 'timestep_1.graphml', \n",
    "            'timestep_5.graphml', 'timestep_2.graphml', 'timestep_0.graphml', \n",
    "            'timestep_6.graphml']\n",
    "\n",
    "shuffles = map(lambda x: int(x[:-8].split('_')[1]), shuffles)\n",
    "\n",
    "fig = plt.figure()\n",
    "right_order_ge = np.concatenate((model2.graph_embeddings[shuffles], model2.graph_embeddings[10:]))\n",
    "norms = np.linalg.norm(right_order_ge, axis=1)\n",
    "\n",
    "plt.scatter(np.where(labels == 0)[0], norms[np.where(labels == 0)[0]], label='normal', c='b')\n",
    "plt.scatter(np.where(labels == 1)[0], norms[np.where(labels == 1)[0]], label='outlier', c='r')\n",
    "\n",
    "plt.xlabel(\"days\")\n",
    "plt.ylabel(\"$||X||$\")\n",
    "plt.title(\"Norm of embedding on time\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(ls='dashed')\n",
    "#fig.savefig('norms_series_shuffled.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Matrix norm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Посмотрим, что происходит с нормой матрицы слов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Embeddings characteristics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One class SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Попробуем поискать выбросы, не учитывая то, что в задаче имеется временная зависимость. Тут пробуем одноклассовый svm, в следующем пункте пробуем isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([122, 343, 345, 365, 412, 417, 420]),)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(labels == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTCAYhmEYHiYQDMMwDMAEgmEYhuHx/wNxH0w1TruIEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3552cbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start, stop = 1, 5000\n",
    "fig = plt.figure()\n",
    "cls = OneClassSVM(kernel='rbf', gamma=0.9)\n",
    "cls.fit(model.graph_embeddings[np.where(labels == 0)[0]][start:stop])\n",
    "plt.scatter(np.where(labels==0), \n",
    "            cls.decision_function(model.graph_embeddings)[np.where(labels==0)], c='b', label='normal')\n",
    "plt.scatter(np.where(labels==1), \n",
    "            cls.decision_function(model.graph_embeddings)[np.where(labels==1)], c='r', label='outlier')\n",
    "plt.grid()\n",
    "plt.title(\"One class SVM outlier score\")\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('days')\n",
    "plt.legend()\n",
    "fig.savefig('oneclass_svm.pdf', format='pdf')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXm0J1V57/15ejhAd6PYB9LKdBqD9y7x4pBuh9yYpAka\nccTci4nYEjRqmyZ6yUpMLq7OYHzDa/K+7QAmKn2dwHNiayBRYryXIOmOJsYBDIpIkGZoaCAaukU4\ntJEenvtHVXHq1NlVtat+9RvOOd/PWnv9fjXv/VTV/tbe+9l7m7sjhBBC9MqSYUdACCHEwkCCIoQQ\nohMkKEIIITpBgiKEEKITJChCCCE6QYIihBCiEyQoYqiY2U4ze2PLY082s2kzW9qHeP2Mmd2Wnv+V\nXZ9/VDCzd5jZZPq/b/YUiwMJimiNmd1lZi8Y1vXc/W53X+Xuh/pwuXcCf5ae/zN9OH+QXgQ24twb\nzGxP2fY+21MsAiQoQoSZAG5uc6CZLes4LiPPMNK8GO086khQRCeY2alm9g9m9kMze8DMPpXb9l/N\n7Ovptq+b2X8tOcdPmtnfm9ne9BxTZnZMuu0TwMnA36TVMr9rZmvNzLOMxcyON7OrzWyfme0yszfl\nzv0OM/u0mV1hZg+b2c1mtr4kHrcDT85d64iIc19pZpNm9hDwOjNbYmYXmdntaXo+bWar0/2PTPfd\na2YPpjZZY2YXAz8L/Fl63T8rid8r0vg/mJZonprb5mZ2am7542b2x2a2EvjfwPHpuafN7PjCeYv2\nfLyZfcTM7jeze9PzLE23vc7M/snM3mtm+4B3BOL5HDO73sweMrPvmdl7ctueb2ZfTtNwj5m9LnfN\nK8zs381st5n9npktqbqmmf2amd1iZj8ws2vMbCJkNzEA3F1BoVUA7gJekP7/JLCF5CPlSOD56frV\nwA+A84BlwLnp8ni6fSfwxvT/qcALgSOA44AvAu8LXS9dXgs4sCxd/gfgA+n1nwn8O3Bmuu0dwH8A\nLwGWAu8CvhKTtshzHwBemab/KOA3ga8AJ6bpuQz4ZLr/m4G/AVakcVkHPK5oj5J4/SfgkdROy4Hf\nBXYBY+l2B07N7f9x4I/T/xuAPYXzvQOYLLHnZ9J4rwR+Avga8OZ02+uAg8Bb0/t6VCCu/wycl/5f\nBTwv/X8y8HD6LCwHxoFnptuuAD4LHJ3G57vAG8qumdp8F/DUdN3vAV8e9ruxWINKKKIrDpBUEx3v\n7v/h7v+Yrn8pcJu7f8LdD7r7J4F/BV5ePIG773L3a939x+7+78B7gJ+PubiZnQQ8H/if6fVvBD5M\nImQZ/+jun/ekjeATwDM6PPc/u/tn3P2wu/+IRDS2uPsed/8xScZ9Tvr1f4AkEz3V3Q+5+w3u/lBM\nXIBfAf42tdMBYCtJxhos9bXFzNYALwZ+090fcffvA+8FXp3b7T53f396X38UOM0B4FQzO9bdp939\nK+n6jcAX3P2T7n7A3fe6+41p6edXgLe7+8PufhfwbmbbuXjNNwPvcvdb3P0g8P8Cz1QpZThIUERX\n/C5gwNfS6phfS9cfD+wu7LsbOKF4AjP7CTPbnlavPARMAsdGXv94YJ+7P1xxnX/L/d8PHBlZDx9z\n7nsKx0wAf51W6TwI3AIcAtaQiNk1wHYzu8/M/j8zWx4Rjywuj9nT3Q+n155jzx6ZICk93J9Lw2Uk\nJZWMYpqLvIGkRPWvabXey9L1JwG3B/Y/Fhhj9vMSY+dLcnHcR/Icdm0PEYEERXSCu/+bu7/J3Y8n\n+Wr8QFqXfx/JS5/nZODewGneRVLl8nR3fxzwWpLM4bHLVEThPmC1mR0dcZ2mxJy7GLd7gBe7+zG5\ncKS735t+lf+Ru59GUrJ4GfCrJecJxeUxe5qZkWTQWVz2k1SlZTyxIo5V3AP8GDg2F//HufvTYs/n\n7re5+7kkIvSnwJVpW849wE8GDnmAmZJuRoyd31yw81Hu/uWYRIpukaCITjCzV5nZieniD0he/EPA\n54H/ZGavMbNlZvYrwGnA5wKnORqYBh40sxOA3yls/x5JY/kc3P0e4MvAu9JG76eTfCFP9Zi0tuf+\nEHBxVvViZseZ2dnp/zPM7PS0iuchkkw0c9UtTWPKp4GXmtmZaanmt0ky/iwDvRF4jZktNbOzmF1l\n+D1g3MweH5Hm+4G/A95tZo9LnQx+0syiqiDTdL7WzI5LS1EPpqsPkdjtBWb2y+kzMW5mz0yrIj9N\nYrejU9v9FklJtYwPAW83s6el13y8mb0qNo6iWyQooiueDXzVzKaBq4EL3f1Od99L8gX+28Bekqqx\nl7n7A4Fz/BHwU8APgb8F/qqw/V3A76XVG28LHH8uSUPufcBfA3/o7tf2nLJ2576ExA5/Z2YPkzTQ\nPzfd9kTgShIxuYWkwX8yd9w5qcfSpcWTuvutJCW395N80b8ceLm7P5rucmG67kGStorP5I79VxLn\niTtSG87y8grwqyRVUN8h+Ui4EnhSzTF5zgJuTp+JS4BXp21Qd5M4R/w2SRXVjcy0Z72VxOngDuAf\ngb8APlp2AXf/a5LSz/a0mvTbJG0/YgiYuybYEkII0TsqoQghhOgECYoQQohOkKAIIYToBAmKEEKI\nTlhUg6sde+yxvnbt2lbHPvLII6xcubLbCM1TZIsZZIsZZIsZFpotbrjhhgfc/bi6/RaVoKxdu5br\nr7++1bE7d+5kw4YN3UZoniJbzCBbzCBbzLDQbGFmxdEugqjKSwghRCdIUIQQQnSCBEUIIUQnSFCE\nEEJ0ggRFCCFEJ0hQhBBCdIIERQghRCdIUIQQQnSCBEUIIUQnSFCEEEJ0ggRFCCFEJ0hQhBBCdIIE\nRQghRCdIUIQQQnTCUAXFzM4ys1vNbJeZXRTY/nNm9g0zO2hm5xS2HTKzG9Nw9eBiLYQQIsTQBMXM\nlgJ/DrwYOA0418xOK+x2N/A64C8Cp/iRuz8zDa/oVzynpmDtWrjhhuR3aqpfVxJCiPnNMEsozwF2\nufsd7v4osB04O7+Du9/l7t8CDg8jglNTsGkT7E6nltm9G177Wjj2WAmLEEIUMXcfzoWTKqyz3P2N\n6fJ5wHPd/S2BfT8OfM7dr8ytOwjcCBwE/sTdP1NynU3AJoA1a9as2759e3Qcb7oJHn00+X/iidPs\n2bNq1vbjjoOTT44+3YJhenqaVatW1e+4CJAtZpAtZlhotjjjjDNucPf1tTu6+1AC8Crgw7nl84D3\nl+z7ceCcwrrj098nA3cBP1l3zXXr1nkTzNwhCVu37njsfxbM3CcnG53SJyfdJyaSYycmmh8/CuzY\nsWPYURgZZIsZZIsZFpotgOs9Il8fZpXXHuCk3PKJwH2xB7v7fenvHcBO4FldRg7qSx/ucP75sGRJ\nuH0la3/Jtl9wwUwVmnvyu2mTqs+EEAuDYQrK14GnmNkpZjYGvBqI8tYysyeY2RHp/2OBnwG+03UE\nL74Yli+v3ufQoRlxeP3rZ8Qh3/6Sbf/Qh2D//tnH798PW7Z0HXMhhBg8QxMUdz8IvAW4BrgF+LS7\n32xm7zSzVwCY2bPNbA9J9dhlZnZzevhTgevN7JvADpI2lM4FZeNGGBuL3//AAbjwwuT/li1zxaOs\nueruu9vFTwghRollw7y4u38e+Hxh3R/k/n+dpCqseNyXgdP7HkHgkUea7b93b/LbRCQWY8O+EGLh\noZ7yfWBqqlwkzGYvr1iRVK0JIcR8R4JSw/h482POOy9pMwmJx6//OkxMJNsmJmDbtqRqTQgh5jsS\nlBouuaRZOwrMtJXk20zGxxPx+MAH4K674PDh5LeNmBS9x+QlJoQYBSQoNWzcCB/9aHNRKbJvX9LL\nftmypHRS/I0VhpD3mFyPhRCjgAQlgo0b4fTTkyqqtmSllUOHwr+xwhDyHstcj1VyEUIMEwlKAy6+\nOGkH6Rf79ycdJYtCcMEFMyWZbFyxIlk/mHzJJd8vpg0SKCFEEyQoDSk2tHfNoUOzSyoXXAAf/OBM\nSaYqXgcOzF6X7xfTlH5WrUmohFiYSFAi2bcvyVCb9ktpQ773/LZt9fuvWFHeaTLrF9OUqqq1Xrjg\nghkvOLUBCbGwkKBEcu+9czPYfpJ1jKwqmeRdj/t1/dj1MUxNJcPPFMVv//7eSlIq7QgxGkhQIsmG\nsR8US5YkmePSpeHtS5fOdj0u6y9T1Y+mKjMu65jZS6/+LVuqS1JNxaCqWk5CIxuIwSNBiaTKbXjF\nivoOkOPjcxv0s/aYULvMoUNJo3qZoGzaNHs51F9mbCxZHyKUGZ93XlIlBWEHhF579deVbppWp5VV\ny114YVz7z0LOcBebe/lCvpfzipgx7hdKaDofSp6rrtrhK1b4nDlRxseTOU0mJz24PR9Wrkz2z+ZS\nqdq3LCxZ4r5580y8Jidnzplth/q5ViYmwufPz/FSNndL07kesvPUpc2s/NjQ/DFNbTgxMfu8xfu1\nYkXz+WlGdd6LMnvnbdA1w7JFV/eyS0b1uWgLkfOhDD2TH2ToRVB27NhROzlWMXMPhbGx+n2aZIrL\nl4evUfcyVWXGdZlOk5clRmjLrluXUcSIVJlgdZXhDjMTrXoWy+5vSLS7Yli2GIZ41iFBWQShV0GJ\noWkm1zTkM4QqYcpeprKMp07UqmaUDNmi7Dqx9gh9UdZlFCHBiRXKrjLcYWQcMV/ki6mEMgzxrEOC\nsgjCIASlbVVWbFi6NLlGjCCUZTybN4dLNqGwfHlyrbxQFEtr4+NJqSiUwdVl8FXCFZNRFIWsKi39\nyHDLnot+TvUcE/dhVAOphDKDBGURhH4JSj7zWLq0PpMeH49rbylm0l2KUttjzdwvuyzcnhR6oXt5\n2dscW3bM+Pjce9avNpR+Z+axX+RlopZv08qehS5EL7NFP8U0hNpQ+o8EJRD6IShN2ghgpn2jrqE6\nexF7aW/pV9i6dUe0+PTysrc5tskxXWR8oeei7J519cXci0hXPa+9ZsJZyXUYmfugRawOCcoiCP0Q\nlLpqlvzXZOYRlqeuiqzfVWj9FJS6dpwYmhzby5d31XWqthWfi8nJ6nvZaxqz/dtm2nXPay+it2PH\njpGsfhoGbQRl1EQxjwQlEPohKFUZfsxLPoolkC4EZdBVDlWZbLG9p9gmVHdslbt48bmoK3U2iXdd\nettkPjEfMG3ZsWPHSDaQD4M2rvWjVm2XR4ISCMMooVR9mZW5/TYJS5bMZCqDEqeQoIQa77sgNuOs\najupqpJcsaK6iqrq/q5YkfRPylOVYYfiPugv+rpnRCWUbmgqKKNut1hBUU/5Hqkb0r6qd/iFF84d\nIbgJY2NwxRXwiU8ky3v39n805CLZeGIf+xg88EBvM1EWadLbu8zOe/dWj8G2f3/5AJp33119//bv\nT8Z4y1M2NM34eNgm/RgzrYypKXjoofLtvY6EAP0ZYWExMMjnoJ9IUHpk48ZkcMayIVLKMpipqfYj\nAWcsXw5vfnMyE2Q2T4p7t6KyYgWsWhXeNjHRu4BUDZkRO+Lx1FRyfNecfHL92GXFMd7KMtSyIXDK\nzp+N5daEuuFHtmwp/4DJBhnt9UMgex8mJmYPXtrFB8ZCph9j5w2FmGLMQgn97IdSVwdarLrpd/VU\nL67B+XOE2hG2bt3RSf1unc2qqo+y4WfqvJZi7ByqFsuuHepjkw/ve9/c56KJc0BXXlcxdfC9tm8U\nvQ7zTiaTk+6XXrpjJBuUMwbZ6K02lEUQ+t2xscrvv4lr8SiE4sOcT9ull+7o5EGvqzeua5/avLl8\nnzIxLEtnlYv28uVJv6DQtne/O2yLpu7LZR8AsXXoMXXwvbobV7X3mc1uW2sqhv3O6AedYcvLaxGE\nQfSUD9Hv4VjKMtSql78u0wy5OMfYoslLUVUCKeuBX0xj0970IS+vbJ8q4cmOK67funVHMENumnn3\nWnqItUO/3I0zW7QRqq4z+tAzOOhGb/VDWQRhWILSS1+S2Gqb4jGbN1dnkEWvrND+ZS92k06eIdfd\nupe8GM82dsvEJvZLr63ob926I5jpNxWI2B7+TeOfldTy96gf7sYhQakaIie7btcZfdkzWBbnfrkz\nS1AWQRi1Esr4ePell2K9dmy/iJhe+3W2KBO+sjaKM88cTFVgTE/5mPOESn1ZCaVYZZZNIxCbUZZV\nKcWMHJ0d348e8LHCHxKUfMfWsg+N1iWzEoWqEtYm96NXJChDCMBZwK3ALuCiwPafA74BHATOKWw7\nH7gtDefHXG9YgrJ589wXJ7bxuerlqKq+yb9rZefJv7R1ccjHt2z8qqrjz2XS72TCD2F+JxN+LpOP\niUqX4hGbcbRt1yoe85737IgebLMuY+91yJYu2mJ6sVFZG0pVKaRJCSV7rl/DpD9iYYWq62hcdz+6\naseQoAw4AEuB24EnA2PAN4HTCvusBZ4OXJEXFGA1cEf6+4T0/xPqrjkMQQm9lGazJ8lqUkqpyxjK\nrld3rpg4ZPuHbFF1/LlM+jSzIzXNCj+XyShvtKxKrm5fs/i2hDZecEuXzjgCZBnOVVeVd+bLjonN\nnLroZd5lT/WmpedMUIpprYpTbBtKfr87CUfsnqUTlaJcJxZdtudIUAYcgJ8Grsktvx14e8m+Hy8I\nyrnAZbnly4Bz6645DEGpKoJXtV9UZZptrlcMxaqU2K/RiYmZap585lp1zN1LwpG6k4na6zWdJ6Vq\nW5UnV8jOdfHJXGV7uV+x6RrkOTKalpzL3Mnr4hRTKsif4xDhiB3CfPny8qkUBmm7xSoow+zYeAJw\nT255T7qu38cOlLKerocOJY/r7t1w+eVw/vlJJ7A6Tj453IEtW5d1cKzDffZyvkNaGWYz59+9Gz74\nwZle7GWMj8OJh8NGOJm7SzuEZhx11Nx1Vb2xy0Yu2L076QBa15l0YiJJzyc+Ee6smnWszHrxFzs2\nFlm9On6u87pe5jHzpnfZU71Np7pQx9O6OG3cmHSOzUZ8OO+8uenLv0d3E47Y3ZzMgQNw9NHtOlaW\nvaux79SgiXkeBo15VW7QzwubvQp4kbu/MV0+D3iOu781sO/Hgc+5+5Xp8u8AR7j7H6fLvw/sd/d3\nB47dBGwCWLNmzbrt27e3iu/09DSryrqMV3DTTfWZDiTDqJx+Ouzbl7xcodtiBscem2SKhw/PXg/V\nGXvVNYvs25e8RPlr5DnxxGn27Km3xZIlyQu9+t6wER5ljH877vQ56Qlx3HGzM7h9+5JhTx59NEnH\nCSckmXe27Z574ODB2iiG45ue54YbyvcdG0uuXWcLs9n3pXiNImXpCt2T0LmKaV+2DE46qfx6VZQ9\nB0uWhO9X3hbr1sWlq+5akOx/+PBMmlazjwl2s4SZnQ+zhN1MsI/VwevHUPWunnJKMxu2zS9iiX0e\nuuKMM864wd3X1+4YU4zpR2CRVHnFViUV6/iLVTOZ91aXXmFV1TFV1UN1ow3PqboIGOERW+Ff2jz5\n2OasyqOsbSOrb4+lqZ2K7rVV58hX88UO5d+2CqWJp14/+nSEHEpC1Up5W7SpIqq7X8Vrhpw88vey\n7VQJMe2N+f3Lquoa5RctPAEG3a+GedCGsoykMf0UZhrln1ayb1FQVgN3kjTIPyH9v7rumsPy8orJ\nMGMfhC7nR6lrqCx7aKsy0WIGl537reOT/vB4yYUi09fkZWlip7JMt8o7L7NNG0GJ6Z+Rbav7GMmf\nqx+ZTJXLe/GDo5cheWLu18qVM+/P0qVxbudN4xN7z0IfW3WekEFafAVUeVT2q1/NyAtKEkdeAnyX\nxNtrS7runcAr0v/PJmkfeQTYC9ycO/bXSNyNdwGvj7nesAQlT69fkV2VUKo6P2alobKXvCwTrRvG\nJCadMW7OTRtxq8L4+FzPrSwNVd55Wb+RXkoodddo4nnn3q2HV13JKHQvehmSp81znT3DXX2sVcUj\n1qmjyhOy8QUD1H1kLNoSyjDCKAiKe2++7lUZfUyI7aledY1QJlqVETd54Ddvrj62iZtpVd+QfHyL\n1TdjY/V9QiYnk86LTQUlpn9GVr3XpG+Qe3XH0ibPV4wnXOg+9lqKb9MvqGtBLXu+Yj0xs2tF26Ik\n0ocJR7rqne3n2GQSlEAYFUHplboHOvZLra0wVQ0CWPelWdduU9fbu+4LMi/UmzeXj46b0XRYm2Lf\niTpBqZp4rMr+4+PVfWVCHyJ1glL2IZO3a8wz0XRInqr7XbxfTeJRfJ6qqufq+p+E4tF0mKD8O9Zr\nCWW3TQRtXGWXfomJu0tQQmGhCEqMX3/MV3zTaobsZasaprwuI6gqoZRliPkG8157Qhdpkv4s/nm7\n1bUnlV2/befKqjRV2aZsrLYzz2z2YVGVpiaDhtaNHVfcP2YUgdBzX9cvJfZdibVRmzaUYq//rNNv\n6F0ZdGN8hgQlEBaKoMS8BDHVak2qGfJ15nlBKX7NVX3xV2WGsQ2NZS9UWeZcV91Tl3mH4p/PXMoE\npeoFb1u9U5ept63CbHLdKpoMGtrEm6rsHDFDp9QJUWwGHfPxlX/WmuQXrynxWAuV5vvhyReDBCUQ\nFoqguPfWDlM8T2ydefYwV32Vl7mUZi9cWTx7dY2tinvWVhKyV1U1UdkxMSWUqqq9Lhwr6oYn6TrE\nZFpl70iT9NZViTapuqpqh8quE9vuUmfbon2a5BdNSx2hdHaVH5QhQQmEhSQoXVNVj97UVTY/inLV\noJh5YuuGiwI4Pl4/wGRotOO8R1pZ433Zi5l3J25TQukqkw9do6qk1zZUfQjkKXtHmlaptaHsQ2PV\nqvI0uTfLzPOZdtnArHW2iI17VdtbbNq7FBUJSiBIUOIo+wLKHtYYz6amDegxHkplL14vGWU+vRAW\nwHy1XnHSr5At6qr2uuxL1LTqKx/q4hHq7FlF0xJK7MdGDLHXyMKSJeH72SYeofclGzQ0tsRQFKsm\ncRpEu4oEJRAkKO0oZuQxghLT4awoDHUvUVOPrKYZctNMKW+L2N7ZXVR31WU0MVVfWf+bLiefajrx\nWsibqi29iHST0kBM2pYvT6aGbitSTQWirjNwFyUVCUogSFDiyX8xFRu8Y/telDWUVzWgl2Uw/ajK\nKb6kbTKlzBaxmW+XpZOqjKbO1TXvEdjVHCpdTQ3d5riyj42yic56TWtGmY3L+mrF0LQ/Td2HVhfV\nXxKUQJCgxFH3hdukM1+TBvR+N2TXXbPNNZqOX1XXV6LXNBSJOaarOviu35Em8SrLVFeubD6WXhOa\njCYRauivc/rIh7IPh5hq316rv2IFZZjD14sRZcuWZBjyGCYmkiHqy7ZlQ+LnhxIvGyK/bLj0qalm\nQ4hnoy/HkL9maJj1mHM1GR6+bCj3Sy5JRpmOmcKgyJIl4aHLp6bK459Pd37qgqZDvveT0HMYGh4f\nktF3Q+zfPzttZdMlNBmqPz9s/JIGOWj+Gtn0B9n0D7t3J8tTU/CSl8w9dmws/Ixt2QIHDtRfe/fu\nAQ1xH6M6CyWohBJHXbVMcRDApl+4TfZv6grbpGdzTD+GsnaGrCqlavyq0LmyeGXVTKGqvTauv6F+\nSHUdJ8vi0Jau35EmVT+xX/W9lsZi7k9MG0pVSTVU4gjNFtrGwaNt9Req8pKgtKWq82DZIIBN68hj\n92/bh6HuxW/iwVTVV+d97wsLSkzGUyWiZe1XdWIae+2uMpo8de9Ik2ekadtOE9fbkOt5rw3mRaeM\nUi+v1AihYfdjnu+q5abPSRMkKIGw2AUl9oWu+4obpC166cNQ1ZDftM68am6YXoa1qRO2JuKQpalt\nW0zdmFd1VLnKdlkqzUqHoWtk11+5cu5x+b5HbUspsaWmHTt2zCopL12azOFSNsxKm/tVFjIh7erZ\nd3cJSigsZkFpUy1VljnE2GLz5tlzV2RDsjclNnMsS0sXPvpVwlTWKN/067HqKznWwy2LQ1eeZFVj\naxXjunnz3IbofAbepLTRRBBD7seheWzy13vreHiokxjX79jn6aqrdsx53+4kfPCdTDyWll5d49s2\n7tchQQmExSwoXT5cdbYoG4K+jajEfqGXZchNv4ybeN3kBaVY3dZm4Meq/hB1mWzMsPi9ZFB1Azma\nhT2bQqMU5EN+jLjM9k3j2KQq6DVM+jT1pYQmpafQvpdeOtcWhwhH7BA2qwNx2866McPFqA2lw7CY\nBaWpb3sVdbao6n/Shnxmk58pMLandUxVX9XLF9OxsZf2i6qQVe+Eek/n72FerMvSEjunR/HcdR8j\n2fa2k411bbOqcM/ScGKyUkIofU2ep2xbyBZlJZTihUJtPDH3rqo/Uq+dRyUogbCYBWWQJZSqh74r\nukxP3fnqSij9Kh0Uw/Ll5R31Qu1HoYwkX68fUxqoKjUUB1hsO9lYP21WjO/hilJCaP+mjgRZph+y\nxbmB0lGTIkPVvev3iMMSlEBYzILSZfF30CWUEF2WuKrOB+Gvw2z/osdbP3rCx2aWTYmtRutHCSV2\njpsu7bN5c3miQyWUqkFF84RsVGaLc5n03TaRCFsPRYYyl3SNNjzAsJgFxb274u8g21DK6KWEErJD\nVeZaNe5U0RaD+toOZX5N723sGFAxbSjFaQ3Mykf6zb78e7VZWeN63ib5/5OT4cQcGFvhr1s+19Pq\nyCPrn7Gy6rqioHTd56dYDdzrAJd1SFACYbELSlcUbRHKoLvy8iqjbYmrbftCmVCFbNGkPSDzLKpq\nIylmkm0G1gxR1bmu6n6GMsnQTJ5V6ejFZitWuH9p89yD8o3rZXYaH08a5u9ZOruUUDcFQlEQs3iX\nVUHmBaVtNWwZsfbq8roSlECQoHRD3hZdVqU1pU2Jq6pk06bfSui5yMerrjqn6B0WM+NlMd2x0+MW\nbRW6d1UTpNVl8lddNdsWTao+6wRozn2uqLqKdb/NP6dNO5DWeWNlgjI21sxRJOaZblKi6woJSiBI\nULohb4uuG8fzdFVFl6eu7aVpemJ6h1dlPKG+D8UOcflMrE2aqkQ/VpxiwqWXzrZFk4yu6mt/bCzw\nHJQk+hBW65kXuq+xaYx1JMgEJTQFdZNScujjrEmbU9th+YtIUAJBgtINxV7AVZlZW/pV8qkTjKbX\njXku6koevaYvttE8RiR7aSDfunW2LWKv26bK6+Hx6pPHfsVnz2lZCSXLjIuZcsx4d2XvQVncYjt/\ntm1z6uUUXNJzAAAZDElEQVT9kaAEggSlG0K9gGMyqyb0q+QTIxhNSkZNp3rNzhuTecTGoy5NXQyy\nGBPyJZQyEe1lmJp8eNPKcAfFL22efOz6sW7R7uWOJKtWtetoWjWtQdO0xnRYjA1t3x8JSiBIULoh\n1Au41y+hYuYZ+3K1ocuqtLbPRS/VVCGq0hQjzk37p4Tu+VVX7agsjYVGM6hrO6kK5zLXyyufppg4\n5+OTdzwwc1+2rHz/2DaUYpqrhK7pYJjFatHY0AYJSiBIULqhqr9Bmww6lHlWjcXUJb2Iy+TkXM+m\nWLqspoqJZ4zbb11GlN2TvPde/p7XlVx7reqKCfnOiFX75R0TQvc/VoTz4rly5cyglPl3JKbja9ZP\npknpOdYzMGSfpkhQAkGC0g1lJZS2mX3VS1b1cvVCkyqZsuOLfS96nVejbTVV7PXyGWe+X02bKZlD\n1JVcuxjvrC7UjR1WJ6Qxw+3EeGAVP7qyd6PuvNnQQmXDunQhwG3eUwlKIEhQuiH0JdpLZl/Xwa7r\n3r91L2bMC1eXccTGo5dqqrY0KZE0KcHV9ZSvcnwICUNI/KqOqXMXLpaoqtyt2zpRlA1DU+dFWHXe\nmBJXk9Dmo2ReCApwFnArsAu4KLD9COBT6favAmvT9WuBHwE3puFDMdeToHRD3sur18y+6ku16+qt\njLqXM+aFq8s4yuiqob3p+fLEZk6xQ49k1AnK5s1xJZOq+17lIdXEXbiq/SMrKcScp8wDq+xDo67t\nJbR/01JJ5ore5Xs18oICLAVuB54MjAHfBE4r7HNBJhbAq4FPpf/XAt9uek0JSje09WxqUoTvZ+fI\nukynXyWULhva25wvNv3ZeWI6TOapE5S66qiY+NeluYsv+ZjSUF58QvELVYXGuo/nz9s0PXXXWrDD\n1wM/DVyTW3478PbCPtcAP53+XwY8AJgEZbjE2qLty99ket42VL2k/WxD6boaq+35quyeF6+m7Th1\nbSh1Ifa+t/1IiQ1NqpjKPLAyZ42q6RZizlt3TGgOnTIbNJnquEisoFiy7+Axs3OAs9z9jenyecBz\n3f0tuX2+ne6zJ12+HXgusAq4Gfgu8BDwe+7+pZLrbAI2AaxZs2bd9u3bW8V3enqaVatWtTp2oVFl\ni3374N574dFHy48fG4PTT4cbbijfZ926HiNZwb59sHs3HD48e/2yZXDSSbB6dfx5Dh6c5p57VjE2\nBiecUH1s1+mNOV/+fmRxhLnpX7IEJiZmx/+mm8L3Mbt/xfOfdNI0e/asok2WErp+KO4x96Z43OHD\ncPBgXDyWLYNnPKP8GamLc8b09DSPPrqq9hx15y27B1Buk5j71pQzzjjjBndfX7tjjOr0IwCvAj6c\nWz4PeH9hn5uBE3PLtwPjJG0r4+m6dcA9wOPqrqkSSjeU2aLJ12HXdbxN6aoNqMlzMegSSlUJMSb9\nTd2Nt27d4cuXl8/nXlbdEyqZtK3OC9HkuSxOVFYW51Wrqqsqm5bWyqYfbmOHrj0E3T26hDJMQWld\n5RU4105gfd01JSjdUGaLLuqvBzWwZFc0bU/q0jtuEFWKTTzR8r3Di5lx2ayDZenvWnxj3ZRD5w/N\nUV8W71BVaK/PfMiWg6xadZ8fgrIMuAM4hZlG+acV9vkNZjfKfzr9fxywNP3/ZOBeYHXdNSUo3VBm\ni7bjQJV9nc0Hmj4XXZWMYs5XdT9iMrG6eBbPnx+/qkzsYieCqvrKbmvDmOezyXA0oQy6zFkjFM5l\n0u9eUj/Z1uTk3A6M2SjGIVt0/eHiPg8EJYkjLyFpB7kd2JKueyfwivT/kcBfkrgNfw14crr+v6fV\nYd8EvgG8POZ6EpRuaFpCyQSjyUs8CsRkXKP8XMT0Fg8RmyFVlVB6/UouO76pK3MTe5TFr0kVUpk7\neUhMHomYDnhysnwU5pUre6vSbMK8EJRBBwlKNzRpQ4mpghlEm0lTYjPVUX4u6toOyoQ89j6F2lDq\neprHfjyU2b+pK3MTe3RR/RZbQrmT+pO29Vjrx/vUuaAAzwden/4/Djgl9thRCRKUbqiyRVOXzlFt\nM4nNREb9uWjTcbSJGOTv96WX7uj04yH0LHUhVNk5q4Y5ydOPNpRD1Cek7dw0/SjxdyoowB8CfwN8\nN10+HvinmGNHKUhQuqEXW3RdFO8XsRlXcfbKUUxbUyFvKwZtZvJsarNBl3JD6cgGcqw6Jj9o6Jln\nzn2edlt1QiYn64WjrCps5EsoJMObGPAvuXXfijl2lIIEpRsWgy2allBGvfTVJONumpbs3Fu37ph1\n7rprtrHZoO3chbhm8c7b4kubqxNS196zfHkzr7le6VpQvpb+fiP9XSlBWbwsBls0bUOZT+1DMcQK\nUN5OWTVPr43kdTYbZEmwbRVb1DtSkZAqJ5a82/CgbNG1oLwNuCx1830T8M/AW2OOHaUgQemGxWKL\nJl5e/ehMNh/Ii0K+3SBGSEfZZtm9ryolVGXgvb4jVV5uwyBWUJYRgbtvNbMXkgxz8p+BP3D3a2OO\nFWK+snFjEmI4+eRkqI7Q+oXM3Xc3W58xNZUMMXLo0Nxtw7bZ1BRs2gT791fvt3t3sh/EPyexXHzx\n3DisWAGXXNLtdbpmSd0OZrbUzL7g7te6+++4+9skJkLM5uKLkxc+z4oVyfp+MjUFa9cmmfPatcny\nICnL/KtEIcuwQ2IyCJvVsWVLvZhk7N+f7N81GzfCtm3JmF5mye+2bd0LV9fUCoq7HwL2m9njBxAf\nIeYlw8gAsox59+6kQiT7Yh6kqLQR0rIMe+nS0cg060pXve6fp+qDYONGuOuuZHDJu+4avl1iqBWU\nlP8AbjKzj5jZpVnoZ8SEmG8MOgMIZcz9+mIuIy+kECekZRnw4cOjkWmWla6WLm22fx39+CAYdok1\nVlD+Fvh94IvADbkghBgSbdsvuiYT0nXr6oU0azsJMey2k4yyUtemTd1Wa3b9QRASqNe+Fo49dnDC\nEiUo7n458ElmhOQv0nVCiCHRpv1imIx620lGWfXlBz5QXa2ZLx3cdNPcTLxYegg5ccDsD4ImJY6y\nqsS9ewdYFRrjCgZsAHYD/0BSSrkT+LmYY0cpyG24G2SLGYZpi1HrTFlniy6G0x9VQuOa5WdTHB+f\nO2Jwmdt01Xw2oRkaM7qY2roMIt2GY6u83g38orv/vLv/HPAi4L2dq5sQIpr55gk0qm0nXbQ7hEoH\nBw4kpQP35Lc4i6J7ct/y5EtqdecstrnUlUwHURUaKyjL3f3WbMHdvwss70+UhBCxzCdPoFGsouuq\nYbxtZu1e/kEQc858m0uo7SfPIOwcKyjXpx5eG9Lwv1CjvBAjwbA9e2IZVl+dKrpqGG+bWU9MlH8Q\nxJ4zE56sxDo+PnefQdk5VlA2k0xo9T+AC4HvAL/er0gJIeIYhb4osYxiFV1XnnJ1pYMQdZl87Dnz\nwrNxIzzwAExODsfOsYKyDLjE3f+bu/8ScClQ4pUthBgUo9AXpQmjVkXXVTVcUSyXLYOxsdn7LF+e\nlB5iM/niOcfH556zTJSGZedYQbkOOCq3fBTwhe6jI4Rowqj0RZmvdFkNl8/En/EM+OhHZ5cSPvax\npPTQJJPPn/OBB+aec9glvCKxgnKku09nC+n/hgU8IUTXjGJD93wiVA13/vlJCa/XNqmmpYSYtrBR\nK+EViRWUR8zsp7IFM1sP/Kg/URJCxDA1BdPTc9cPu6F7vpHPpC++GC6/fPBtUvOpLayKWEG5EPhL\nM/uSmX0R2A68pX/REkJUkWVAe/fOXj8+PphBKeeDV1kbhtUmNd/awsqImg8FOAV4FnAy8EvA8wDv\nV6SEENWUDbOxatVgRjjOrp2fE+SEE/p33UExrDaphdIWFltC+X13fwg4BnghsA34YN9iJYSoZFgZ\n0EL5ki5jWG1SC6UtLFZQsuHcXgp8yN0/C4xV7C+E6CPDyoAWypd0GcPqfDmKnT7bECso95rZZcAv\nA583syMaHCuE6JhhZUAL5Uu6jGF1vhzFTp9tiBWFXwauAc5y9weB1cDv9C1WQohKhpUBLZQv6SqG\n5Zo76i7BMcTOh7Lf3f/K3W9Ll+9397/rb9SEEFDuVTWMDKgLIVvIXmKLnaFWW5nZWWZ2q5ntMrOL\nAtuPMLNPpdu/amZrc9venq6/1cxeNMh4CzEoRrF/Qi9CNorpEd0xNEExs6XAnwMvBk4DzjWz0wq7\nvQH4gbufSjL/yp+mx54GvBp4GnAW8IH0fEIsKBaaV9VCS4+YzTBLKM8Bdrn7He7+KElnybML+5wN\nZFMNXwmcaWaWrt/u7j929zuBXen5hFhQLDSvqoWWHjGb2I6N/eAE4J7c8h7guWX7uPtBM/shMJ6u\n/0rh2GC3KjPbBGwCWLNmDTt37mwV2enp6dbHLjRkixn6bYtLLpk70x8ko86O2i2IscV8Sk8vLNZ3\nZJiCYoF1xd73ZfvEHJusdN9G0hGT9evX+4YNGxpEcYadO3fS9tiFhmwxQ79tce+9s3umQ+JVtW0b\njNotiLHFfEpPLyzWd2SYVV57gJNyyycC95XtY2bLgMcD+yKPFWLes1D6J2QstPSI2QxTUL4OPMXM\nTjGzMZJG9qsL+1wNnJ/+Pwf4e3f3dP2rUy+wU4CnAF8bULyFGCiDdA8ehEvvQuhvIcIMTVDc/SDJ\niMXXALcAn3b3m83snWb2inS3jwDjZrYL+C3govTYm4FPk0xF/H+A33D3Q8VriPmL+ioMHrn0il4Z\nZhsK7v554POFdX+Q+/8fwKtKjr0YWED9c0VG1Yi2+prtH1UuvbK7iEHjcYmRQ30VhoNcekWvSFDE\nyKGMbTgs9IEfRf+RoIiRQxnbcFgMAz+K/iJBESOHMrbhIJde0SsSFDFyKGNrTldecXLpFb0wVC8v\nIcrYuFGZWSzyihOjgkooQsxz5BUnRgUJihDzHHnFiVFBgiLEPEdecWJUkKAIMc+RV5wYFSQoQsxz\n5BUnRgV5eQmxAJBXnBgFVEIRQgjRCRIUIYQQnSBBEUII0QkSFCGEEJ0gQRFCCNEJEhQhhBCdIEER\nQgjRCRIUIYQQnSBBEUII0QkSFCGEEJ0gQRFCCNEJEhQhhBCdIEERQgjRCRIUIYQQnSBBEUII0QkS\nFCGEEJ0wFEExs9Vmdq2Z3Zb+PqFkv/PTfW4zs/Nz63ea2a1mdmMafmJwsRdCCBFiWCWUi4Dr3P0p\nwHXp8izMbDXwh8BzgecAf1gQno3u/sw0fH8QkRZCCFHOsATlbODy9P/lwCsD+7wIuNbd97n7D4Br\ngbMGFD8hhBANGdac8mvc/X4Ad7+/pMrqBOCe3PKedF3Gx8zsEHAV8Mfu7qELmdkmYBPAmjVr2Llz\nZ6sIT09Ptz52oSFbzCBbzCBbzLBYbdE3QTGzLwBPDGzaEnuKwLpMNDa6+71mdjSJoJwHXBE6ibtv\nA7YBrF+/3jds2BB5+dns3LmTtscuNGSLGWSLGWSLGRarLfomKO7+grJtZvY9M3tSWjp5EhBqA9kD\nbMgtnwjsTM99b/r7sJn9BUkbS1BQhBBCDIZhtaFcDWReW+cDnw3scw3wi2b2hLQx/heBa8xsmZkd\nC2Bmy4GXAd8eQJyFEEJUMCxB+RPghWZ2G/DCdBkzW29mHwZw933A/wN8PQ3vTNcdQSIs3wJuBO4F\n/tfgkyCEECLPUBrl3X0vcGZg/fXAG3PLHwU+WtjnEWBdv+MohBCiGeopL4QQohMkKEIIITpBgiKE\nEKITJChCCCE6QYIihBCiEyQoQgghOkGCIoQQohMkKEIIITpBgiKEEKITJChCCCE6QYIihBCiEyQo\nYmhMTcHatbBkSfI7NTXsGAkhemFYMzaKRc7UFGzaBPv3J8u7dyfLABs3Di9eQoj2qIQihsKWLTNi\nkrF/f7JeCDE/kaCIoXD33c3WCyFGHwmKGAonn9xsvRBi9JGgiKFw8cWwYsXsdStWJOuFEPMTCYoY\nChs3wrZtMDEBZsnvtm1qkBdiPiMvLzE0Nm6UgAixkFAJRQghRCdIUIQQQnSCBEUIIUQnSFCEEEJ0\nggRFCCFEJ0hQRE9ogEchRIbchkVrNMCjECKPSiiiNRrgUQiRZyiCYmarzexaM7st/X1CyX7/x8we\nNLPPFdafYmZfTY//lJmNDSbmIo8GeBRC5BlWCeUi4Dp3fwpwXboc4v8Hzgus/1PgvenxPwDe0JdY\niko0wKMQIs+wBOVs4PL0/+XAK0M7uft1wMP5dWZmwC8AV9YdL/qLBngUQuQxdx/8Rc0edPdjcss/\ncPeyaq8NwNvc/WXp8rHAV9z91HT5JOB/u/t/KTl+E7AJYM2aNeu2b9/eKs7T09OsWrWq1bELjbwt\n9u2De++FRx+FsTE44QRYvXrIERwgei5mkC1mWGi2OOOMM25w9/V1+/XNy8vMvgA8MbCp1yZbC6wr\nVUV33wZsA1i/fr1v2LCh1UV37txJ22MXGrLFDLLFDLLFDIvVFn0TFHd/Qdk2M/uemT3J3e83sycB\n329w6geAY8xsmbsfBE4E7usxukIIIXpkWG0oVwPnp//PBz4be6AndXQ7gHPaHC+EEKI/DEtQ/gR4\noZndBrwwXcbM1pvZh7OdzOxLwF8CZ5rZHjN7UbrpfwK/ZWa7gHHgIwONvRBCiDkMpae8u+8Fzgys\nvx54Y275Z0uOvwN4Tt8iKIQQojHqKS+EEKITJChCCCE6QYIihBCiEyQoQgghOkGCIoQQohMkKEII\nITpBgiKEEKITJChCCCE6QYIihBCiEyQoQgghOkGCIoQQohOGMsHWsDCzfwd2tzz8WJKh84VskUe2\nmEG2mGGh2WLC3Y+r22lRCUovmNn1MTOWLQZkixlkixlkixkWqy1U5SWEEKITJChCCCE6QYISz7Zh\nR2CEkC1mkC1mkC1mWJS2UBuKEEKITlAJRQghRCdIUIQQQnSCBKUGMzvLzG41s11mdtGw4zMIzOyj\nZvZ9M/t2bt1qM7vWzG5Lf5+QrjczuzS1z7fM7KeGF/NuMbOTzGyHmd1iZjeb2YXp+sVoiyPN7Gtm\n9s3UFn+Urj/FzL6a2uJTZjaWrj8iXd6Vbl87zPj3AzNbamb/YmafS5cXrS0yJCgVmNlS4M+BFwOn\nAeea2WnDjdVA+DhwVmHdRcB17v4U4Lp0GRLbPCUNm4APDiiOg+Ag8Nvu/lTgecBvpPd/Mdrix8Av\nuPszgGcCZ5nZ84A/Bd6b2uIHwBvS/d8A/MDdTwXem+630LgQuCW3vJhtAUhQ6ngOsMvd73D3R4Ht\nwNlDjlPfcfcvAvsKq88GLk//Xw68Mrf+Ck/4CnCMmT1pMDHtL+5+v7t/I/3/MEnmcQKL0xbu7tPp\n4vI0OPALwJXp+qItMhtdCZxpZjag6PYdMzsReCnw4XTZWKS2yCNBqeYE4J7c8p503WJkjbvfD0lG\nC/xEun5R2CitpngW8FUWqS3SKp4bge8D1wK3Aw+6+8F0l3x6H7NFuv2HwPhgY9xX3gf8LnA4XR5n\n8driMSQo1YS+IuRnPZsFbyMzWwVcBfymuz9UtWtg3YKxhbsfcvdnAieSlN6fGtot/V2wtjCzlwHf\nd/cb8qsDuy54WxSRoFSzBzgpt3wicN+Q4jJsvpdV36S/30/XL2gbmdlyEjGZcve/SlcvSltkuPuD\nwE6SdqVjzGxZuimf3sdskW5/PHOrUecrPwO8wszuIqkG/wWSEstitMUsJCjVfB14Suq9MQa8Grh6\nyHEaFlcD56f/zwc+m1v/q6mH0/OAH2bVQfOdtJ77I8At7v6e3KbFaIvjzOyY9P9RwAtI2pR2AOek\nuxVtkdnoHODvfYH0onb3t7v7ie6+liRP+Ht338gitMUc3F2hIgAvAb5LUl+8ZdjxGVCaPwncDxwg\n+bp6A0md73XAbenv6nRfI/GEux24CVg/7Ph3aIfnk1RNfAu4MQ0vWaS2eDrwL6ktvg38Qbr+ycDX\ngF3AXwJHpOuPTJd3pdufPOw09MkuG4DPyRZJ0NArQgghOkFVXkIIITpBgiKEEKITJChCCCE6QYIi\nhBCiEyQoQgghOkGCIsSAMLN3mNnbhh0PIfqFBEUIIUQnSFCE6CNmtiWdT+cLwH9O173JzL6ezi1y\nlZmtMLOjzezOdKgXzOxxZnaXmS03s/9hZt9J51jZPtQECVGBBEWIPmFm60iG5ngW8N+AZ6eb/srd\nn+3J3CK3AG/wZHj8nSRDopMed5W7HyCZb+VZ7v504NcHmAQhGiFBEaJ//Czw1+6+35NRirNx4P6L\nmX3JzG4CNgJPS9d/GHh9+v/1wMfS/98CpszstSSTfgkxkkhQhOgvobGNPg68xd1PB/6IZKwn3P2f\ngLVm9vPAUnfPpmB+KckYYeuAG3Ij2goxUkhQhOgfXwR+ycyOMrOjgZen648G7k/bSzYWjrmCZHDO\njwGY2RLgJHffQTKh0zHAqkFEXoimaHBIIfqImW0BfhXYTTJy83eAR0jEYTfJqMRHu/vr0v2fCNwJ\nPMndH0xFZwfJHBoGTLr7nww6HULEIEERYoQws3OAs939vGHHRYimqC5WiBHBzN4PvJhkzhUh5h0q\noQghhOgENcoLIYToBAmKEEKITpCgCCGE6AQJihBCiE6QoAghhOiE/wuAY4573x9eEQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2e924fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "cls = IsolationForest(n_estimators=200)\n",
    "cls.fit(model.graph_embeddings[np.where(labels == 0)[0]][:])\n",
    "plt.scatter(np.where(labels==0), \n",
    "            cls.decision_function(model.graph_embeddings)[np.where(labels==0)], c='b', label='normal')\n",
    "plt.scatter(np.where(labels==1), \n",
    "            cls.decision_function(model.graph_embeddings)[np.where(labels==1)], c='r', label='outlier')\n",
    "\n",
    "plt.title(\"Isolation forest outlier score\")\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('days')\n",
    "plt.grid()\n",
    "fig.savefig('isolation_forest.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5004866180048662"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 * sum(labels) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Всякие разные вопросы, возникшие по ходу \n",
    "\n",
    "* можно ли считать AUC для такой задачи, чтобы сравнивать модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-446-d3da39328c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    771\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, random_state, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 815\u001b[0;31m                                                       **opt_args)\n\u001b[0m\u001b[1;32m    816\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             print(\"[t-SNE] KL divergence after %d iterations with early \"\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m     error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,\n\u001b[1;32m    246\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                                       dof=degrees_of_freedom)\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tsne = TSNE(perplexity=40)\n",
    "start = 50\n",
    "X = tsne.fit_transform(model.graph_embeddings[start:])\n",
    "\n",
    "plt.scatter(X[np.where(labels[start:] == 0)[0], 0], X[np.where(labels[start:] == 0)[0], 1], label='normal', c='b')\n",
    "plt.scatter(X[np.where(labels[start:] == 1)[0], 0], X[np.where(labels[start:] == 1)[0], 1], label='outlier', c='r')\n",
    "plt.title(\"TSNE on embeddings\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.7)\n",
    "#plt.scatter(norms[np.where(labels == 1)[0]], label='outlier', c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-64973097d897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprev_cnt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a227e3c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "prev_cnt = 40\n",
    "for i, label in zip(range(prev_cnt, len(labels[prev_cnt:])), labels[prev_cnt:]):\n",
    "    vector_weight = 1 / np.exp(range(0, prev_cnt))\n",
    "    if label == 0:\n",
    "        ax.scatter(i, np.linalg.norm(model.graph_embeddings[i] - sum(model.graph_embeddings[i-prev_cnt:i].T.dot(vector_weight))), c='b')\n",
    "    else:\n",
    "        ax.scatter(i, np.linalg.norm(model.graph_embeddings[i] - sum(model.graph_embeddings[i-prev_cnt:i].T.dot(vector_weight))), c='r')\n",
    "    fig.canvas.draw()   \n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-0a5e90e497d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfunc_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterative_drawing_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Macbook/Desktop/IITP/GraphAnomaly/awe/func_tools.py\u001b[0m in \u001b[0;36miterative_drawing_2d\u001b[0;34m(model, labels)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/backends/backend_agg.pyc\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/artist.pyc\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/figure.pyc\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1144\u001b[0;31m                 renderer, self, dsu, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/image.pyc\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, dsu, suppress_composite)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdsu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/artist.pyc\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2424\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/image.pyc\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, dsu, suppress_composite)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdsu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/artist.pyc\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/collections.pyc\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mCollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/artist.pyc\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/collections.pyc\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    344\u001b[0m             renderer.draw_markers(\n\u001b[1;32m    345\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 mpath.Path(offsets), transOffset, tuple(facecolors[0]))\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             renderer.draw_path_collection(\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/backends/backend_agg.pyc\u001b[0m in \u001b[0;36mdraw_markers\u001b[0;34m(self, *kl, **kw)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# maybe there is better way to do it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_markers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_markers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_path_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/matplotlib/backend_bases.pyc\u001b[0m in \u001b[0;36mget_hatch_linewidth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hatch_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhatch_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_hatch_linewidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \"\"\"\n\u001b[1;32m   1124\u001b[0m         \u001b[0mGets\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhatching\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkJn/B6r81x7Jdn2/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1fe42790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "func_tools.iterative_drawing_2d(model, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a495357d0>"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2e54bb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prev_cnt = 3\n",
    "prev_dataset = []\n",
    "for i, label in zip(range(prev_cnt+20, len(labels[prev_cnt:])), labels[prev_cnt:]):\n",
    "    vector_weight = sum(np.exp(range(0, prev_cnt))) / np.exp(range(0, prev_cnt))\n",
    "    prev_dataset.append(model.graph_embeddings[i] - sum(model.graph_embeddings[i-prev_cnt:i].T.dot(vector_weight)))\n",
    "\n",
    "fig = plt.figure()\n",
    "cls = OneClassSVM(kernel='rbf', gamma=5)\n",
    "cls.fit(model.graph_embeddings[np.where(labels[prev_cnt:] == 0)[0]][:])\n",
    "plt.scatter(np.where(labels[prev_cnt:]==0), \n",
    "            cls.decision_function(model.graph_embeddings)[np.where(labels[prev_cnt:]==0)], c='b', label='normal')\n",
    "plt.scatter(np.where(labels[prev_cnt:]==1), \n",
    "            cls.decision_function(model.graph_embeddings)[np.where(labels[prev_cnt:]==1)], c='r', label='outlier')\n",
    "plt.grid()\n",
    "plt.title(\"One class SVM forest outlier score\")\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('days')\n",
    "#fig.savefig('oneclass_svm.pdf', format='pdf')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels=636 does not match number of samples=637",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-b0173da2815a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 327\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Macbook/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 236\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels=636 does not match number of samples=637"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cls = RandomForestClassifier(n_estimators = 100)\n",
    "cls.fit(model.graph_embeddings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simple SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = open('../DD/DD_label.txt')\n",
    "labels = labels.read()\n",
    "labels = np.array(map(lambda x: int(x), labels.strip().split()))\n",
    "\n",
    "graph_embeddings = np.load('../experiments_data/DD/DD_doc_emb.npy')\n",
    "\n",
    "outliers = graph_embeddings[np.random.choice(np.where(labels == 1)[0], 10)]\n",
    "normal = graph_embeddings[np.where(labels == 0)[0]]\n",
    "\n",
    "#outliers = graph_embeddings[np.where(labels == 1)[0]]\n",
    "#normal = graph_embeddings[np.where(labels == 0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZcdGza3GwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2019f250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "cls = OneClassSVM(kernel='rbf', gamma=1.9)\n",
    "cls.fit(np.concatenate((normal, outliers), axis=0))\n",
    "#cls.fit(normal[:])\n",
    "plt.scatter(range(len(normal)), cls.decision_function(normal), c='r', s=20)\n",
    "plt.scatter(range(len(normal), len(normal) + 10), cls.decision_function(outliers), c='b')\n",
    "plt.ylabel('OneClass SVM score')\n",
    "plt.xlabel('graph number')\n",
    "plt.grid()\n",
    "#fig.savefig('oneclass_svm_nci1_2.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.8, kernel='rbf',\n",
       "      max_iter=-1, nu=0.5, random_state=None, shrinking=True, tol=0.001,\n",
       "      verbose=False)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = OneClassSVM(kernel='rbf', gamma=0.8)\n",
    "cls.fit(np.concatenate((normal, outliers), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "accuracy:  0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    3.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "cls = SVC()\n",
    "X_train, X_test, y_train, y_test = train_test_split(graph_embeddings, \n",
    "                                                    labels, stratify=labels, test_size=0.01)\n",
    "\n",
    "srch = RandomizedSearchCV(cls, {'gamma': np.linspace(0.01, 2, 40), 'kernel': ('linear', 'rbf')}, \n",
    "                          verbose=True, scoring='accuracy')\n",
    "\n",
    "# srch = RandomizedSearchCV(cls, {'n_estimators': range(10, 200, 20), 'max_depth': range(3, 20, 2)}, \n",
    "#                          verbose=True, scoring='accuracy')\n",
    "\n",
    "srch.fit(X_train, y_train)\n",
    "\n",
    "#srch.fit(graph_embeddings, labels)\n",
    "print \"accuracy: \", accuracy_score(y_test, srch.best_estimator_.predict(X_test))\n",
    "#print \"prescicion: \", precision_score(y_test, srch.best_estimator_.predict(X_test))\n",
    "#print \"recall: \", recall_score(y_test, srch.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8842195540308748"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 1.4897435897435898, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.863448275862069, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87118644 0.87457627 0.89455782 0.88095238]\n"
     ]
    }
   ],
   "source": [
    "cls = SVC(kernel='rbf', gamma=1.5)\n",
    "cv=KFold(n_splits=4, shuffle=True)\n",
    "res = cross_val_score(cls, graph_embeddings, labels, scoring='accuracy', cv=cv)\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.851063829787234"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prescicion: "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dc3da064b004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 'kernel': ('linear', 'rbf')}, verbose=True, scoring='roc_auc')\n\u001b[1;32m      4\u001b[0m \u001b[0msrch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"prescicion: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"recall: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "cls = SVC()\n",
    "srch = RandomizedSearchCV(cls, {'gamma': np.linspace(0.001, 10, 100), \n",
    "                                'kernel': ('linear', 'rbf')}, verbose=True, scoring='roc_auc')\n",
    "srch.fit(model.graph_embeddings, labels)\n",
    "print \"prescicion: \", precision_score(y_test, srch.best_estimator_.predict(X_test))\n",
    "print \"recall: \", recall_score(y_test, srch.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print \"prescicion: \", precision_score(y_test, cls.predict(X_test))\n",
    "print \"recall: \", recall_score(y_test, cls.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193.41898\n"
     ]
    }
   ],
   "source": [
    "with model.graph.as_default():\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    v = sess.run(model.word_embeddings)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "recall_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel27",
   "language": "python",
   "name": "kernel27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
